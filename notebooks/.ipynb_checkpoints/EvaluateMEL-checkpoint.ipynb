{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff95f192-5b1e-4caf-a5cc-3290b0aebf22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a100-st-p4d24xlarge-9\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yaml\n",
    "from hydra.experimental import compose, initialize_config_module\n",
    "import hydra\n",
    "from functools import lru_cache\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import faiss\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "from bela.transforms.spm_transform import SPMTransform\n",
    "from bela.evaluation.model_eval import ModelEval, load_file\n",
    "# from bela.utils.prediction_utils import get_predictions_using_windows\n",
    "from bela.utils.analysis_utils import Entity, Sample\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "!cat /etc/hostname  # Double check that we are on a gpu node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db8ddf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GroundTruthEntity:\n",
    "    offset: int\n",
    "    length: int\n",
    "    text: str\n",
    "    entity_id: str\n",
    "\n",
    "    @property\n",
    "    def mention(self):\n",
    "        return self.text[self.offset : self.offset + self.length]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"mention=\\\"{self.mention}\\\" -> entity_id={self.entity_id}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PredictedEntity:\n",
    "    offset: int\n",
    "    length: int\n",
    "    text: str\n",
    "    entity_id: str\n",
    "    md_score: float\n",
    "    el_score: float\n",
    "\n",
    "    @property\n",
    "    def mention(self):\n",
    "        return self.text[self.offset : self.offset + self.length]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"mention=\\\"{self.mention}\\\" -> entity_id={self.entity_id} (md_score={self.md_score:.2f}, el_score={self.el_score:.2f})\"\n",
    "\n",
    "\n",
    "def print_sample(text: str, ground_truth_entities: List[GroundTruthEntity], predicted_entities: List[PredictedEntity], max_display_length=1000):\n",
    "    print(f\"{text[:max_display_length]=}\")\n",
    "    print(\"***************** Ground truth entities *****************\")\n",
    "    print(f\"{len(ground_truth_entities)=}\")\n",
    "    for ground_truth_entity in ground_truth_entities:\n",
    "        if ground_truth_entity.offset + ground_truth_entity.length > max_display_length:\n",
    "            continue\n",
    "        print(ground_truth_entity)\n",
    "    print(\"***************** Predicted entities *****************\")\n",
    "    print(f\"{len(predicted_entities)=}\")\n",
    "    for predicted_entity in predicted_entities:\n",
    "        if predicted_entity.offset + predicted_entity.length > max_display_length:\n",
    "            continue\n",
    "        print(predicted_entity)\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def get_sp_transform():\n",
    "    return SPMTransform(max_seq_len=100000)\n",
    "\n",
    "\n",
    "def get_windows(text, window_length=254, overlap=127):\n",
    "    sp_transform = get_sp_transform()\n",
    "    tokens = sp_transform([text])[0]\n",
    "    tokens = tokens[1:-1]\n",
    "    windows = []\n",
    "    for window_start in range(0, len(tokens), window_length - overlap):\n",
    "        start_pos = tokens[window_start][1]\n",
    "        if window_start + window_length >= len(tokens):\n",
    "            end_pos = tokens[-1][2]\n",
    "        else:\n",
    "            end_pos = tokens[window_start + window_length][2]\n",
    "        windows.append((start_pos, end_pos))\n",
    "    return windows\n",
    "\n",
    "\n",
    "def convert_predictions_to_dict(example_predictions):\n",
    "    if len(example_predictions) > 0:\n",
    "        offsets, lengths, entities, md_scores, el_scores, window_idx, window_start, window_end = zip(*example_predictions)\n",
    "    else:\n",
    "        offsets, lengths, entities, md_scores, el_scores, window_idx, window_start, window_end = [], [], [], [], [], [], [], []\n",
    "    return {\n",
    "        \"offsets\": offsets,\n",
    "        \"lengths\": lengths,\n",
    "        \"entities\": entities,\n",
    "        \"md_scores\": md_scores,\n",
    "        \"el_scores\": el_scores,\n",
    "        \"window_idx\": window_idx,\n",
    "        \"window_start\":window_start,\n",
    "        \"window_end\": window_end\n",
    "    }\n",
    "\n",
    "\n",
    "def group_predictions_by_example(all_predictions, extended_examples):\n",
    "    grouped_predictions = defaultdict(list)\n",
    "    for prediction, extended_example in zip(all_predictions, extended_examples):\n",
    "        window_start = extended_example[\"window_start\"]\n",
    "        prediction = dict(prediction)\n",
    "        prediction[\"offsets\"] = [\n",
    "            offset + window_start for offset in prediction[\"offsets\"]\n",
    "        ]\n",
    "        grouped_predictions[extended_example[\"document_id\"]].append((prediction))\n",
    "\n",
    "    predictions = {}\n",
    "    for document_id, example_prediction_list in grouped_predictions.items():\n",
    "        example_predictions = []\n",
    "        for prediction in example_prediction_list:\n",
    "            for offset, length, ent, md_score, el_score in zip(\n",
    "                prediction[\"offsets\"],\n",
    "                prediction[\"lengths\"],\n",
    "                prediction[\"entities\"],\n",
    "                prediction[\"md_scores\"],\n",
    "                prediction[\"el_scores\"],  \n",
    "            ):\n",
    "                example_predictions.append((offset, length, ent, md_score, el_score, extended_example[\"window_idx\"], extended_example[\"window_start\"], extended_example[\"window_end\"]))\n",
    "                example_predictions = sorted(example_predictions)\n",
    "        predictions[document_id] = example_predictions\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def merge_predictions(example_predictions):\n",
    "    filtered_example_predictions = []\n",
    "\n",
    "    current_end = None\n",
    "    current_offset = None\n",
    "    current_length = None\n",
    "    current_ent_id = None\n",
    "    current_md_score = None\n",
    "    current_el_score = None\n",
    "    current_window_idx = None\n",
    "    current_window_start = None\n",
    "    current_window_end = None\n",
    "\n",
    "    for offset, length, ent_id, md_score, el_score, window_idx, window_start, window_end in example_predictions:\n",
    "        if current_end is None:\n",
    "            current_end = offset + length\n",
    "            current_offset = offset\n",
    "            current_length = length\n",
    "            current_ent_id = ent_id\n",
    "            current_md_score = md_score\n",
    "            current_el_score = el_score\n",
    "            current_window_idx = window_idx\n",
    "            current_window_start= window_start\n",
    "            current_window_end= window_end\n",
    "            continue\n",
    "\n",
    "        if offset < current_end:\n",
    "            # intersection of two predictions\n",
    "            if md_score > current_md_score:\n",
    "                current_ent_id = ent_id\n",
    "                current_offset = offset\n",
    "                current_length = length\n",
    "                current_md_score = md_score\n",
    "                current_el_score = el_score\n",
    "                current_window_idx = window_idx\n",
    "                current_window_start= window_start\n",
    "                current_window_end= window_end\n",
    "        else:\n",
    "            filtered_example_predictions.append(\n",
    "                (\n",
    "                    current_offset,\n",
    "                    current_length,\n",
    "                    current_ent_id,\n",
    "                    current_md_score,\n",
    "                    current_el_score,\n",
    "                    current_window_idx,\n",
    "                    current_window_start,\n",
    "                    current_window_end\n",
    "                )\n",
    "            )\n",
    "            current_ent_id = ent_id\n",
    "            current_offset = offset\n",
    "            current_length = length\n",
    "            current_md_score = md_score\n",
    "            current_el_score = el_score\n",
    "            current_window_idx = window_idx\n",
    "            current_window_start= window_start\n",
    "            current_window_end= window_end\n",
    "\n",
    "        current_end = offset + length\n",
    "\n",
    "    if current_offset is not None:\n",
    "        filtered_example_predictions.append(\n",
    "            (\n",
    "                current_offset,\n",
    "                current_length,\n",
    "                current_ent_id,\n",
    "                current_md_score,\n",
    "                current_el_score,\n",
    "                current_window_idx,\n",
    "                current_window_start,\n",
    "                current_window_end\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return filtered_example_predictions\n",
    "\n",
    "\n",
    "def get_predictions_using_windows(model_eval: ModelEval, test_data, batch_size=1024, window_length=254, window_overlap=127):\n",
    "    extended_examples = []\n",
    "    sample_to_window_bound = []\n",
    "    for example in test_data:\n",
    "        assert \"document_id\" in example or \"data_example_id\" in example\n",
    "        document_id = example.get(\"document_id\") or example[\"data_example_id\"]\n",
    "        text = example[\"original_text\"]\n",
    "        windows = get_windows(text, window_length, window_overlap)\n",
    "        sample_to_window_bound.append(windows)\n",
    "        for idx, (start_pos, end_pos) in enumerate(windows):\n",
    "            new_text = text[start_pos:end_pos]\n",
    "            extended_examples.append(\n",
    "                {\n",
    "                    \"document_id\": document_id,\n",
    "                    \"original_text\": new_text,\n",
    "                    \"gt_entities\": example[\"gt_entities\"],\n",
    "                    \"window_idx\": idx,\n",
    "                    \"window_start\": start_pos,\n",
    "                    \"window_end\": end_pos,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    all_predictions = model_eval.get_predictions(\n",
    "        extended_examples, batch_size=batch_size\n",
    "    )\n",
    "    predictions_dict = group_predictions_by_example(all_predictions, extended_examples)\n",
    "\n",
    "    predictions = []\n",
    "    for example in test_data:\n",
    "        assert \"document_id\" in example or \"data_example_id\" in example\n",
    "        document_id = example.get(\"document_id\") or example[\"data_example_id\"]\n",
    "        text = example[\"original_text\"]\n",
    "        example_predictions = predictions_dict[document_id]\n",
    "        example_predictions = merge_predictions(example_predictions)\n",
    "        example_predictions = convert_predictions_to_dict(example_predictions)\n",
    "        predictions.append(example_predictions)\n",
    "\n",
    "    return predictions, sample_to_window_bound\n",
    "\n",
    "#########################\n",
    "\n",
    "def safe_division(a, b):\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return a / b\n",
    "\n",
    "\n",
    "def compute_f1_p_r(tp, fp, fn):\n",
    "    precision = safe_division(tp, (tp + fp))\n",
    "    recall = safe_division(tp, (tp + fn))\n",
    "    f1 = safe_division(2 * tp, (2 * tp + fp + fn))\n",
    "    return f1, precision, recall\n",
    "\n",
    "def compute_scores(data, predictions, md_threshold=0.2, el_threshold=0.05):\n",
    "    tp, fp, support = 0, 0, 0\n",
    "    tp_boe, fp_boe, support_boe = 0, 0, 0\n",
    "\n",
    "    predictions_per_example = []\n",
    "    for example, example_predictions in zip(data, predictions):\n",
    "\n",
    "        example_targets = {\n",
    "            (offset,length):ent_id\n",
    "            for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "        }\n",
    "\n",
    "        example_predictions = {\n",
    "            (offset, length):ent_id\n",
    "            for offset, length, ent_id, md_score, el_score in zip(\n",
    "                example_predictions['offsets'],\n",
    "                example_predictions['lengths'],\n",
    "                example_predictions['entities'],\n",
    "                example_predictions['md_scores'],\n",
    "                example_predictions['el_scores'],\n",
    "            )\n",
    "            if (el_score > el_threshold and md_score > md_threshold) \n",
    "        }\n",
    "\n",
    "        predictions_per_example.append((len(example_targets), len(example_predictions)))\n",
    "\n",
    "        for pos, ent in example_targets.items():\n",
    "            support += 1\n",
    "            if pos in example_predictions and example_predictions[pos] == ent:\n",
    "                tp += 1\n",
    "        for pos, ent in example_predictions.items():\n",
    "            if pos not in example_targets or example_targets[pos] != ent:\n",
    "                fp += 1\n",
    "\n",
    "        example_targets_set = set(example_targets.values())\n",
    "        example_predictions_set = set(example_predictions.values())\n",
    "\n",
    "        for ent in example_targets_set:\n",
    "            support_boe += 1\n",
    "            if ent in example_predictions_set:\n",
    "                tp_boe += 1\n",
    "        for ent in example_predictions_set:\n",
    "            if ent not in example_targets_set:\n",
    "                fp_boe += 1\n",
    "\n",
    "    fn = support - tp\n",
    "    fn_boe = support_boe - tp_boe\n",
    "    return compute_f1_p_r(tp, fp, fn), compute_f1_p_r(tp_boe, fp_boe, fn_boe)\n",
    "\n",
    "\n",
    "def compute_md_scores(data, predictions, sample_to_window_bound, md_threshold=0.2, only_preds_of_window = None, debug_samples=0):\n",
    "    tp, fp, support = 0, 0, 0\n",
    "\n",
    "    predictions_per_example = []\n",
    "    for i, (example, example_predictions, window_bounds) in enumerate(zip(data, predictions, sample_to_window_bound)):\n",
    "        if i < debug_samples:\n",
    "            predicted_entities = [Entity(offset=offset, length=length, text=text, entity_id=entity_id, md_score=md_score, el_score=el_score) \n",
    "                                for offset, length, entity_id, md_score, el_score in \n",
    "                              zip(example_predictions[\"offsets\"], example_predictions[\"lengths\"], example_predictions[\"entities\"], example_predictions[\"md_scores\"], example_predictions[\"el_scores\"])]\n",
    "            Sample(text=example[\"original_text\"], predicted_entities=predicted_entities).print()\n",
    "\n",
    "        window_start, window_end = window_bounds[only_preds_of_window] if len(window_bounds)>only_preds_of_window else (0,0)\n",
    "        example_targets = {\n",
    "            (offset,length):ent_id\n",
    "            for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "            if (only_preds_of_window is None or (offset> window_start and offset< window_end))\n",
    "            \n",
    "        }\n",
    "                    \n",
    "\n",
    "        example_predictions = {\n",
    "            (offset, length):ent_id\n",
    "            for offset, length, ent_id, md_score, window_idx in zip(\n",
    "                example_predictions['offsets'],\n",
    "                example_predictions['lengths'],\n",
    "                example_predictions['entities'],\n",
    "                example_predictions['md_scores'],\n",
    "                example_predictions['window_idx'],\n",
    "            )\n",
    "            if ((md_score > md_threshold) and ( only_preds_of_window is None or window_idx==only_preds_of_window))\n",
    "        }\n",
    "\n",
    "        predictions_per_example.append((len(example_targets), len(example_predictions)))\n",
    "\n",
    "        for pos in example_targets.keys():\n",
    "            support += 1\n",
    "            if pos in example_predictions:\n",
    "                tp += 1\n",
    "        for pos in example_predictions.keys():\n",
    "            if pos not in example_targets:\n",
    "                fp += 1\n",
    "\n",
    "    fn = support - tp\n",
    "    return (*compute_f1_p_r(tp, fp, fn), support)\n",
    "\n",
    "\n",
    "def correct_mention_offsets(example_predictions, text):\n",
    "    TEXT_SEPARATORS = [' ','.',',','!','?','-','\\n']\n",
    "    corrected_example_predictions = []\n",
    "    for offset,length,ent_id,md_score,el_score in example_predictions:\n",
    "        while offset !=0 and offset<len(text) and (text[offset-1] not in TEXT_SEPARATORS or text[offset] in TEXT_SEPARATORS):\n",
    "            offset += 1\n",
    "            length -= 1\n",
    "        while offset+length < len(text) and text[offset+length] not in TEXT_SEPARATORS:\n",
    "            length += 1\n",
    "        corrected_example_predictions.append((\n",
    "            offset,length,ent_id,md_score,el_score\n",
    "        ))\n",
    "    return corrected_example_predictions\n",
    "\n",
    "\"\"\"\n",
    "def get_predictions_using_windows(test_data, batch_size=1024):\n",
    "    extended_examples = []\n",
    "\n",
    "    for example in test_data:\n",
    "        text = example['original_text']\n",
    "        windows = get_windows(text)\n",
    "        for idx, (start_pos, end_pos) in enumerate(windows):\n",
    "            new_text = text[start_pos:end_pos]\n",
    "            extended_examples.append({\n",
    "                'document_id': example['document_id'] if 'document_id' in example else example['data_example_id'],\n",
    "                'original_text': new_text,\n",
    "                'gt_entities': example['gt_entities'],\n",
    "                'window_idx': idx,\n",
    "                'window_start': start_pos,\n",
    "                'window_end': end_pos,\n",
    "            })\n",
    "\n",
    "    all_predictions = model_eval.get_predictions(extended_examples, batch_size=batch_size)\n",
    "    predictions_dict = group_predictions_by_example(all_predictions, extended_examples)\n",
    "\n",
    "    predictions = []\n",
    "    for example in test_data:\n",
    "        document_id = example['document_id']  if 'document_id' in example else example['data_example_id']\n",
    "        text = example['original_text']\n",
    "        example_predictions = predictions_dict[document_id]\n",
    "        example_predictions = merge_predictions(example_predictions)\n",
    "        example_predictions = correct_mention_offsets(example_predictions, text)\n",
    "        example_predictions = convert_predictions_to_dict(example_predictions)\n",
    "        predictions.append(example_predictions)\n",
    "\n",
    "    return predictions\n",
    "\"\"\"\n",
    "\n",
    "def shift_shift(text):\n",
    "    for idx,ch in enumerate(text):\n",
    "        if not ch.isalpha():\n",
    "            return idx\n",
    "\n",
    "def convert_data_for_disambiguation(data, lang):\n",
    "    # convert examples to 1 entity per example and shift if needed\n",
    "    if lang=='ar':\n",
    "        MAX_LENGTH = 600\n",
    "        MAX_OFFSET = 400\n",
    "    elif lang == 'ja':\n",
    "        MAX_LENGTH = 350\n",
    "        MAX_OFFSET = 250\n",
    "    else:\n",
    "        MAX_LENGTH = 800\n",
    "        MAX_OFFSET = 600\n",
    "    new_examples = []\n",
    "    for example in tqdm(data):\n",
    "        original_text = example['original_text']\n",
    "        for _, _, ent, _, offset, length in example['gt_entities']:\n",
    "            shift = 0\n",
    "            if len(original_text) > MAX_LENGTH and offset > MAX_OFFSET:\n",
    "                shift = (offset - MAX_OFFSET)\n",
    "                shift += shift_shift(original_text[shift:])\n",
    "            new_example = {\n",
    "                'original_text': original_text[shift:],\n",
    "                'gt_entities': [[0,0,ent,_,offset-shift,length]],\n",
    "            }\n",
    "            new_examples.append(new_example)\n",
    "    return new_examples\n",
    "\n",
    "\n",
    "def metrics_disambiguation(test_data, predictions):\n",
    "    support = 0\n",
    "    support_only_predicted = 0\n",
    "    correct = 0\n",
    "    incorrect_pos = 0\n",
    "\n",
    "    for example_idx, (example, prediction) in tqdm(enumerate(zip(test_data, predictions))):\n",
    "#         targets = {\n",
    "#             (offset,length):ent_id\n",
    "#             for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "#         }\n",
    "#         prediction = {\n",
    "#             (offset,length):ent_id\n",
    "#             for offset,length,ent_id in zip(prediction['offsets'], prediction['lengths'], prediction['entities'])\n",
    "#         }\n",
    "\n",
    "#         support += len(targets)\n",
    "#         support_only_predicted += len(prediction)\n",
    "        \n",
    "#         correct += sum(1 for pos,ent_id in prediction.items() if (pos in targets and targets[pos] == ent_id))\n",
    "#         incorrect_pos += sum(1 for pos,_ in prediction.items() if pos not in targets)\n",
    "        if len(prediction['entities']) == 0:\n",
    "            continue\n",
    "        target = example['gt_entities'][0][2]\n",
    "        prediction = prediction['entities'][0]\n",
    "        correct += (target == prediction)\n",
    "        support += 1\n",
    "\n",
    "    accuracy = correct/support\n",
    "    # accuracy_only_predicted = correct/support_only_predicted\n",
    "\n",
    "    return accuracy, support #, accuracy_only_predicted, support_only_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f2dc7",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf93bf95-295e-4efd-8133-bd9aedf156d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         35247279 function calls (35184412 primitive calls) in 52.815 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "   List reduced from 6673 to 10 due to restriction <10>\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "       34   12.711    0.374   12.713    0.374 serialization.py:1076(load_tensor)\n",
       "        1   10.844   10.844   11.923   11.923 joint_el_datamodule.py:21(__init__)\n",
       "        1    6.706    6.706    6.706    6.706 {built-in method faiss._swigfaiss.GpuIndexFlat_add}\n",
       "        1    3.542    3.542    3.542    3.542 {built-in method faiss._swigfaiss.new_GpuIndexFlatIP}\n",
       "        1    2.720    2.720   52.819   52.819 model_eval.py:61(__init__)\n",
       "      302    2.086    0.007    2.086    0.007 {method 'uniform_' of 'torch._C._TensorBase' objects}\n",
       "        4    1.861    0.465    1.861    0.465 {method 'normal_' of 'torch._C._TensorBase' objects}\n",
       "     1351    1.643    0.001    1.643    0.001 {built-in method io.open_code}\n",
       "     8761    1.581    0.000    1.581    0.000 {built-in method posix.stat}\n",
       "      396    1.227    0.003    1.227    0.003 {method '_set_from_file' of 'torch._C.StorageBase' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%prun -s tottime -l 10\n",
    "\n",
    "# e2e model with isotropic embeddings\n",
    "#checkpoint_path = '/checkpoints/movb/bela/2023-01-13-023711/0/lightning_logs/version_4144/checkpoints/last.ckpt'  # Not working: Unexpected key(s) in state_dict: \"saliency_encoder.mlp.0.weight\", \"saliency_encoder.mlp.0.bias\", \"saliency_encoder.mlp.3.weight\", \"saliency_encoder.mlp.3.bias\", \"saliency_encoder.mlp.6.weight\", \"saliency_encoder.mlp.6.bias\". \n",
    "#checkpoint_path = '/checkpoints/movb/bela/2022-11-27-225013/0/lightning_logs/version_286287/checkpoints/last_15000.ckpt'  # Works but 0 F1\n",
    "\n",
    "# E2E checkpoint with new embeddings\n",
    "# https://fb.quip.com/QVUxA4UcAZ7k#temp:C:OcG977f71fab43d42379521a0dff\n",
    "# Works but give 0 F1 on tackbp (mention detection is okish, but entity disambiguation is random)\n",
    "#checkpoint_path = '/checkpoints/movb/bela/2023-01-18-220105/0/lightning_logs/version_4820/checkpoints/last.ckpt'\n",
    "checkpoint_path = '/checkpoints/movb/bela/2023-02-15-200343/0/lightning_logs/version_127953/checkpoints/checkpoint_9.ckpt'\n",
    "#model_eval = ModelEval(checkpoint_path, config_name=\"joint_el_mel_new\")\n",
    "model_eval = ModelEval(checkpoint_path, config_name=\"joint_el_mel_new_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0516d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set low thresholds\n",
    "model_eval.task.md_threshold = 0.01\n",
    "model_eval.task.el_threshold = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df37725",
   "metadata": {},
   "source": [
    "# End-to-end Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fd83c-fd03-4e8a-a3da-d8df70477d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model_eval.checkpoint_path=}\")\n",
    "print(f\"{model_eval.task.md_threshold=}\")\n",
    "print(f\"{model_eval.task.el_threshold=}\")\n",
    "datasets = [\n",
    "    \"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\",\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ta.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ar.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/fa.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/sr.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/tr.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/de.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/es.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ja.jsonl',\n",
    "]\n",
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    test_data = load_file(test_data_path)\n",
    "    #test_data = test_data[:1000]\n",
    "    \n",
    "    predictions = get_predictions_using_windows(model_eval, test_data)\n",
    "    (f1, precision, recall), (f1_boe, precision_boe, recall_boe) = ModelEval.compute_scores(test_data, predictions)\n",
    "    \n",
    "    print(f\"F1 = {f1:.4f}, precision = {precision:.4f}, recall = {recall:.4f}\")\n",
    "    print(f\"F1 boe = {f1_boe:.4f}, precision = {precision_boe:.4f}, recall = {recall_boe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da72624",
   "metadata": {},
   "source": [
    "# Disambiguation Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49519b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def shift_shift(text):\n",
    "    for idx,ch in enumerate(text):\n",
    "        if not ch.isalpha():\n",
    "            return idx\n",
    "\n",
    "def convert_data_for_disambiguation(data, lang):\n",
    "    # convert examples to 1 entity per example and shift if needed\n",
    "    if lang=='ar':\n",
    "        MAX_LENGTH = 600\n",
    "        MAX_OFFSET = 400\n",
    "    elif lang == 'ja':\n",
    "        MAX_LENGTH = 350\n",
    "        MAX_OFFSET = 250\n",
    "    else:\n",
    "        MAX_LENGTH = 800\n",
    "        MAX_OFFSET = 600\n",
    "    new_examples = []\n",
    "    for example in tqdm(data):\n",
    "        original_text = example['original_text']\n",
    "        for _, _, ent, _, offset, length in example['gt_entities']:\n",
    "            shift = 0\n",
    "            if len(original_text) > MAX_LENGTH and offset > MAX_OFFSET:\n",
    "                shift = (offset - MAX_OFFSET)\n",
    "                shift += shift_shift(original_text[shift:])\n",
    "            new_example = {\n",
    "                'original_text': original_text[shift:],\n",
    "                'gt_entities': [[0,0,ent,_,offset-shift,length]],\n",
    "            }\n",
    "            new_examples.append(new_example)\n",
    "    return new_examples\n",
    "\n",
    "\n",
    "def metrics_disambiguation(test_data, predictions):\n",
    "    support = 0\n",
    "    support_only_predicted = 0\n",
    "    correct = 0\n",
    "    incorrect_pos = 0\n",
    "\n",
    "    for example_idx, (example, prediction) in tqdm(enumerate(zip(test_data, predictions))):\n",
    "#         targets = {\n",
    "#             (offset,length):ent_id\n",
    "#             for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "#         }\n",
    "#         prediction = {\n",
    "#             (offset,length):ent_id\n",
    "#             for offset,length,ent_id in zip(prediction['offsets'], prediction['lengths'], prediction['entities'])\n",
    "#         }\n",
    "\n",
    "#         support += len(targets)\n",
    "#         support_only_predicted += len(prediction)\n",
    "        \n",
    "#         correct += sum(1 for pos,ent_id in prediction.items() if (pos in targets and targets[pos] == ent_id))\n",
    "#         incorrect_pos += sum(1 for pos,_ in prediction.items() if pos not in targets)\n",
    "        if len(prediction['entities']) == 0:\n",
    "            continue\n",
    "        target = example['gt_entities'][0][2]\n",
    "        prediction = prediction['entities'][0]\n",
    "        correct += (target == prediction)\n",
    "        support += 1\n",
    "\n",
    "    accuracy = correct/support\n",
    "    # accuracy_only_predicted = correct/support_only_predicted\n",
    "\n",
    "    return accuracy, support #, accuracy_only_predicted, support_only_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ta.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ar.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/fa.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/sr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/tr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/de.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/es.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ja.jsonl',\n",
    "]\n",
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    lang = test_data_path[-8:-6]\n",
    "    test_data = load_file(test_data_path)\n",
    "    test_data = convert_data_for_disambiguation(test_data[:10000], lang)\n",
    "    predictions = model_eval.get_disambiguation_predictions(test_data)\n",
    "    accuracy, support = metrics_disambiguation(test_data, predictions)\n",
    "    print(f\"Accuracty {accuracy}, support {support}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2709a",
   "metadata": {},
   "source": [
    "# Mention Detection Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "141b5ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_eval.checkpoint_path='/checkpoints/movb/bela/2023-02-15-200343/0/lightning_logs/version_127953/checkpoints/checkpoint_9.ckpt'\n",
      "model_eval.task.md_threshold=0.2\n",
      "model_eval.task.el_threshold=0.0\n",
      "Processing /fsx/movb/data/matcha/mel/test.1st.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42902it [00:06, 6937.59it/s] \n",
      "100%|██████████| 58/58 [16:44<00:00, 17.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window=0 F1 = 0.5519, precision = 0.5672, recall = 0.5373, support=233576\n",
      "Window=1 F1 = 0.0000, precision = 0.0000, recall = 0.0000, support=34205\n",
      "Window=2 F1 = 0.0000, precision = 0.0000, recall = 0.0000, support=11877\n",
      "Window=3 F1 = 0.0000, precision = 0.0000, recall = 0.0000, support=3942\n",
      "Window=4 F1 = 0.0000, precision = 0.0000, recall = 0.0000, support=859\n",
      "Window=5 F1 = 0.0000, precision = 0.0000, recall = 0.0000, support=288\n",
      "Window=6 F1 = 0.0000, precision = 0.0000, recall = 0.0000, support=168\n",
      "Window=7 F1 = 0.0000, precision = 0.0000, recall = 0.0000, support=87\n",
      "Window=8 F1 = 0.0000, precision = 0.0000, recall = 0.0000, support=23\n",
      "Window=9 F1 = 0.0000, precision = 0.0000, recall = 0.0000, support=10\n"
     ]
    }
   ],
   "source": [
    "model_eval.task.md_threshold = 0.2\n",
    "model_eval.task.el_threshold = 0.0\n",
    "print(f\"{model_eval.checkpoint_path=}\")\n",
    "print(f\"{model_eval.task.md_threshold=}\")\n",
    "print(f\"{model_eval.task.el_threshold=}\")\n",
    "datasets = [\n",
    "    # \"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\",\n",
    "    '/fsx/movb/data/matcha/mel/test.1st.txt'\n",
    "]\n",
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    test_data = load_file(test_data_path)\n",
    "    test_data = test_data\n",
    "    \n",
    "    predictions, sample_to_window_bound = get_predictions_using_windows(model_eval, test_data)\n",
    "    # print(predictions)\n",
    "    for w in range(0, 10):\n",
    "        (f1, precision, recall, support) = compute_md_scores(test_data, predictions, sample_to_window_bound, only_preds_of_window=w)\n",
    "        print(f\"Window={w} F1 = {f1:.4f}, precision = {precision:.4f}, recall = {recall:.4f}, support={support}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "810cc8ff-5ba6-414c-8758-987b61bc683d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15842997-9a76-4012-816c-97cf300997d3",
   "metadata": {},
   "source": [
    "# Rejection head ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced4dd2-be07-4f3d-8cad-b8acb0f82815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_eval.task.md_threshold = 0.05\n",
    "model_eval.task.el_threshold = 0.00\n",
    "datasets = [\n",
    "    # \"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\",\n",
    "    '/fsx/movb/data/matcha/mel/test.1st.txt'\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "]\n",
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    test_data = load_file(test_data_path)\n",
    "    #test_data = test_data[:1000]\n",
    "    \n",
    "    predictions = get_predictions_using_windows(test_data)\n",
    "    \n",
    "    \"\"\"\n",
    "    (f1, precision, recall), (f1_boe, precision_boe, recall_boe) = compute_scores(test_data, predictions, el_threshold=0.5)\n",
    "    print(f\"F1 = {f1:.4f}, precision = {precision:.4f}, recall = {recall:.4f}\")\n",
    "    print(f\"F1 boe = {f1_boe:.4f}, precision = {precision_boe:.4f}, recall = {recall_boe:.4f}\")\n",
    "    \n",
    "    print(f\"Without rejection head\")\n",
    "    (f1, precision, recall), (f1_boe, precision_boe, recall_boe) = compute_scores(test_data, predictions, el_threshold=0.0)\n",
    "    print(f\"F1 = {f1:.4f}, precision = {precision:.4f}, recall = {recall:.4f}\")\n",
    "    print(f\"F1 boe = {f1_boe:.4f}, precision = {precision_boe:.4f}, recall = {recall_boe:.4f}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    el_thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for el_threshold in el_thresholds:\n",
    "        _, (_, precision_boe, recall_boe) = compute_scores(test_data, predictions, el_threshold=el_threshold)\n",
    "        precision.append(precision_boe)\n",
    "        recall.append(recall_boe)\n",
    "        \n",
    "    plt.plot(recall, precision)\n",
    "    for x,y, th in zip(recall[::10], precision[::10], el_thresholds[::10]): \n",
    "        plt.text(x, y, f'{th:.2f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11594ae-68a2-45f5-a4ed-3386de26703d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum([len(e['gt_entities']) for e in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930a292",
   "metadata": {},
   "source": [
    "# Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_samples(samples, batch_size):\n",
    "    # Yield batches of samples\n",
    "    for i in range(0, len(samples), batch_size):\n",
    "        yield samples[i : i + batch_size]\n",
    "\n",
    "\n",
    "texts = [sample[\"original_text\"] for sample in load_file(\"/fsx/movb/data/matcha/mewsli-9/en.jsonl\")]\n",
    "batch_size = 1024\n",
    "print(f\"Processing {len(texts)} texts, {batch_size=}, {model_eval.transform.max_seq_len=}\")\n",
    "%time _ = [model_eval.process_batch(batch_texts) for batch_texts in tqdm(batch_samples(texts, batch_size), desc=\"Inference\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff17084",
   "metadata": {},
   "source": [
    "## Eyeball Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced14ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Taylor Swift lives in New York City. New York City is a city in the United States.\"\n",
    "prediction = model_eval.process_batch([text])[0]\n",
    "predicted_entities = [PredictedEntity(offset, length, text, entity_id, md_score, el_score) for offset, length, entity_id, md_score, el_score in zip(prediction[\"offsets\"], prediction[\"lengths\"], prediction[\"entities\"], prediction[\"md_scores\"], prediction[\"el_scores\"])]\n",
    "print_sample(text, [], predicted_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ac96a-b29e-45c3-9ab1-43ddcd387572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_path = \"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\"\n",
    "print(f\"Processing {test_data_path}\")\n",
    "test_data = load_file(test_data_path)\n",
    "sample = test_data[200]\n",
    "prediction = get_predictions_using_windows(model_eval, [sample])[0]\n",
    "text = sample[\"original_text\"]\n",
    "max_length = 1024\n",
    "\n",
    "ground_truth_entities = [GroundTruthEntity(offset, length, text, entity_id) for _, _, entity_id, _, offset, length in sample[\"gt_entities\"]]\n",
    "predicted_entities = [PredictedEntity(offset, length, text, entity_id, md_score, el_score) for offset, length, entity_id, md_score, el_score in zip(prediction[\"offsets\"], prediction[\"lengths\"], prediction[\"entities\"], prediction[\"md_scores\"], prediction[\"el_scores\"])]\n",
    "print_sample(text, ground_truth_entities, predicted_entities, max_display_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e7307",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "35dd729434122dacb96674391113d1d3e10628e04c33485b220c6b43d303a630"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
