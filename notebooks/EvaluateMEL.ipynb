{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff95f192-5b1e-4caf-a5cc-3290b0aebf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a100-st-p4d24xlarge-18\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yaml\n",
    "from hydra.experimental import compose, initialize_config_module\n",
    "import hydra\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import faiss\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "from bela.transforms.spm_transform import SPMTransform\n",
    "from bela.evaluation.model_eval import ModelEval, load_file\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "!cat /etc/hostname  # Double check that we are on a gpu node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db8ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_transform = SPMTransform(max_seq_len=100000)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GroundTruthEntity:\n",
    "    offset: int\n",
    "    length: int\n",
    "    text: str\n",
    "    entity_id: str\n",
    "\n",
    "    @property\n",
    "    def mention(self):\n",
    "        return self.text[self.offset : self.offset + self.length]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"mention={self.mention} -> entity_id={self.entity_id}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PredictedEntity:\n",
    "    offset: int\n",
    "    length: int\n",
    "    text: str\n",
    "    entity_id: str\n",
    "    md_score: float\n",
    "    el_score: float\n",
    "\n",
    "    @property\n",
    "    def mention(self):\n",
    "        return self.text[self.offset : self.offset + self.length]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"mention={self.mention} -> entity_id={self.entity_id} (md_score={self.md_score:.2f}, el_score={self.el_score:.2f})\"\n",
    "\n",
    "\n",
    "def print_sample(text: str, ground_truth_entities: List[GroundTruthEntity], predicted_entities: List[PredictedEntity], max_display_length=1000):\n",
    "    print(f\"{text[:max_display_length]=}\")\n",
    "    print(\"***************** Ground truth entities *****************\")\n",
    "    print(f\"{len(ground_truth_entities)=}\")\n",
    "    for ground_truth_entity in ground_truth_entities:\n",
    "        if ground_truth_entity.offset + ground_truth_entity.length > max_display_length:\n",
    "            continue\n",
    "        print(ground_truth_entity)\n",
    "    print(\"***************** Predicted entities *****************\")\n",
    "    print(f\"{len(predicted_entities)=}\")\n",
    "    for predicted_entity in predicted_entities:\n",
    "        if predicted_entity.offset + predicted_entity.length > max_display_length:\n",
    "            continue\n",
    "        print(predicted_entity)\n",
    "\n",
    "\n",
    "def get_windows(text):\n",
    "    tokens = sp_transform([text])[0]\n",
    "    tokens = tokens[1:-1]\n",
    "    window_length = 254\n",
    "    windows = []\n",
    "    for window_start in range(0,len(tokens),window_length//2):\n",
    "        start_pos = tokens[window_start][1]\n",
    "        if window_start + window_length >= len(tokens):\n",
    "            end_pos = tokens[-1][2]\n",
    "        else:\n",
    "            end_pos = tokens[window_start + window_length][2]\n",
    "        windows.append((start_pos, end_pos))\n",
    "    return windows\n",
    "\n",
    "    \n",
    "def convert_predictions_to_dict(example_predictions):\n",
    "    predictions = []\n",
    "    if len(example_predictions) > 0:\n",
    "        offsets, lengths, entities, md_scores, el_scores = zip(*example_predictions) \n",
    "    else:\n",
    "        offsets, lengths, entities, md_scores, el_scores = [], [], [], [], []\n",
    "    return {\n",
    "        'offsets': offsets,\n",
    "        'lengths': lengths,\n",
    "        'entities': entities,\n",
    "        'md_scores': md_scores,\n",
    "        'el_scores': el_scores,\n",
    "    }\n",
    "\n",
    "\n",
    "def group_predictions_by_example(all_predictions, extended_examples):\n",
    "    grouped_predictions = defaultdict(list)\n",
    "    for prediction, extended_example in zip(all_predictions, extended_examples):\n",
    "        window_start = extended_example['window_start']\n",
    "        prediction = dict(prediction)\n",
    "        prediction['offsets'] = [offset + window_start for offset in prediction['offsets']]\n",
    "        grouped_predictions[extended_example['document_id']].append((\n",
    "            prediction\n",
    "        ))\n",
    "    \n",
    "    predictions = {}\n",
    "    for document_id, example_prediction_list in grouped_predictions.items():\n",
    "        example_predictions = []\n",
    "        for prediction in example_prediction_list:\n",
    "            for offset,length,ent,md_score,el_score in zip(\n",
    "                prediction['offsets'],\n",
    "                prediction['lengths'],\n",
    "                prediction['entities'],\n",
    "                prediction['md_scores'],\n",
    "                prediction['el_scores'],\n",
    "            ):\n",
    "                example_predictions.append((offset,length,ent,md_score,el_score))\n",
    "                example_predictions = sorted(example_predictions)\n",
    "        predictions[document_id] = example_predictions\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "    \n",
    "def group_predictions_by_example(all_predictions, extended_examples):\n",
    "    grouped_predictions = defaultdict(list)\n",
    "    for prediction, extended_example in zip(all_predictions, extended_examples):\n",
    "        window_start = extended_example['window_start']\n",
    "        prediction = dict(prediction)\n",
    "        prediction['offsets'] = [offset + window_start for offset in prediction['offsets']]\n",
    "        grouped_predictions[extended_example['document_id']].append((\n",
    "            prediction\n",
    "        ))\n",
    "    \n",
    "    predictions = {}\n",
    "    for document_id, example_prediction_list in grouped_predictions.items():\n",
    "        example_predictions = []\n",
    "        for prediction in example_prediction_list:\n",
    "            for offset,length,ent,md_score,el_score in zip(\n",
    "                prediction['offsets'],\n",
    "                prediction['lengths'],\n",
    "                prediction['entities'],\n",
    "                prediction['md_scores'],\n",
    "                prediction['el_scores'],\n",
    "            ):\n",
    "                example_predictions.append((offset,length,ent,md_score,el_score))\n",
    "                example_predictions = sorted(example_predictions)\n",
    "        predictions[document_id] = example_predictions\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def merge_predictions(example_predictions):\n",
    "    filtered_example_predictions = []\n",
    "\n",
    "    current_end = None\n",
    "    current_offset = None\n",
    "    current_length = None\n",
    "    current_ent_id = None\n",
    "    current_md_score = None\n",
    "    current_el_score = None\n",
    "\n",
    "    for offset,length,ent_id,md_score,el_score in example_predictions:\n",
    "        if current_end is None:\n",
    "            current_end = offset + length\n",
    "            current_offset = offset\n",
    "            current_length = length\n",
    "            current_ent_id = ent_id\n",
    "            current_md_score = md_score\n",
    "            current_el_score = el_score\n",
    "            continue\n",
    "\n",
    "        if offset < current_end:\n",
    "            # intersection of two predictions\n",
    "            if md_score > current_md_score:\n",
    "                current_ent_id = ent_id\n",
    "                current_offset = offset\n",
    "                current_length = length\n",
    "                current_md_score = md_score\n",
    "                current_el_score = el_score\n",
    "        else:\n",
    "            filtered_example_predictions.append((\n",
    "                current_offset,\n",
    "                current_length,\n",
    "                current_ent_id,\n",
    "                current_md_score,\n",
    "                current_el_score\n",
    "            ))\n",
    "            current_ent_id = ent_id\n",
    "            current_offset = offset\n",
    "            current_length = length\n",
    "            current_md_score = md_score\n",
    "            current_el_score = el_score\n",
    "\n",
    "        current_end = offset+length\n",
    "\n",
    "    if current_offset is not None:\n",
    "        filtered_example_predictions.append((\n",
    "            current_offset,\n",
    "            current_length,\n",
    "            current_ent_id,\n",
    "            current_md_score,\n",
    "            current_el_score\n",
    "        ))\n",
    "    \n",
    "    return filtered_example_predictions\n",
    "\n",
    "def compute_scores(data, predictions, md_threshold=0.2, el_threshold=0.05):\n",
    "    tp, fp, support = 0, 0, 0\n",
    "    tp_boe, fp_boe, support_boe = 0, 0, 0\n",
    "\n",
    "    predictions_per_example = []\n",
    "    for example, example_predictions in zip(data, predictions):\n",
    "\n",
    "        example_targets = {\n",
    "            (offset,length):ent_id\n",
    "            for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "        }\n",
    "\n",
    "        example_predictions = {\n",
    "            (offset, length):ent_id\n",
    "            for offset, length, ent_id, md_score, el_score in zip(\n",
    "                example_predictions['offsets'],\n",
    "                example_predictions['lengths'],\n",
    "                example_predictions['entities'],\n",
    "                example_predictions['md_scores'],\n",
    "                example_predictions['el_scores'],\n",
    "            )\n",
    "            if (el_score > el_threshold and md_score > md_threshold) \n",
    "        }\n",
    "\n",
    "        predictions_per_example.append((len(example_targets), len(example_predictions)))\n",
    "\n",
    "        for pos, ent in example_targets.items():\n",
    "            support += 1\n",
    "            if pos in example_predictions and example_predictions[pos] == ent:\n",
    "                tp += 1\n",
    "        for pos, ent in example_predictions.items():\n",
    "            if pos not in example_targets or example_targets[pos] != ent:\n",
    "                fp += 1\n",
    "\n",
    "        example_targets_set = set(example_targets.values())\n",
    "        example_predictions_set = set(example_predictions.values())\n",
    "\n",
    "        for ent in example_targets_set:\n",
    "            support_boe += 1\n",
    "            if ent in example_predictions_set:\n",
    "                tp_boe += 1\n",
    "        for ent in example_predictions_set:\n",
    "            if ent not in example_targets_set:\n",
    "                fp_boe += 1\n",
    "\n",
    "    def safe_division(a, b):\n",
    "        if b == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return a / b\n",
    "\n",
    "\n",
    "    def compute_f1_p_r(tp, fp, fn):\n",
    "        precision = safe_division(tp, (tp + fp))\n",
    "        recall = safe_division(tp, (tp + fn))\n",
    "        f1 = safe_division(2 * tp, (2 * tp + fp + fn))\n",
    "        return f1, precision, recall\n",
    "\n",
    "    fn = support - tp\n",
    "    fn_boe = support_boe - tp_boe\n",
    "    return compute_f1_p_r(tp, fp, fn), compute_f1_p_r(tp_boe, fp_boe, fn_boe)\n",
    "\n",
    "def correct_mention_offsets(example_predictions, text):\n",
    "    TEXT_SEPARATORS = [' ','.',',','!','?','-','\\n']\n",
    "    corrected_example_predictions = []\n",
    "    for offset,length,ent_id,md_score,el_score in example_predictions:\n",
    "        while offset !=0 and offset<len(text) and (text[offset-1] not in TEXT_SEPARATORS or text[offset] in TEXT_SEPARATORS):\n",
    "            offset += 1\n",
    "            length -= 1\n",
    "        while offset+length < len(text) and text[offset+length] not in TEXT_SEPARATORS:\n",
    "            length += 1\n",
    "        corrected_example_predictions.append((\n",
    "            offset,length,ent_id,md_score,el_score\n",
    "        ))\n",
    "    return corrected_example_predictions\n",
    "\n",
    "\n",
    "def get_predictions_using_windows(test_data, batch_size=1024):\n",
    "    extended_examples = []\n",
    "\n",
    "    for example in test_data:\n",
    "        text = example['original_text']\n",
    "        windows = get_windows(text)\n",
    "        for idx, (start_pos, end_pos) in enumerate(windows):\n",
    "            new_text = text[start_pos:end_pos]\n",
    "            extended_examples.append({\n",
    "                'document_id': example['document_id'],\n",
    "                'original_text': new_text,\n",
    "                'gt_entities': example['gt_entities'],\n",
    "                'window_idx': idx,\n",
    "                'window_start': start_pos,\n",
    "                'window_end': end_pos,\n",
    "            })\n",
    "\n",
    "    all_predictions = model_eval.get_predictions(extended_examples, batch_size=batch_size)\n",
    "    predictions_dict = group_predictions_by_example(all_predictions, extended_examples)\n",
    "\n",
    "    predictions = []\n",
    "    for example in test_data:\n",
    "        document_id = example['document_id']\n",
    "        text = example['original_text']\n",
    "        example_predictions = predictions_dict[document_id]\n",
    "        example_predictions = merge_predictions(example_predictions)\n",
    "        example_predictions = correct_mention_offsets(example_predictions, text)\n",
    "        example_predictions = convert_predictions_to_dict(example_predictions)\n",
    "        predictions.append(example_predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def shift_shift(text):\n",
    "    for idx,ch in enumerate(text):\n",
    "        if not ch.isalpha():\n",
    "            return idx\n",
    "\n",
    "def convert_data_for_disambiguation(data, lang):\n",
    "    # convert examples to 1 entity per example and shift if needed\n",
    "    if lang=='ar':\n",
    "        MAX_LENGTH = 600\n",
    "        MAX_OFFSET = 400\n",
    "    elif lang == 'ja':\n",
    "        MAX_LENGTH = 350\n",
    "        MAX_OFFSET = 250\n",
    "    else:\n",
    "        MAX_LENGTH = 800\n",
    "        MAX_OFFSET = 600\n",
    "    new_examples = []\n",
    "    for example in tqdm(data):\n",
    "        original_text = example['original_text']\n",
    "        for _, _, ent, _, offset, length in example['gt_entities']:\n",
    "            shift = 0\n",
    "            if len(original_text) > MAX_LENGTH and offset > MAX_OFFSET:\n",
    "                shift = (offset - MAX_OFFSET)\n",
    "                shift += shift_shift(original_text[shift:])\n",
    "            new_example = {\n",
    "                'original_text': original_text[shift:],\n",
    "                'gt_entities': [[0,0,ent,_,offset-shift,length]],\n",
    "            }\n",
    "            new_examples.append(new_example)\n",
    "    return new_examples\n",
    "\n",
    "\n",
    "def metrics_disambiguation(test_data, predictions):\n",
    "    support = 0\n",
    "    support_only_predicted = 0\n",
    "    correct = 0\n",
    "    incorrect_pos = 0\n",
    "\n",
    "    for example_idx, (example, prediction) in tqdm(enumerate(zip(test_data, predictions))):\n",
    "#         targets = {\n",
    "#             (offset,length):ent_id\n",
    "#             for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "#         }\n",
    "#         prediction = {\n",
    "#             (offset,length):ent_id\n",
    "#             for offset,length,ent_id in zip(prediction['offsets'], prediction['lengths'], prediction['entities'])\n",
    "#         }\n",
    "\n",
    "#         support += len(targets)\n",
    "#         support_only_predicted += len(prediction)\n",
    "        \n",
    "#         correct += sum(1 for pos,ent_id in prediction.items() if (pos in targets and targets[pos] == ent_id))\n",
    "#         incorrect_pos += sum(1 for pos,_ in prediction.items() if pos not in targets)\n",
    "        if len(prediction['entities']) == 0:\n",
    "            continue\n",
    "        target = example['gt_entities'][0][2]\n",
    "        prediction = prediction['entities'][0]\n",
    "        correct += (target == prediction)\n",
    "        support += 1\n",
    "\n",
    "    accuracy = correct/support\n",
    "    # accuracy_only_predicted = correct/support_only_predicted\n",
    "\n",
    "    return accuracy, support #, accuracy_only_predicted, support_only_predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f8f2dc7",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf93bf95-295e-4efd-8133-bd9aedf156d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         170161623 function calls (170084496 primitive calls) in 175.521 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 6808 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       67   78.354    1.169   78.358    1.170 serialization.py:994(load_tensor)\n",
      " 44771119   23.901    0.000   23.901    0.000 {method 'readline' of 'mmap.mmap' objects}\n",
      "        3   21.949    7.316   51.159   17.053 joint_el_datamodule.py:47(__init__)\n",
      "        1   12.414   12.414   12.414   12.414 {built-in method faiss._swigfaiss.GpuIndexFlat_add}\n",
      "        1    9.625    9.625   10.698   10.698 joint_el_datamodule.py:21(__init__)\n",
      "        1    4.735    4.735    4.735    4.735 {built-in method faiss._swigfaiss.new_GpuIndexFlatIP}\n",
      " 61313958    3.610    0.000    3.610    0.000 {method 'append' of 'list' objects}\n",
      " 44771116    2.604    0.000    2.604    0.000 {method 'tell' of 'mmap.mmap' objects}\n",
      "        1    2.340    2.340  175.453  175.453 model_eval.py:60(__init__)\n",
      "      302    2.033    0.007    2.033    0.007 {method 'uniform_' of 'torch._C._TensorBase' objects}\n",
      "        4    1.769    0.442    1.769    0.442 {method 'normal_' of 'torch._C._TensorBase' objects}\n",
      "     1369    1.734    0.001    1.734    0.001 {built-in method io.open_code}\n",
      "      396    1.149    0.003    1.149    0.003 {method '_set_from_file' of 'torch._C.StorageBase' objects}\n",
      "        1    1.036    1.036    1.036    1.036 {built-in method torch._C._cuda_init}\n",
      " 16471124    1.032    0.000    1.032    0.000 {method 'strip' of 'str' objects}\n",
      "      407    0.912    0.002    1.956    0.005 {method 'to' of 'torch._C._TensorBase' objects}\n",
      "     1367    0.875    0.001    0.875    0.001 {built-in method marshal.loads}\n",
      "     9096    0.741    0.000    0.741    0.000 {built-in method posix.stat}\n",
      "     1203    0.671    0.001    0.671    0.001 {method 'copy_' of 'torch._C._TensorBase' objects}\n",
      "     1369    0.515    0.000    2.261    0.002 <frozen importlib._bootstrap_external>:1029(get_data)"
     ]
    }
   ],
   "source": [
    "%%prun -s tottime -l 20\n",
    "\n",
    "# e2e model with isotropic embeddings\n",
    "checkpoint_path = '/checkpoints/movb/bela/2023-01-13-023711/0/lightning_logs/version_4144/checkpoints/last.ckpt'  # Not working: Unexpected key(s) in state_dict: \"saliency_encoder.mlp.0.weight\", \"saliency_encoder.mlp.0.bias\", \"saliency_encoder.mlp.3.weight\", \"saliency_encoder.mlp.3.bias\", \"saliency_encoder.mlp.6.weight\", \"saliency_encoder.mlp.6.bias\". \n",
    "checkpoint_path = '/checkpoints/movb/bela/2022-11-27-225013/0/lightning_logs/version_286287/checkpoints/last_15000.ckpt'  # Works but 0 F1\n",
    "\n",
    "# E2E checkpoint with new embeddings\n",
    "# https://fb.quip.com/QVUxA4UcAZ7k#temp:C:OcG977f71fab43d42379521a0dff\n",
    "# Works but give 0 F1 on tackbp (mention detection is okish, but entity disambiguation is random)\n",
    "checkpoint_path = '/checkpoints/movb/bela/2023-01-18-220105/0/lightning_logs/version_4820/checkpoints/last.ckpt'  \n",
    "#model_eval = ModelEval(checkpoint_path, config_name=\"joint_el_mel_new\")\n",
    "model_eval = ModelEval(checkpoint_path, config_name=\"joint_el_mel_new_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0516d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set thresholds\n",
    "model_eval.task.md_threshold = 0.05\n",
    "model_eval.task.el_threshold = 0.05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9df37725",
   "metadata": {},
   "source": [
    "# End-to-end Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000fd83c-fd03-4e8a-a3da-d8df70477d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_eval.checkpoint_path='/checkpoints/movb/bela/2023-01-18-220105/0/lightning_logs/version_4820/checkpoints/last.ckpt'\n",
      "model_eval.task.md_threshold=0.2\n",
      "model_eval.task.el_threshold=0.4\n",
      "Processing /fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "269it [00:00, 5836.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 4/4 [01:20<00:00, 20.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0040, precision = 0.0103, recall = 0.0025\n",
      "F1 boe = 0.2744, precision = 0.3104, recall = 0.2460\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/en.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12679it [00:02, 5051.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 5/5 [01:29<00:00, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0058, precision = 0.0049, recall = 0.0071\n",
      "F1 boe = 0.4924, precision = 0.4392, recall = 0.5604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model_eval.checkpoint_path=}\")\n",
    "print(f\"{model_eval.task.md_threshold=}\")\n",
    "print(f\"{model_eval.task.el_threshold=}\")\n",
    "datasets = [\n",
    "    \"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\",\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ta.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ar.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/fa.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/sr.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/tr.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/de.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/es.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ja.jsonl',\n",
    "]\n",
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    test_data = load_file(test_data_path)\n",
    "    test_data = test_data[:1000]\n",
    "    \n",
    "    predictions = get_predictions_using_windows(test_data)\n",
    "    (f1, precision, recall), (f1_boe, precision_boe, recall_boe) = compute_scores(test_data, predictions)\n",
    "    \n",
    "    print(f\"F1 = {f1:.4f}, precision = {precision:.4f}, recall = {recall:.4f}\")\n",
    "    print(f\"F1 boe = {f1_boe:.4f}, precision = {precision_boe:.4f}, recall = {recall_boe:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6da72624",
   "metadata": {},
   "source": [
    "# Disambiguation Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ta.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ar.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/fa.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/sr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/tr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/de.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/es.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ja.jsonl',\n",
    "]\n",
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    lang = test_data_path[-8:-6]\n",
    "    test_data = load_file(test_data_path)\n",
    "    test_data = convert_data_for_disambiguation(test_data[:10000], lang)\n",
    "    predictions = model_eval.get_disambiguation_predictions(test_data)\n",
    "    accuracy, support = metrics_disambiguation(test_data, predictions)\n",
    "    print(f\"Accuracty {accuracy}, support {support}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2930a292",
   "metadata": {},
   "source": [
    "# Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4388e8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12679it [00:00, 70671.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 12679 texts, batch_size=1024, model_eval.transform.max_seq_len=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [04:00, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 50s, sys: 35.3 s, total: 12min 25s\n",
      "Wall time: 4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_samples(samples, batch_size):\n",
    "    # Yield batches of samples\n",
    "    for i in range(0, len(samples), batch_size):\n",
    "        yield samples[i : i + batch_size]\n",
    "\n",
    "\n",
    "texts = [sample[\"original_text\"] for sample in load_file(\"/fsx/movb/data/matcha/mewsli-9/en.jsonl\")]\n",
    "batch_size = 1024\n",
    "print(f\"Processing {len(texts)} texts, {batch_size=}, {model_eval.transform.max_seq_len=}\")\n",
    "%time _ = [model_eval.process_batch(batch_texts) for batch_texts in tqdm(batch_samples(texts, batch_size), desc=\"Inference\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eff17084",
   "metadata": {},
   "source": [
    "## Debug some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced14ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text[:max_display_length]='Taylor Swift lives in New York City. New York City is a city in the United States.'\n",
      "***************** Ground truth entities *****************\n",
      "***************** Predicted entities *****************\n",
      "mention=Taylor Swift lives -> entity_id=Q2555292 (md_score=0.58, el_score=0.02)\n",
      "mention=York City. New -> entity_id=Q16452508 (md_score=0.41, el_score=0.00)\n"
     ]
    }
   ],
   "source": [
    "text = \"Taylor Swift lives in New York City. New York City is a city in the United States.\"\n",
    "prediction = model_eval.process_batch([text])[0]\n",
    "predicted_entities = [PredictedEntity(offset, length, text, entity_id, md_score, el_score) for offset, length, entity_id, md_score, el_score in zip(prediction[\"offsets\"], prediction[\"lengths\"], prediction[\"entities\"], prediction[\"md_scores\"], prediction[\"el_scores\"])]\n",
    "print_sample(text, [], predicted_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a77ac96a-b29e-45c3-9ab1-43ddcd387572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "269it [00:00, 8680.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text[:max_display_length]='<DOC id=\"ENG_NW_001048_20150228_F0000001S\"> <SOURCE>http://www.telegraph.co.uk/news/worldnews/europe/russia/11441729/David-Cameron-says-callous-murder-of-Boris-Nemtsov-must-be-rapidly-investigated.html</SOURCE> <DATE_TIME>2015-02-28T00:00:00</DATE_TIME> <HEADLINE> David Cameron says \\'callous murder\\' of Boris Nemtsov must be rapidly investigated </HEADLINE> <TEXT> <P> Prime Minister says \"callous murder\" of Russian opposition politician \"must be fully, rapidly and transparently investigated, and those responsible brought to justice\" </P> <P> David Cameron has said he is \"shocked and sickened\" by the murder of Boris Nemtsov. </P> <P> The Prime Minister said the \"callous\" killing of the Russian opposition politician \"must be fully, rapidly and transparently investigated, and those responsible brought to justice\". </P> <P> Mr Nemtsov, a leading critic of president Vladimir Putin and of the war in Ukraine, was gunned down near the Kremlin on the eve of a major rally in Moscow. </P> <P> “I a'\n",
      "***************** Ground truth entities *****************\n",
      "mention=Vladimir Putin -> entity_id=Q7747\n",
      "mention=Ukraine -> entity_id=Q212\n",
      "mention=Kremlin -> entity_id=Q133274\n",
      "mention=Moscow -> entity_id=Q649\n",
      "mention=Russian -> entity_id=Q159\n",
      "mention=Russian -> entity_id=Q159\n",
      "mention=David Cameron -> entity_id=Q192\n",
      "mention=David-Cameron -> entity_id=Q192\n",
      "mention=David Cameron -> entity_id=Q192\n",
      "mention=Prime Minister -> entity_id=Q192\n",
      "mention=Prime Minister -> entity_id=Q192\n",
      "mention=politician -> entity_id=Q363846\n",
      "mention=Boris Nemtsov -> entity_id=Q363846\n",
      "mention=politician -> entity_id=Q363846\n",
      "mention=Nemtsov -> entity_id=Q363846\n",
      "mention=Boris-Nemtsov -> entity_id=Q363846\n",
      "mention=Boris Nemtsov -> entity_id=Q363846\n",
      "***************** Predicted entities *****************\n",
      "mention=Boris Nemtsov must -> entity_id=Q31519442 (md_score=0.10, el_score=0.01)\n",
      "mention=David Cameron has -> entity_id=Q12658993 (md_score=0.52, el_score=0.05)\n",
      "mention=Boris Nemtsov. -> entity_id=Q18438075 (md_score=0.12, el_score=0.00)\n",
      "mention=Vladimir Putin and -> entity_id=Q49867013 (md_score=0.66, el_score=0.02)\n",
      "mention=Ukraine, was -> entity_id=Q12084495 (md_score=0.09, el_score=0.00)\n",
      "mention=Kremlin on -> entity_id=Q2460158 (md_score=0.35, el_score=0.02)\n",
      "mention=Moscow. -> entity_id=Q2460158 (md_score=0.10, el_score=0.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data_path = \"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\"\n",
    "print(f\"Processing {test_data_path}\")\n",
    "test_data = load_file(test_data_path)\n",
    "sample = test_data[200]\n",
    "prediction = get_predictions_using_windows([sample])[0]\n",
    "text = sample[\"original_text\"]\n",
    "max_length = 1024\n",
    "\n",
    "ground_truth_entities = [GroundTruthEntity(offset, length, text, entity_id) for _, _, entity_id, _, offset, length in sample[\"gt_entities\"]]\n",
    "predicted_entities = [PredictedEntity(offset, length, text, entity_id, md_score, el_score) for offset, length, entity_id, md_score, el_score in zip(prediction[\"offsets\"], prediction[\"lengths\"], prediction[\"entities\"], prediction[\"md_scores\"], prediction[\"el_scores\"])]\n",
    "print_sample(text, ground_truth_entities, predicted_entities, max_display_length=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e9e7307",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "235bcda9aabf5f8363267684c8e0d7a57e9fb12569fe58ae215aef35e2f0a58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
