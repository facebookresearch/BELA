{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff95f192-5b1e-4caf-a5cc-3290b0aebf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a100-st-p4d24xlarge-91\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yaml\n",
    "from hydra.experimental import compose, initialize_config_module\n",
    "import hydra\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import faiss\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "from bela.transforms.spm_transform import SPMTransform\n",
    "from bela.evaluation.model_eval import ModelEval, load_file\n",
    "from bela.utils.prediction_utils import get_predictions_using_windows\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "!cat /etc/hostname  # Double check that we are on a gpu node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2db8ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GroundTruthEntity:\n",
    "    offset: int\n",
    "    length: int\n",
    "    text: str\n",
    "    entity_id: str\n",
    "\n",
    "    @property\n",
    "    def mention(self):\n",
    "        return self.text[self.offset : self.offset + self.length]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"mention=\\\"{self.mention}\\\" -> entity_id={self.entity_id}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.offset == other.offset and self.length == other.length and self.entity_id == other.entity_id\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PredictedEntity:\n",
    "    offset: int\n",
    "    length: int\n",
    "    text: str\n",
    "    entity_id: str\n",
    "    md_score: float\n",
    "    el_score: float\n",
    "\n",
    "    @property\n",
    "    def mention(self):\n",
    "        return self.text[self.offset : self.offset + self.length]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"mention=\\\"{self.mention}\\\" -> entity_id={self.entity_id} (md_score={self.md_score:.2f}, el_score={self.el_score:.2f})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.offset == other.offset and self.length == other.length and self.entity_id == other.entity_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_sample(text: str, ground_truth_entities: List[GroundTruthEntity], predicted_entities: List[PredictedEntity], max_display_length=1000):\n",
    "    print(f\"{text[:max_display_length]=}\")\n",
    "    print(\"***************** Ground truth entities *****************\")\n",
    "    print(f\"{len(ground_truth_entities)=}\")\n",
    "    for ground_truth_entity in ground_truth_entities:\n",
    "        if ground_truth_entity.offset + ground_truth_entity.length > max_display_length:\n",
    "            continue\n",
    "        print(ground_truth_entity)\n",
    "    print(\"***************** Predicted entities *****************\")\n",
    "    print(f\"{len(predicted_entities)=}\")\n",
    "    for predicted_entity in predicted_entities:\n",
    "        if predicted_entity.offset + predicted_entity.length > max_display_length:\n",
    "            continue\n",
    "        print(predicted_entity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f8f2dc7",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf93bf95-295e-4efd-8133-bd9aedf156d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path='bela.transforms.joint_el_transform.JointELXlmrRawTextTransform', mod='bela.transforms.joint_el_transform'\n",
      "path='bela.datamodule.joint_el_datamodule.JointELDataModule', mod='bela.datamodule'\n",
      "path='bela.datamodule.joint_el_datamodule.JointELDataModule', mod='bela.datamodule.joint_el_datamodule'\n",
      "path='bela.task.joint_el_task.JointELTask', mod='bela.task'\n",
      "path='bela.task.joint_el_task.JointELTask', mod='bela.task.joint_el_task'\n",
      "path='bela.models.hf_encoder.HFEncoder', mod='bela.models'\n",
      "path='bela.models.hf_encoder.HFEncoder', mod='bela.models.hf_encoder'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         170163130 function calls (170086003 primitive calls) in 149.769 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 6809 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      " 44771119   38.025    0.000   38.025    0.000 {method 'readline' of 'mmap.mmap' objects}\n",
      "        3   29.437    9.812   74.120   24.707 joint_el_datamodule.py:47(__init__)\n",
      "        5   15.115    3.023   15.116    3.023 serialization.py:994(load_tensor)\n",
      "        1   13.817   13.817   13.817   13.817 {built-in method faiss._swigfaiss.GpuIndexFlat_add}\n",
      "        1   11.451   11.451   12.661   12.661 joint_el_datamodule.py:21(__init__)\n",
      "        1    4.650    4.650  149.773  149.773 <string>:2(<module>)\n",
      " 61314008    4.082    0.000    4.082    0.000 {method 'append' of 'list' objects}\n",
      "        1    3.884    3.884    3.884    3.884 {built-in method faiss._swigfaiss.new_GpuIndexFlatIP}\n",
      " 44771116    3.524    0.000    3.524    0.000 {method 'tell' of 'mmap.mmap' objects}\n",
      "        1    2.969    2.969  145.123  145.123 model_eval.py:61(__init__)"
     ]
    }
   ],
   "source": [
    "%%prun -s tottime -l 10\n",
    "\n",
    "# e2e model with isotropic embeddings\n",
    "checkpoint_path = '/checkpoints/movb/bela/2023-01-13-023711/0/lightning_logs/version_4144/checkpoints/last.ckpt'  # Not working: Unexpected key(s) in state_dict: \"saliency_encoder.mlp.0.weight\", \"saliency_encoder.mlp.0.bias\", \"saliency_encoder.mlp.3.weight\", \"saliency_encoder.mlp.3.bias\", \"saliency_encoder.mlp.6.weight\", \"saliency_encoder.mlp.6.bias\". \n",
    "checkpoint_path = '/checkpoints/movb/bela/2022-11-27-225013/0/lightning_logs/version_286287/checkpoints/last_15000.ckpt'  # Works but 0 F1\n",
    "\n",
    "# E2E checkpoint with new embeddings\n",
    "# https://fb.quip.com/QVUxA4UcAZ7k#temp:C:OcG977f71fab43d42379521a0dff\n",
    "# Works but give 0 F1 on tackbp (mention detection is okish, but entity disambiguation is random)\n",
    "checkpoint_path = '/checkpoints/movb/bela/2023-01-18-220105/0/lightning_logs/version_4820/checkpoints/last.ckpt'  \n",
    "# Overfit on one sample from /fsx/louismartin/bela/data/debug_mention_detection/mel/train.1st.1_sample.txt\n",
    "checkpoint_path = '/data/home/louismartin/dev/BELA/multirun/2023-02-14/08-45-13/0/lightning_logs/version_0/checkpoints/checkpoint_best.ckpt'\n",
    "model_eval = ModelEval(checkpoint_path, config_name=\"joint_el_mel_new_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0516d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set low thresholds\n",
    "model_eval.task.md_threshold = 0.01\n",
    "model_eval.task.el_threshold = 0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9df37725",
   "metadata": {},
   "source": [
    "# End-to-end Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "000fd83c-fd03-4e8a-a3da-d8df70477d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_eval.checkpoint_path='/data/home/louismartin/dev/BELA/multirun/2023-02-14/08-45-13/0/lightning_logs/version_0/checkpoints/checkpoint_best.ckpt'\n",
      "model_eval.task.md_threshold=0.2\n",
      "model_eval.task.el_threshold=0.4\n",
      "Processing /fsx/louismartin/bela/data/debug_mention_detection/mel/eval.1st.1_sample.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [00:00, 30116.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0000, precision = 0.0000, recall = 0.0000\n",
      "F1 boe = 0.7179, precision = 0.8750, recall = 0.6087\n"
     ]
    }
   ],
   "source": [
    "model_eval.task.md_threshold = 0.2\n",
    "model_eval.task.el_threshold = 0.4\n",
    "print(f\"{model_eval.checkpoint_path=}\")\n",
    "print(f\"{model_eval.task.md_threshold=}\")\n",
    "print(f\"{model_eval.task.el_threshold=}\")\n",
    "datasets = [\n",
    "    #\"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\",\n",
    "    \"/fsx/louismartin/bela/data/debug_mention_detection/mel/eval.1st.1_sample.txt\",  # Overfit on one sample\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ta.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ar.jsonl',\n",
    "    ##'/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/fa.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/sr.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/tr.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/de.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/es.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ja.jsonl',\n",
    "]\n",
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    test_data = load_file(test_data_path)\n",
    "    test_data = test_data[:100]\n",
    "    \n",
    "    predictions = get_predictions_using_windows(model_eval, test_data)\n",
    "    (f1, precision, recall), (f1_boe, precision_boe, recall_boe) = ModelEval.compute_scores(test_data, predictions)\n",
    "    #model_results = ModelResults(test_data, predictions)\n",
    "    #(f1, precision, recall), (f1_boe, precision_boe, recall_boe) = model_results.compute_scores()\n",
    "    \n",
    "    print(f\"F1 = {f1:.4f}, precision = {precision:.4f}, recall = {recall:.4f}\")\n",
    "    print(f\"F1 boe = {f1_boe:.4f}, precision = {precision_boe:.4f}, recall = {recall_boe:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6da72624",
   "metadata": {},
   "source": [
    "# Disambiguation Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shift_shift(text):\n",
    "    for idx,ch in enumerate(text):\n",
    "        if not ch.isalpha():\n",
    "            return idx\n",
    "\n",
    "def convert_data_for_disambiguation(data, lang):\n",
    "    # convert examples to 1 entity per example and shift if needed\n",
    "    if lang=='ar':\n",
    "        MAX_LENGTH = 600\n",
    "        MAX_OFFSET = 400\n",
    "    elif lang == 'ja':\n",
    "        MAX_LENGTH = 350\n",
    "        MAX_OFFSET = 250\n",
    "    else:\n",
    "        MAX_LENGTH = 800\n",
    "        MAX_OFFSET = 600\n",
    "    new_examples = []\n",
    "    for example in tqdm(data):\n",
    "        original_text = example['original_text']\n",
    "        for _, _, ent, _, offset, length in example['gt_entities']:\n",
    "            shift = 0\n",
    "            if len(original_text) > MAX_LENGTH and offset > MAX_OFFSET:\n",
    "                shift = (offset - MAX_OFFSET)\n",
    "                shift += shift_shift(original_text[shift:])\n",
    "            new_example = {\n",
    "                'original_text': original_text[shift:],\n",
    "                'gt_entities': [[0,0,ent,_,offset-shift,length]],\n",
    "            }\n",
    "            new_examples.append(new_example)\n",
    "    return new_examples\n",
    "\n",
    "\n",
    "def metrics_disambiguation(test_data, predictions):\n",
    "    support = 0\n",
    "    support_only_predicted = 0\n",
    "    correct = 0\n",
    "    incorrect_pos = 0\n",
    "\n",
    "    for example_idx, (example, prediction) in tqdm(enumerate(zip(test_data, predictions))):\n",
    "#         targets = {\n",
    "#             (offset,length):ent_id\n",
    "#             for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "#         }\n",
    "#         prediction = {\n",
    "#             (offset,length):ent_id\n",
    "#             for offset,length,ent_id in zip(prediction['offsets'], prediction['lengths'], prediction['entities'])\n",
    "#         }\n",
    "\n",
    "#         support += len(targets)\n",
    "#         support_only_predicted += len(prediction)\n",
    "        \n",
    "#         correct += sum(1 for pos,ent_id in prediction.items() if (pos in targets and targets[pos] == ent_id))\n",
    "#         incorrect_pos += sum(1 for pos,_ in prediction.items() if pos not in targets)\n",
    "        if len(prediction['entities']) == 0:\n",
    "            continue\n",
    "        target = example['gt_entities'][0][2]\n",
    "        prediction = prediction['entities'][0]\n",
    "        correct += (target == prediction)\n",
    "        support += 1\n",
    "\n",
    "    accuracy = correct/support\n",
    "    # accuracy_only_predicted = correct/support_only_predicted\n",
    "\n",
    "    return accuracy, support #, accuracy_only_predicted, support_only_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ta.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ar.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/fa.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/sr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/tr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/de.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/es.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ja.jsonl',\n",
    "]\n",
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    lang = test_data_path[-8:-6]\n",
    "    test_data = load_file(test_data_path)\n",
    "    test_data = convert_data_for_disambiguation(test_data[:10000], lang)\n",
    "    predictions = model_eval.get_disambiguation_predictions(test_data)\n",
    "    accuracy, support = metrics_disambiguation(test_data, predictions)\n",
    "    print(f\"Accuracty {accuracy}, support {support}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eff17084",
   "metadata": {},
   "source": [
    "## Eyeball Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ced14ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text[:max_display_length]='He is the Dalai Lama. He is a Buddhist monk.'\n",
      "***************** Ground truth entities *****************\n",
      "len(ground_truth_entities)=0\n",
      "***************** Predicted entities *****************\n",
      "len(predicted_entities)=3\n",
      "mention=\"Dalai Lama.\" -> entity_id=Q37349 (md_score=0.25, el_score=0.01)\n",
      "mention=\"i Lama.\" -> entity_id=Q37349 (md_score=0.37, el_score=0.05)\n",
      "mention=\"Buddhist\" -> entity_id=Q748 (md_score=0.22, el_score=0.01)\n"
     ]
    }
   ],
   "source": [
    "text = \"Her name is Taylor Swift. New York City is a city in the United States.\"\n",
    "text = \"My dog is a Shiba Inu. Taylor Swift is a singer.\"\n",
    "text = \"Her name is Taylor Swift.\"\n",
    "text = \"He is the Dalai Lama. He is a Buddhist monk.\"\n",
    "prediction = model_eval.process_batch([text])[0]\n",
    "predicted_entities = [PredictedEntity(offset, length, text, entity_id, md_score, el_score) for offset, length, entity_id, md_score, el_score in zip(prediction[\"offsets\"], prediction[\"lengths\"], prediction[\"entities\"], prediction[\"md_scores\"], prediction[\"el_scores\"])]\n",
    "print_sample(text, [], predicted_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77ac96a-b29e-45c3-9ab1-43ddcd387572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /fsx/louismartin/bela/data/debug_mention_detection/mel/eval.1st.1_sample.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [00:00, 20122.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text[:max_display_length]=\"   Martina Steuk (née Kämpfert; born 11 November 1959) is a German former track and field athlete who represented East Germany. She competed in the 800 metres and occasionally the 400 metres.  Her first success came at the 1977 European Athletics Junior Championships, where she won the 800 m title. At twenty years old, reached the final at the 1980 Moscow Olympics and placed fourth in a lifetime best time of 1:56.21 behind a Soviet trio of Nadiya Olizarenko, Olga Mineyeva and Tatyana Providokhina that broke the world record. A successful 1981 season followed, which included a win at the 1981 European Cup, and at the 1981 IAAF World Cup she won an 800 m silver behind Lyudmila Veselkova and a gold with the East German 4 × 400 metres relay team. She ranked second in the world that year for the 800 m behind Vesselkova with a time of 1:57.16. That success continued into 1982 with a silver medal at the 1982 European Athletics Indoor Championships, finishing second only to Romania's Doina Melinte. She was top of the world indoor rankings that season with 1:59.24 minutes.  She was a three-time national champion, winning the 800 m at the East German Athletics Championships in 1981 before taking a national indoor and outdoor double in 1982.  She holds one of the fastest times for the 1000 metres at 2:30.85 minutes. This was the second fastest ever when it was set in 1980, behind Soviet runner Tatyana Providokhina, though it remained the fastest recorded electronically recorded time for ten years. It remains the best mark by an under-23 athlete and she also holds the under-23 best for the 600 metres event with 1:24.56 minutes.  Steuk was born in Berlin and was a member of her local club Berliner TSC. She married East German hammer thrower Roland Steuk in 1991. The two subsequently divorced. She was awarded the Patriotic Order of Merit for her athletic feats in 1982. \"\n",
      "***************** Ground truth entities *****************\n",
      "len(ground_truth_entities)=24\n",
      "mention=\"track and field\" -> entity_id=Q3312129\n",
      "mention=\"East Germany\" -> entity_id=Q16957\n",
      "mention=\"800 metres\" -> entity_id=Q271008\n",
      "mention=\"400 metres\" -> entity_id=Q334734\n",
      "mention=\"1977 European Athletics Junior Championships\" -> entity_id=Q974310\n",
      "mention=\"1980 Moscow Olympics\" -> entity_id=Q8450\n",
      "mention=\"Nadiya Olizarenko\" -> entity_id=Q231445\n",
      "mention=\"Olga Mineyeva\" -> entity_id=Q452613\n",
      "mention=\"Tatyana Providokhina\" -> entity_id=Q452678\n",
      "mention=\"1981 season\" -> entity_id=Q4580115\n",
      "mention=\"1981 European Cup\" -> entity_id=Q2999663\n",
      "mention=\"1981 IAAF World Cup\" -> entity_id=Q1814515\n",
      "mention=\"Lyudmila Veselkova\" -> entity_id=Q6710553\n",
      "mention=\"4 × 400 metres relay\" -> entity_id=Q230057\n",
      "mention=\"1982 European Athletics Indoor Championships\" -> entity_id=Q265197\n",
      "mention=\"Doina Melinte\" -> entity_id=Q237005\n",
      "mention=\"East German Athletics Championships\" -> entity_id=Q55610396\n",
      "mention=\"1000 metres\" -> entity_id=Q1629556\n",
      "mention=\"Tatyana Providokhina\" -> entity_id=Q452678\n",
      "mention=\"under-23 athlete\" -> entity_id=Q14510042\n",
      "mention=\"600 metres\" -> entity_id=Q2817913\n",
      "mention=\"Berlin\" -> entity_id=Q64\n",
      "mention=\"Roland Steuk\" -> entity_id=Q319394\n",
      "mention=\"Patriotic Order of Merit\" -> entity_id=Q819570\n",
      "***************** Predicted entities *****************\n",
      "len(predicted_entities)=22\n",
      "mention=\"track and field athlete\" -> entity_id=Q3312129 (md_score=1.00, el_score=1.00)\n",
      "mention=\"East Germany. She\" -> entity_id=Q16957 (md_score=1.00, el_score=1.00)\n",
      "mention=\"800 metres and\" -> entity_id=Q271008 (md_score=1.00, el_score=1.00)\n",
      "mention=\"400 metres.  Her\" -> entity_id=Q334734 (md_score=1.00, el_score=1.00)\n",
      "mention=\"the 1977 European Athletics Junior Championships, where\" -> entity_id=Q974310 (md_score=0.99, el_score=1.00)\n",
      "mention=\"the 1980 Moscow Olympics and\" -> entity_id=Q8450 (md_score=0.99, el_score=1.00)\n",
      "mention=\"Nadiya Olizarenko, Olga Mineyeva and\" -> entity_id=Q55733 (md_score=0.58, el_score=0.00)\n",
      "mention=\"Tatyana Providokhina that\" -> entity_id=Q452678 (md_score=0.99, el_score=1.00)\n",
      "mention=\"1981 season followed\" -> entity_id=Q4580115 (md_score=1.00, el_score=1.00)\n",
      "mention=\"the 1981 European Cup, and\" -> entity_id=Q2999663 (md_score=0.99, el_score=1.00)\n",
      "mention=\"the 1981 IAAF World Cup she\" -> entity_id=Q1814515 (md_score=0.99, el_score=1.00)\n",
      "mention=\"Veselkova and\" -> entity_id=Q6710553 (md_score=1.00, el_score=1.00)\n",
      "mention=\"4 × 400 metres relay team\" -> entity_id=Q230057 (md_score=0.99, el_score=1.00)\n",
      "mention=\"1982 European Athletics Indoor Championships, finishing\" -> entity_id=Q265197 (md_score=1.00, el_score=1.00)\n",
      "mention=\"Melinte. She\" -> entity_id=Q237005 (md_score=1.00, el_score=1.00)\n",
      "mention=\"East German Athletics Championships in\" -> entity_id=Q19962174 (md_score=0.96, el_score=1.00)\n",
      "mention=\"1982.  She\" -> entity_id=Q19962174 (md_score=0.89, el_score=1.00)\n",
      "mention=\"metres at\" -> entity_id=Q271008 (md_score=0.93, el_score=1.00)\n",
      "mention=\"Providokhina, though\" -> entity_id=Q452678 (md_score=0.98, el_score=1.00)\n",
      "mention=\"metres event with\" -> entity_id=Q271008 (md_score=0.72, el_score=1.00)\n",
      "mention=\"Berliner TSC. She\" -> entity_id=Q12277722 (md_score=0.52, el_score=1.00)\n",
      "mention=\"Patriotic Order of Merit for\" -> entity_id=Q12277722 (md_score=0.44, el_score=1.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data_path = \"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\"\n",
    "test_data_path = \"/fsx/louismartin/bela/data/debug_mention_detection/mel/eval.1st.1_sample.txt\"  # Overfit on one sample\n",
    "print(f\"Processing {test_data_path}\")\n",
    "test_data = load_file(test_data_path)\n",
    "#sample = test_data[200]\n",
    "sample = test_data[0]\n",
    "prediction = get_predictions_using_windows(model_eval, [sample])[0]\n",
    "text = sample[\"original_text\"]\n",
    "max_length = 1024\n",
    "\n",
    "ground_truth_entities = [GroundTruthEntity(offset, length, text, entity_id) for _, _, entity_id, _, offset, length in sample[\"gt_entities\"]]\n",
    "predicted_entities = [PredictedEntity(offset, length, text, entity_id, md_score, el_score) for offset, length, entity_id, md_score, el_score in zip(prediction[\"offsets\"], prediction[\"lengths\"], prediction[\"entities\"], prediction[\"md_scores\"], prediction[\"el_scores\"])]\n",
    "print_sample(text, ground_truth_entities, predicted_entities, max_display_length=100000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cca8e49",
   "metadata": {},
   "source": [
    "# Debug mention detection bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6b2f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text[:max_display_length]='Taylor Swift is a singer.'\n",
      "***************** Ground truth entities *****************\n",
      "len(ground_truth_entities)=0\n",
      "***************** Predicted entities *****************\n",
      "len(predicted_entities)=1\n",
      "mention=\"Taylor Swift\" -> entity_id=Q26876 (md_score=0.15, el_score=0.02)\n"
     ]
    }
   ],
   "source": [
    "# Set low thresholds\n",
    "model_eval.task.md_threshold = 0.1\n",
    "model_eval.task.el_threshold = 0.1\n",
    "text = \"Taylor Swift is a singer.\"\n",
    "prediction = model_eval.process_batch([text])[0]\n",
    "predicted_entities = [PredictedEntity(offset, length, text, entity_id, md_score, el_score) for offset, length, entity_id, md_score, el_score in zip(prediction[\"offsets\"], prediction[\"lengths\"], prediction[\"entities\"], prediction[\"md_scores\"], prediction[\"el_scores\"])]\n",
    "print_sample(text, [], predicted_entities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd802abc",
   "metadata": {},
   "source": [
    "## SPM Encode and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59b7563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4081, 106383, 4, 1839]\n",
      ". Her\n"
     ]
    }
   ],
   "source": [
    "text = \"400 metres.  Her\"\n",
    "print(model_eval.transform.processor.encode(text))\n",
    "print(model_eval.transform.processor.decode([4, 1839]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2930a292",
   "metadata": {},
   "source": [
    "# Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4388e8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12679it [00:00, 70671.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 12679 texts, batch_size=1024, model_eval.transform.max_seq_len=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [04:00, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 50s, sys: 35.3 s, total: 12min 25s\n",
      "Wall time: 4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_samples(samples, batch_size):\n",
    "    # Yield batches of samples\n",
    "    for i in range(0, len(samples), batch_size):\n",
    "        yield samples[i : i + batch_size]\n",
    "\n",
    "\n",
    "texts = [sample[\"original_text\"] for sample in load_file(\"/fsx/movb/data/matcha/mewsli-9/en.jsonl\")]\n",
    "batch_size = 1024\n",
    "print(f\"Processing {len(texts)} texts, {batch_size=}, {model_eval.transform.max_seq_len=}\")\n",
    "%time _ = [model_eval.process_batch(batch_texts) for batch_texts in tqdm(batch_samples(texts, batch_size), desc=\"Inference\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e9e7307",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sample:\n",
    "    text: str\n",
    "    ground_truth_entities: List[GroundTruthEntity]\n",
    "    predicted_entities: List[PredictedEntity]\n",
    "\n",
    "    def __init__(self, text, ground_truth_entities, predicted_entities):\n",
    "        self.text = text\n",
    "        self.ground_truth_entities = ground_truth_entities\n",
    "        self.predicted_entities = predicted_entities\n",
    "        self._compute_scores()\n",
    "\n",
    "    def _compute_scores(self):\n",
    "        self.true_positives = [predicted_entity for predicted_entity in self.predicted_entities if predicted_entity in self.ground_truth_entities]\n",
    "        self.false_positives = [predicted_entity for predicted_entity in self.predicted_entities if predicted_entity not in self.ground_truth_entities]\n",
    "        self.false_negatives = [ground_truth_entity for ground_truth_entity in self.ground_truth_entities if ground_truth_entity not in self.predicted_entities]\n",
    "        # Bag of entities\n",
    "        gold_entity_ids = set([ground_truth_entity.entity_id for ground_truth_entity in self.ground_truth_entities])\n",
    "        predicted_entity_ids = set([predicted_entity.entity_id for predicted_entity in self.predicted_entities])\n",
    "        self.true_positives_boe = [predicted_entity_id for predicted_entity_id in predicted_entity_ids if predicted_entity_id in gold_entity_ids]\n",
    "        self.false_positives_boe = [predicted_entity_id for predicted_entity_id in predicted_entity_ids if predicted_entity_id not in gold_entity_ids]\n",
    "        self.false_negatives_boe = [ground_truth_entity_id for ground_truth_entity_id in gold_entity_ids if ground_truth_entity_id not in predicted_entity_ids]\n",
    "\n",
    "\n",
    "    def print(self, max_display_length=1000):\n",
    "        print(f\"{self.text[:max_display_length]=}\")\n",
    "        print(\"***************** Ground truth entities *****************\")\n",
    "        print(f\"{len(self.ground_truth_entities)=}\")\n",
    "        for ground_truth_entity in self.ground_truth_entities:\n",
    "            if ground_truth_entity.offset + ground_truth_entity.length > max_display_length:\n",
    "                continue\n",
    "            print(ground_truth_entity)\n",
    "        print(\"***************** Predicted entities *****************\")\n",
    "        print(f\"{len(self.predicted_entities)=}\")\n",
    "        for predicted_entity in self.predicted_entities:\n",
    "            if predicted_entity.offset + predicted_entity.length > max_display_length:\n",
    "                continue\n",
    "            print(predicted_entity)\n",
    "\n",
    "\n",
    "\n",
    "class ModelResults:\n",
    "    def __init__(self, data, predictions, md_threshold=0.2, el_threshold=0.05, verbose=False):\n",
    "        self.data = data\n",
    "        self.predictions = predictions\n",
    "        self.md_threshold = md_threshold\n",
    "        self.el_threshold = el_threshold\n",
    "        self.verbose = verbose\n",
    "        self.samples = []\n",
    "        self._compute_scores()\n",
    "\n",
    "    def _compute_scores(self):\n",
    "        self.samples = []\n",
    "        for ground_truth_sample, predicted_sample in zip(self.data, self.predictions):\n",
    "            ground_truth_entities = [\n",
    "                GroundTruthEntity(\n",
    "                    offset=offset,\n",
    "                    length=length,\n",
    "                    text=ground_truth_sample['original_text'],\n",
    "                    entity_id=ent_id,\n",
    "                )\n",
    "                for offset, length, ent_id, _, _, _ in ground_truth_sample['gt_entities']\n",
    "            ]\n",
    "            predicted_entities = [\n",
    "                PredictedEntity(\n",
    "                    offset=offset,\n",
    "                    length=length,\n",
    "                    text=ground_truth_sample['original_text'],\n",
    "                    entity_id=ent_id,\n",
    "                    md_score=md_score,\n",
    "                    el_score=el_score,\n",
    "                )\n",
    "                for offset, length, ent_id, md_score, el_score in zip(\n",
    "                    predicted_sample['offsets'],\n",
    "                    predicted_sample['lengths'],\n",
    "                    predicted_sample['entities'],\n",
    "                    predicted_sample['md_scores'],\n",
    "                    predicted_sample['el_scores'],\n",
    "                )\n",
    "                if (el_score > self.el_threshold and md_score > self.md_threshold)\n",
    "            ]\n",
    "            sample = Sample(\n",
    "                text=ground_truth_sample['original_text'],\n",
    "                ground_truth_entities=ground_truth_entities,\n",
    "                predicted_entities=predicted_entities,\n",
    "            )\n",
    "            self.samples.append(sample)\n",
    "        \n",
    "        def safe_division(numerator, denominator):\n",
    "            return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "        self.precision = safe_division(\n",
    "            sum([len(sample.true_positives) for sample in self.samples]),\n",
    "            sum([len(sample.predicted_entities) for sample in self.samples]),\n",
    "        )\n",
    "        self.recall = safe_division(\n",
    "            sum([len(sample.true_positives) for sample in self.samples]),\n",
    "            # TODO: Check that we can't predict the same entity twice\n",
    "            sum([len(sample.ground_truth_entities) for sample in self.samples]),\n",
    "        )\n",
    "        self.f1 = safe_division(2 * self.precision * self.recall, self.precision + self.recall)\n",
    "        self.precision_boe = safe_division(\n",
    "            sum([len(sample.true_positives_boe) for sample in self.samples]),\n",
    "            sum([len(sample.predicted_entities) for sample in self.samples]),\n",
    "        )\n",
    "        self.recall_boe = safe_division(\n",
    "            sum([len(sample.true_positives_boe) for sample in self.samples]),\n",
    "            sum([len(sample.ground_truth_entities) for sample in self.samples]),\n",
    "        )\n",
    "        self.f1_boe = safe_division(2 * self.precision_boe * self.recall_boe, self.precision_boe + self.recall_boe)\n",
    "        #return (f1, precision, recall), (f1_boe, precision_boe, recall_boe)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "235bcda9aabf5f8363267684c8e0d7a57e9fb12569fe58ae215aef35e2f0a58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
