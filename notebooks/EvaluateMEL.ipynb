{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff95f192-5b1e-4caf-a5cc-3290b0aebf22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bela'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional, List, Dict, Any, Tuple\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbela\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspm_transform\u001b[39;00m \u001b[39mimport\u001b[39;00m SPMTransform\n\u001b[1;32m     18\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bela'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yaml\n",
    "from hydra.experimental import compose, initialize_config_module\n",
    "import hydra\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import faiss\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "from bela.transforms.spm_transform import SPMTransform\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70df6d4e-a2a6-4178-a305-dfb17382cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_encodings = data[\"text_encodings\"].to(device)\n",
    "# text_pad_mask = data[\"batch\"][\"attention_mask\"].to(device)\n",
    "# gold_mention_offsets = data[\"batch\"][\"mention_offsets\"].to(device)\n",
    "# gold_mention_lengths = data[\"batch\"][\"mention_lengths\"].to(device)\n",
    "# entities_ids = data[\"batch\"][\"entities\"].to(device)\n",
    "# tokens_mapping = data[\"batch\"][\"tokens_mapping\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ada5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set git editor to vim\n",
    "!git config --global core.editor vim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aff9a386-d071-494f-abaf-7ad23c1f0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "    all_data = []\n",
    "    with open(path, 'rt') as fd:\n",
    "        for line in tqdm(fd):\n",
    "            data = json.loads(line)\n",
    "            all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def convert_sp_to_char_offsets(\n",
    "    text: str,\n",
    "    sp_offsets: List[int],\n",
    "    sp_lengths: List[int],\n",
    "    sp_tokens_boundaries: List[List[int]],\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Function convert sentecepiece offsets and lengths to character level\n",
    "    offsets and lengths for a given `text`.\n",
    "    \"\"\"\n",
    "    \n",
    "    char_offsets: List[int] = []\n",
    "    char_lengths: List[int] = []\n",
    "    text_utf8_chars: List[str] = [char for char in text]\n",
    "\n",
    "    for sp_offset, sp_length in zip(sp_offsets, sp_lengths):\n",
    "        # sp_offsets include cls_token, while boundaries doesn't\n",
    "        if sp_offset == 0:\n",
    "            continue\n",
    "\n",
    "        sp_offset = sp_offset - 1\n",
    "        char_offset = sp_tokens_boundaries[sp_offset][0]\n",
    "        char_end = sp_tokens_boundaries[sp_offset + sp_length - 1][1]\n",
    "\n",
    "        # sp token boundaries include whitespaces, so remove them\n",
    "        while text_utf8_chars[char_offset].isspace():\n",
    "            char_offset += 1\n",
    "            assert char_offset < len(text_utf8_chars)\n",
    "\n",
    "        char_offsets.append(char_offset)\n",
    "        char_lengths.append(char_end - char_offset)\n",
    "\n",
    "    return char_offsets, char_lengths\n",
    "    \n",
    "\n",
    "class ModelEval:\n",
    "    def __init__(self, checkpoint_path, config_name=\"joint_el_mel\"):\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        \n",
    "        logger.info(\"Create task\")\n",
    "        with initialize_config_module(\"bela/conf\"):\n",
    "            cfg = compose(config_name=config_name)\n",
    "            \n",
    "        self.transform = hydra.utils.instantiate(cfg.task.transform)\n",
    "        datamodule = hydra.utils.instantiate(cfg.datamodule, transform=self.transform)\n",
    "        self.task = hydra.utils.instantiate(cfg.task, datamodule=datamodule, _recursive_=False)\n",
    "        \n",
    "        self.task.setup(\"train\")\n",
    "        self.task = self.task.eval()\n",
    "        self.task = self.task.to(self.device)\n",
    "        self.embeddings = self.task.embeddings\n",
    "        self.faiss_index = self.task.faiss_index\n",
    "        \n",
    "        # logger.info(\"Create GPU index\")\n",
    "        # self.create_gpu_index()\n",
    "        \n",
    "        logger.info(\"Create ent index\")\n",
    "        self.ent_idx = []\n",
    "        for ent in datamodule.ent_catalogue.idx:\n",
    "            self.ent_idx.append(ent)\n",
    "        \n",
    "        logger.info(\"Load checkpoint\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "        self.task.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        \n",
    "    def create_gpu_index(self, gpu_id=0):\n",
    "        flat_config = faiss.GpuIndexFlatConfig()\n",
    "        flat_config.device = gpu_id\n",
    "        flat_config.useFloat16 = True\n",
    "\n",
    "        res = faiss.StandardGpuResources()\n",
    "\n",
    "        self.faiss_index = faiss.GpuIndexFlatIP(res, embeddings.shape[1], flat_config)\n",
    "        self.faiss_index.add(self.embeddings)\n",
    "        \n",
    "    def lookup(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "    ):\n",
    "        scores, indices = self.faiss_index.search(query, k=1)\n",
    "\n",
    "        return scores.squeeze(-1).to(self.device), indices.squeeze(-1).to(self.device)\n",
    "    \n",
    "    def process_batch(self, texts): \n",
    "        batch: Dict[str, Any] = {\"texts\": texts}\n",
    "        model_inputs = self.transform(batch)\n",
    "\n",
    "        token_ids = model_inputs[\"input_ids\"].to(self.device)\n",
    "        text_pad_mask = model_inputs[\"attention_mask\"].to(self.device)\n",
    "        tokens_mapping = model_inputs[\"tokens_mapping\"].to(self.device)\n",
    "        sp_tokens_boundaries = model_inputs[\"sp_tokens_boundaries\"].tolist()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, last_layer = self.task.encoder(token_ids)\n",
    "            text_encodings = last_layer\n",
    "            text_encodings = self.task.project_encoder_op(text_encodings)\n",
    "\n",
    "            mention_logits, mention_bounds = self.task.mention_encoder(\n",
    "                text_encodings, text_pad_mask, tokens_mapping\n",
    "            )\n",
    "\n",
    "            (\n",
    "                chosen_mention_logits,\n",
    "                chosen_mention_bounds,\n",
    "                chosen_mention_mask,\n",
    "                mention_pos_mask,\n",
    "            ) = self.task.mention_encoder.prune_ctxt_mentions(\n",
    "                mention_logits,\n",
    "                mention_bounds,\n",
    "                num_cand_mentions=50,\n",
    "                threshold=self.task.md_threshold,\n",
    "            )\n",
    "\n",
    "            mention_offsets = chosen_mention_bounds[:, :, 0]\n",
    "            mention_lengths = (\n",
    "                chosen_mention_bounds[:, :, 1] - chosen_mention_bounds[:, :, 0] + 1\n",
    "            )\n",
    "            mention_lengths[mention_offsets == 0] = 0\n",
    "\n",
    "            mentions_repr = self.task.span_encoder(\n",
    "                text_encodings, mention_offsets, mention_lengths\n",
    "            )\n",
    "\n",
    "            # flat mentions and entities indices (mentions_num x embedding_dim)\n",
    "            flat_mentions_repr = mentions_repr[mention_lengths != 0]\n",
    "            mentions_scores = torch.sigmoid(chosen_mention_logits)\n",
    "\n",
    "            # retrieve candidates top-1 ids and scores\n",
    "            cand_scores, cand_indices = self.lookup(\n",
    "                flat_mentions_repr.detach()\n",
    "            )\n",
    "\n",
    "            entities_repr = self.embeddings[cand_indices].to(self.device)\n",
    "\n",
    "            chosen_mention_limits: List[int] = (\n",
    "                chosen_mention_mask.int().sum(-1).detach().cpu().tolist()\n",
    "            )\n",
    "            flat_mentions_scores = mentions_scores[mention_lengths != 0].unsqueeze(-1)\n",
    "            cand_scores = cand_scores.unsqueeze(-1)\n",
    "\n",
    "            el_scores = torch.sigmoid(\n",
    "                self.task.el_encoder(\n",
    "                    flat_mentions_repr,\n",
    "                    entities_repr,\n",
    "                    flat_mentions_scores,\n",
    "                    cand_scores,\n",
    "                )\n",
    "            ).squeeze(1)\n",
    "\n",
    "        predictions = []\n",
    "        cand_idx = 0\n",
    "        example_idx = 0\n",
    "        for offsets, lengths, md_scores in zip(\n",
    "            mention_offsets, mention_lengths, mentions_scores\n",
    "        ):\n",
    "            ex_sp_offsets = []\n",
    "            ex_sp_lengths = []\n",
    "            ex_entities = []\n",
    "            ex_md_scores = []\n",
    "            ex_el_scores = []\n",
    "            for offset, length, md_score in zip(offsets, lengths, md_scores):\n",
    "                if length != 0:\n",
    "                    if md_score >= self.task.md_threshold:\n",
    "                        ex_sp_offsets.append(offset.detach().cpu().item())\n",
    "                        ex_sp_lengths.append(length.detach().cpu().item())\n",
    "                        ex_entities.append(self.ent_idx[cand_indices[cand_idx].detach().cpu().item()])\n",
    "                        ex_md_scores.append(md_score.item())       \n",
    "                        ex_el_scores.append(el_scores[cand_idx].item())     \n",
    "                    cand_idx += 1\n",
    "\n",
    "            char_offsets, char_lengths = convert_sp_to_char_offsets(\n",
    "                texts[example_idx],\n",
    "                ex_sp_offsets,\n",
    "                ex_sp_lengths,\n",
    "                sp_tokens_boundaries[example_idx],\n",
    "            )\n",
    "\n",
    "            predictions.append(\n",
    "                {\n",
    "                    \"offsets\": char_offsets,\n",
    "                    \"lengths\": char_lengths,\n",
    "                    \"entities\": ex_entities,\n",
    "                    \"md_scores\": ex_md_scores,\n",
    "                    \"el_scores\": ex_el_scores,\n",
    "                }\n",
    "            )\n",
    "            example_idx += 1\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def process_disambiguation_batch(self, texts, mention_offsets, mention_lengths, entities):\n",
    "        batch: Dict[str, Any] = {\n",
    "            \"texts\": texts,\n",
    "            \"mention_offsets\": mention_offsets,\n",
    "            \"mention_lengths\": mention_lengths,\n",
    "            \"entities\": entities,\n",
    "        }\n",
    "        model_inputs = self.transform(batch)\n",
    "\n",
    "        token_ids = model_inputs[\"input_ids\"].to(self.device)\n",
    "        mention_offsets = model_inputs[\"mention_offsets\"]\n",
    "        mention_lengths = model_inputs[\"mention_lengths\"]\n",
    "        tokens_mapping = model_inputs[\"tokens_mapping\"].to(self.device)\n",
    "        sp_tokens_boundaries = model_inputs[\"sp_tokens_boundaries\"].tolist()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, last_layer = self.task.encoder(token_ids)\n",
    "            text_encodings = last_layer\n",
    "            text_encodings = self.task.project_encoder_op(text_encodings)\n",
    "\n",
    "            mentions_repr = self.task.span_encoder(\n",
    "                text_encodings, mention_offsets, mention_lengths\n",
    "            )\n",
    "\n",
    "            flat_mentions_repr = mentions_repr[mention_lengths != 0]\n",
    "            # retrieve candidates top-1 ids and scores\n",
    "            cand_scores, cand_indices = self.lookup(\n",
    "                flat_mentions_repr.detach()\n",
    "            )\n",
    "            predictions = []\n",
    "            cand_idx = 0\n",
    "            example_idx = 0\n",
    "            for offsets, lengths in zip(\n",
    "                mention_offsets, mention_lengths,\n",
    "            ):\n",
    "                ex_sp_offsets = []\n",
    "                ex_sp_lengths = []\n",
    "                ex_entities = []\n",
    "                ex_dis_scores = []\n",
    "                for offset, length in zip(offsets, lengths):\n",
    "                    if length != 0:\n",
    "                        ex_sp_offsets.append(offset.detach().cpu().item())\n",
    "                        ex_sp_lengths.append(length.detach().cpu().item())\n",
    "                        ex_entities.append(self.ent_idx[cand_indices[cand_idx].detach().cpu().item()])\n",
    "                        ex_dis_scores.append(cand_scores[cand_idx].detach().cpu().item())           \n",
    "                        cand_idx += 1\n",
    "                        \n",
    "                # print(\"ex_sp_offsets\", ex_sp_offsets)\n",
    "                # print(\"ex_sp_lengths\", ex_sp_lengths)\n",
    "\n",
    "                char_offsets, char_lengths = convert_sp_to_char_offsets(\n",
    "                    texts[example_idx],\n",
    "                    ex_sp_offsets,\n",
    "                    ex_sp_lengths,\n",
    "                    sp_tokens_boundaries[example_idx],\n",
    "                )\n",
    "                \n",
    "                # print(\"char_offsets\", char_offsets)\n",
    "                # print(\"char_lengths\", char_lengths)\n",
    "\n",
    "                predictions.append({\n",
    "                    \"offsets\": char_offsets,\n",
    "                    \"lengths\": char_lengths,\n",
    "                    \"entities\": ex_entities,\n",
    "                    \"scores\": ex_dis_scores\n",
    "                })\n",
    "                example_idx+= 1\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def get_predictions(self, test_data, batch_size=256):\n",
    "        all_predictions = []\n",
    "        for batch_start in tqdm(range(0,len(test_data),batch_size)):\n",
    "            batch = test_data[batch_start:batch_start+batch_size]\n",
    "            texts = [example['original_text'] for example in batch]\n",
    "            predictions = self.process_batch(texts)\n",
    "            all_predictions.extend(predictions)\n",
    "        return all_predictions\n",
    "    \n",
    "    def get_disambiguation_predictions(self, test_data, batch_size=256):\n",
    "        all_predictions = []\n",
    "        for batch_start in tqdm(range(0,len(test_data),batch_size)):\n",
    "            batch = test_data[batch_start:batch_start+batch_size]\n",
    "            texts = [example['original_text'] for example in batch]\n",
    "            mention_offsets = [[offset for _,_,_,_,offset,_ in example['gt_entities']] for example in batch]\n",
    "            mention_lengths = [[length for _,_,_,_,_,length in example['gt_entities']] for example in batch]\n",
    "            entities = [[0 for _,_,_,_,_,_ in example['gt_entities']] for example in batch]\n",
    "\n",
    "            predictions = self.process_disambiguation_batch(texts, mention_offsets, mention_lengths, entities)\n",
    "            all_predictions.extend(predictions)\n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eec100b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/data/home/louismartin/miniconda3/envs/bela3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/data/home/louismartin/miniconda3/envs/bela3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining file:///data/home/louismartin/dev/BELA\n",
      "\u001b[31mERROR: file:///data/home/louismartin/dev/BELA does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/data/home/louismartin/miniconda3/envs/bela3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/data/home/louismartin/miniconda3/envs/bela3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/data/home/louismartin/miniconda3/envs/bela3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf93bf95-295e-4efd-8133-bd9aedf156d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingConfigException",
     "evalue": "Primary config module 'bela.conf' not found.\nCheck that it's correct and contains an __init__.py file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingConfigException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#from bela.evaluation.model_eval import ModelEval  # Use overwriting model eval\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# checkpoint_path = '/checkpoints/movb/bela/2022-11-27-225013/0/lightning_logs/version_286287/checkpoints/last_15000.ckpt'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# checkpoint_path = '/checkpoints/movb/bela/2023-01-13-023711/0/lightning_logs/version_4144/checkpoints/last.ckpt'\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[39m# e2e model with isotropic embeddings\u001b[39;00m\n\u001b[1;32m      6\u001b[0m checkpoint_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/checkpoints/movb/bela/2023-01-18-220105/0/lightning_logs/version_4820/checkpoints/last.ckpt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model_eval \u001b[39m=\u001b[39m ModelEval(checkpoint_path, config_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mjoint_el_mel_new\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m, in \u001b[0;36mModelEval.__init__\u001b[0;34m(self, checkpoint_path, config_name)\u001b[0m\n\u001b[1;32m     49\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreate task\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[39mwith\u001b[39;00m initialize_config_module(\u001b[39m\"\u001b[39m\u001b[39mbela/conf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     cfg \u001b[39m=\u001b[39m compose(config_name\u001b[39m=\u001b[39;49mconfig_name)\n\u001b[1;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m hydra\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39minstantiate(cfg\u001b[39m.\u001b[39mtask\u001b[39m.\u001b[39mtransform)\n\u001b[1;32m     54\u001b[0m datamodule \u001b[39m=\u001b[39m hydra\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39minstantiate(cfg\u001b[39m.\u001b[39mdatamodule, transform\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform)\n",
      "File \u001b[0;32m~/miniconda3/envs/bela3.8/lib/python3.8/site-packages/hydra/experimental/compose.py:26\u001b[0m, in \u001b[0;36mcompose\u001b[0;34m(config_name, overrides, return_hydra_config, strict)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(message)\n\u001b[1;32m     25\u001b[0m deprecation_warning(message\u001b[39m=\u001b[39mmessage)\n\u001b[0;32m---> 26\u001b[0m \u001b[39mreturn\u001b[39;00m real_compose(\n\u001b[1;32m     27\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m     28\u001b[0m     overrides\u001b[39m=\u001b[39;49moverrides,\n\u001b[1;32m     29\u001b[0m     return_hydra_config\u001b[39m=\u001b[39;49mreturn_hydra_config,\n\u001b[1;32m     30\u001b[0m     strict\u001b[39m=\u001b[39;49mstrict,\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bela3.8/lib/python3.8/site-packages/hydra/compose.py:34\u001b[0m, in \u001b[0;36mcompose\u001b[0;34m(config_name, overrides, return_hydra_config, strict)\u001b[0m\n\u001b[1;32m     32\u001b[0m gh \u001b[39m=\u001b[39m GlobalHydra\u001b[39m.\u001b[39minstance()\n\u001b[1;32m     33\u001b[0m \u001b[39massert\u001b[39;00m gh\u001b[39m.\u001b[39mhydra \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m cfg \u001b[39m=\u001b[39m gh\u001b[39m.\u001b[39;49mhydra\u001b[39m.\u001b[39;49mcompose_config(\n\u001b[1;32m     35\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m     36\u001b[0m     overrides\u001b[39m=\u001b[39;49moverrides,\n\u001b[1;32m     37\u001b[0m     run_mode\u001b[39m=\u001b[39;49mRunMode\u001b[39m.\u001b[39;49mRUN,\n\u001b[1;32m     38\u001b[0m     from_shell\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     39\u001b[0m     with_log_configuration\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(cfg, DictConfig)\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_hydra_config:\n",
      "File \u001b[0;32m~/miniconda3/envs/bela3.8/lib/python3.8/site-packages/hydra/_internal/hydra.py:594\u001b[0m, in \u001b[0;36mHydra.compose_config\u001b[0;34m(self, config_name, overrides, run_mode, with_log_configuration, from_shell, validate_sweep_overrides)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompose_config\u001b[39m(\n\u001b[1;32m    577\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    578\u001b[0m     config_name: Optional[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m     validate_sweep_overrides: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    584\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DictConfig:\n\u001b[1;32m    585\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[39m    :param config_name:\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[39m    :param overrides:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39m    :return:\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     cfg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_loader\u001b[39m.\u001b[39;49mload_configuration(\n\u001b[1;32m    595\u001b[0m         config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m    596\u001b[0m         overrides\u001b[39m=\u001b[39;49moverrides,\n\u001b[1;32m    597\u001b[0m         run_mode\u001b[39m=\u001b[39;49mrun_mode,\n\u001b[1;32m    598\u001b[0m         from_shell\u001b[39m=\u001b[39;49mfrom_shell,\n\u001b[1;32m    599\u001b[0m         validate_sweep_overrides\u001b[39m=\u001b[39;49mvalidate_sweep_overrides,\n\u001b[1;32m    600\u001b[0m     )\n\u001b[1;32m    601\u001b[0m     \u001b[39mif\u001b[39;00m with_log_configuration:\n\u001b[1;32m    602\u001b[0m         configure_log(cfg\u001b[39m.\u001b[39mhydra\u001b[39m.\u001b[39mhydra_logging, cfg\u001b[39m.\u001b[39mhydra\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/miniconda3/envs/bela3.8/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py:142\u001b[0m, in \u001b[0;36mConfigLoaderImpl.load_configuration\u001b[0;34m(self, config_name, overrides, run_mode, from_shell, validate_sweep_overrides)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_configuration\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     config_name: Optional[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     validate_sweep_overrides: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DictConfig:\n\u001b[1;32m    141\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_configuration_impl(\n\u001b[1;32m    143\u001b[0m             config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m    144\u001b[0m             overrides\u001b[39m=\u001b[39;49moverrides,\n\u001b[1;32m    145\u001b[0m             run_mode\u001b[39m=\u001b[39;49mrun_mode,\n\u001b[1;32m    146\u001b[0m             from_shell\u001b[39m=\u001b[39;49mfrom_shell,\n\u001b[1;32m    147\u001b[0m             validate_sweep_overrides\u001b[39m=\u001b[39;49mvalidate_sweep_overrides,\n\u001b[1;32m    148\u001b[0m         )\n\u001b[1;32m    149\u001b[0m     \u001b[39mexcept\u001b[39;00m OmegaConfBaseException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    150\u001b[0m         \u001b[39mraise\u001b[39;00m ConfigCompositionException()\u001b[39m.\u001b[39mwith_traceback(sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m]) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bela3.8/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py:243\u001b[0m, in \u001b[0;36mConfigLoaderImpl._load_configuration_impl\u001b[0;34m(self, config_name, overrides, run_mode, from_shell, validate_sweep_overrides)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_configuration_impl\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     config_name: Optional[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     validate_sweep_overrides: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    240\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DictConfig:\n\u001b[1;32m    241\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mhydra\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__, version\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensure_main_config_source_available()\n\u001b[1;32m    244\u001b[0m     parsed_overrides, caching_repo \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_overrides_and_create_caching_repo(\n\u001b[1;32m    245\u001b[0m         config_name, overrides\n\u001b[1;32m    246\u001b[0m     )\n\u001b[1;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m validate_sweep_overrides:\n",
      "File \u001b[0;32m~/miniconda3/envs/bela3.8/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py:129\u001b[0m, in \u001b[0;36mConfigLoaderImpl.ensure_main_config_source_available\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    125\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPrimary config directory not found.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheck that the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m config directory \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msource\u001b[39m.\u001b[39mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m exists and readable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m     )\n\u001b[0;32m--> 129\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_missing_config_error(\n\u001b[1;32m    130\u001b[0m     config_name\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, msg\u001b[39m=\u001b[39;49mmsg, with_search_path\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bela3.8/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py:102\u001b[0m, in \u001b[0;36mConfigLoaderImpl._missing_config_error\u001b[0;34m(self, config_name, msg, with_search_path)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[39mreturn\u001b[39;00m msg\n\u001b[0;32m--> 102\u001b[0m \u001b[39mraise\u001b[39;00m MissingConfigException(\n\u001b[1;32m    103\u001b[0m     missing_cfg_file\u001b[39m=\u001b[39mconfig_name, message\u001b[39m=\u001b[39madd_search_path()\n\u001b[1;32m    104\u001b[0m )\n",
      "\u001b[0;31mMissingConfigException\u001b[0m: Primary config module 'bela.conf' not found.\nCheck that it's correct and contains an __init__.py file"
     ]
    }
   ],
   "source": [
    "#from bela.evaluation.model_eval import ModelEval  # Use overriden ModelEval class\n",
    "# checkpoint_path = '/checkpoints/movb/bela/2022-11-27-225013/0/lightning_logs/version_286287/checkpoints/last_15000.ckpt'\n",
    "# checkpoint_path = '/checkpoints/movb/bela/2023-01-13-023711/0/lightning_logs/version_4144/checkpoints/last.ckpt'\n",
    "\n",
    "# e2e model with isotropic embeddings\n",
    "checkpoint_path = '/checkpoints/movb/bela/2023-01-18-220105/0/lightning_logs/version_4820/checkpoints/last.ckpt'\n",
    "model_eval = ModelEval(checkpoint_path, config_name=\"joint_el_mel_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e5d9cb9-d950-431a-b1be-c7d96cf22a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_transform = SPMTransform(max_seq_len=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08dde28a-a663-42f7-a745-ee4e95800a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(text):\n",
    "    tokens = sp_transform([text])[0]\n",
    "    tokens = tokens[1:-1]\n",
    "    window_length = 254\n",
    "    windows = []\n",
    "    for window_start in range(0,len(tokens),window_length//2):\n",
    "        start_pos = tokens[window_start][1]\n",
    "        if window_start + window_length >= len(tokens):\n",
    "            end_pos = tokens[-1][2]\n",
    "        else:\n",
    "            end_pos = tokens[window_start + window_length][2]\n",
    "        windows.append((start_pos, end_pos))\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c06ec10-5907-4197-ad38-fbea356a2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions_to_dict(example_predictions):\n",
    "    predictions = []\n",
    "    if len(example_predictions) > 0:\n",
    "        offsets, lengths, entities, md_scores, el_scores = zip(*example_predictions) \n",
    "    else:\n",
    "        offsets, lengths, entities, md_scores, el_scores = [], [], [], [], []\n",
    "    return {\n",
    "        'offsets': offsets,\n",
    "        'lengths': lengths,\n",
    "        'entities': entities,\n",
    "        'md_scores': md_scores,\n",
    "        'el_scores': el_scores,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e9305a-44cf-40d1-842e-5bf16f788dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_predictions_by_example(all_predictions, extended_examples):\n",
    "    grouped_predictions = defaultdict(list)\n",
    "    for prediction, extended_example in zip(all_predictions, extended_examples):\n",
    "        window_start = extended_example['window_start']\n",
    "        prediction = dict(prediction)\n",
    "        prediction['offsets'] = [offset + window_start for offset in prediction['offsets']]\n",
    "        grouped_predictions[extended_example['document_id']].append((\n",
    "            prediction\n",
    "        ))\n",
    "    \n",
    "    predictions = {}\n",
    "    for document_id, example_prediction_list in grouped_predictions.items():\n",
    "        example_predictions = []\n",
    "        for prediction in example_prediction_list:\n",
    "            for offset,length,ent,md_score,el_score in zip(\n",
    "                prediction['offsets'],\n",
    "                prediction['lengths'],\n",
    "                prediction['entities'],\n",
    "                prediction['md_scores'],\n",
    "                prediction['el_scores'],\n",
    "            ):\n",
    "                example_predictions.append((offset,length,ent,md_score,el_score))\n",
    "                example_predictions = sorted(example_predictions)\n",
    "        predictions[document_id] = example_predictions\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96bb996a-a227-4adc-8f44-debcd79bd5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_predictions(example_predictions):\n",
    "    filtered_example_predictions = []\n",
    "\n",
    "    current_end = None\n",
    "    current_offset = None\n",
    "    current_length = None\n",
    "    current_ent_id = None\n",
    "    current_md_score = None\n",
    "    current_el_score = None\n",
    "\n",
    "    for offset,length,ent_id,md_score,el_score in example_predictions:\n",
    "        if current_end is None:\n",
    "            current_end = offset + length\n",
    "            current_offset = offset\n",
    "            current_length = length\n",
    "            current_ent_id = ent_id\n",
    "            current_md_score = md_score\n",
    "            current_el_score = el_score\n",
    "            continue\n",
    "\n",
    "        if offset < current_end:\n",
    "            # intersection of two predictions\n",
    "            if md_score > current_md_score:\n",
    "                current_ent_id = ent_id\n",
    "                current_offset = offset\n",
    "                current_length = length\n",
    "                current_md_score = md_score\n",
    "                current_el_score = el_score\n",
    "        else:\n",
    "            filtered_example_predictions.append((\n",
    "                current_offset,\n",
    "                current_length,\n",
    "                current_ent_id,\n",
    "                current_md_score,\n",
    "                current_el_score\n",
    "            ))\n",
    "            current_ent_id = ent_id\n",
    "            current_offset = offset\n",
    "            current_length = length\n",
    "            current_md_score = md_score\n",
    "            current_el_score = el_score\n",
    "\n",
    "        current_end = offset+length\n",
    "\n",
    "    if current_offset is not None:\n",
    "        filtered_example_predictions.append((\n",
    "            current_offset,\n",
    "            current_length,\n",
    "            current_ent_id,\n",
    "            current_md_score,\n",
    "            current_el_score\n",
    "        ))\n",
    "    \n",
    "    return filtered_example_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be56b3d1-12c7-4354-87ea-cfd048c6fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_mention_offsets(example_predictions, text):\n",
    "    TEXT_SEPARATORS = [' ','.',',','!','?','-','\\n']\n",
    "    corrected_example_predictions = []\n",
    "    for offset,length,ent_id,md_score,el_score in example_predictions:\n",
    "        while offset !=0 and offset<len(text) and (text[offset-1] not in TEXT_SEPARATORS or text[offset] in TEXT_SEPARATORS):\n",
    "            offset += 1\n",
    "            length -= 1\n",
    "        while offset+length < len(text) and text[offset+length] not in TEXT_SEPARATORS:\n",
    "            length += 1\n",
    "        corrected_example_predictions.append((\n",
    "            offset,length,ent_id,md_score,el_score\n",
    "        ))\n",
    "    return corrected_example_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9697cde-300d-41de-928c-9bed12795090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_using_windows(test_data):\n",
    "    extended_examples = []\n",
    "\n",
    "    for example in test_data:\n",
    "        text = example['original_text']\n",
    "        windows = get_windows(text)\n",
    "        for idx, (start_pos, end_pos) in enumerate(windows):\n",
    "            new_text = text[start_pos:end_pos]\n",
    "            extended_examples.append({\n",
    "                'document_id': example['document_id'],\n",
    "                'original_text': new_text,\n",
    "                'gt_entities': example['gt_entities'],\n",
    "                'window_idx': idx,\n",
    "                'window_start': start_pos,\n",
    "                'window_end': end_pos,\n",
    "            })\n",
    "\n",
    "    all_predictions = model_eval.get_predictions(extended_examples)\n",
    "    predictions_dict = group_predictions_by_example(all_predictions, extended_examples)\n",
    "\n",
    "    predictions = []\n",
    "    for example in test_data:\n",
    "        document_id = example['document_id']\n",
    "        text = example['original_text']\n",
    "        example_predictions = predictions_dict[document_id]\n",
    "        example_predictions = merge_predictions(example_predictions)\n",
    "        example_predictions = correct_mention_offsets(example_predictions, text)\n",
    "        example_predictions = convert_predictions_to_dict(example_predictions)\n",
    "        predictions.append(example_predictions)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e28615-bbcb-4f51-8062-486848ae0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(data, predictions, md_threshold=0.2, el_threshold=0.05):\n",
    "    tp, fp, support = 0, 0, 0\n",
    "    tp_boe, fp_boe, support_boe = 0, 0, 0\n",
    "\n",
    "    predictions_per_example = []\n",
    "    for example, example_predictions in zip(data, predictions):\n",
    "\n",
    "        example_targets = {\n",
    "            (offset,length):ent_id\n",
    "            for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "        }\n",
    "\n",
    "        example_predictions = {\n",
    "            (offset, length):ent_id\n",
    "            for offset, length, ent_id, md_score, el_score in zip(\n",
    "                example_predictions['offsets'],\n",
    "                example_predictions['lengths'],\n",
    "                example_predictions['entities'],\n",
    "                example_predictions['md_scores'],\n",
    "                example_predictions['el_scores'],\n",
    "            )\n",
    "            if (el_score > el_threshold and md_score > md_threshold) \n",
    "        }\n",
    "\n",
    "        predictions_per_example.append((len(example_targets), len(example_predictions)))\n",
    "\n",
    "        for pos, ent in example_targets.items():\n",
    "            support += 1\n",
    "            if pos in example_predictions and example_predictions[pos] == ent:\n",
    "                tp += 1\n",
    "        for pos, ent in example_predictions.items():\n",
    "            if pos not in example_targets or example_targets[pos] != ent:\n",
    "                fp += 1\n",
    "\n",
    "        example_targets_set = set(example_targets.values())\n",
    "        example_predictions_set = set(example_predictions.values())\n",
    "\n",
    "        for ent in example_targets_set:\n",
    "            support_boe += 1\n",
    "            if ent in example_predictions_set:\n",
    "                tp_boe += 1\n",
    "        for ent in example_predictions_set:\n",
    "            if ent not in example_targets_set:\n",
    "                fp_boe += 1\n",
    "\n",
    "\n",
    "    def compute_f1_p_r(tp, fp, fn):\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "        return f1, precision, recall\n",
    "\n",
    "    fn = support - tp\n",
    "    fn_boe = support_boe - tp_boe\n",
    "    return compute_f1_p_r(tp, fp, fn), compute_f1_p_r(tp_boe, fp_boe, fn_boe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658abfa4-9a29-48e0-98eb-1b140216c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = get_predictions_using_windows(test_data)\n",
    "# (f1, precision, recall), (f1_boe, precision_boe, recall_boe) = compute_scores(\n",
    "#     test_data, predictions\n",
    "# )\n",
    "# (f1, precision, recall), (f1_boe, precision_boe, recall_boe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c6fb059-0777-4e1c-846f-d766603b160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (f1, precision, recall), (f1_boe, precision_boe, recall_boe) = compute_scores(\n",
    "#     test_data, predictions\n",
    "# )\n",
    "# (f1, precision, recall), (f1_boe, precision_boe, recall_boe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "000fd83c-fd03-4e8a-a3da-d8df70477d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ta.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ar.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/fa.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/sr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/tr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/de.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/es.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ja.jsonl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9282b8ff-6c64-4d0d-90d0-3526193cd5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /fsx/movb/data/matcha/mewsli-9/ta.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 11604.33it/s]\n",
      "100%|██████████| 14/14 [01:11<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0490, precision = 0.0410, recall = 0.0609\n",
      "F1 boe = 0.3388, precision = 0.2991, recall = 0.3908\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/ar.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1468it [00:00, 2019.90it/s]\n",
      "100%|██████████| 30/30 [02:36<00:00,  5.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0107, precision = 0.0085, recall = 0.0143\n",
      "F1 boe = 0.3904, precision = 0.3109, recall = 0.5243\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/en.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12679it [00:02, 5611.53it/s]\n",
      "100%|██████████| 200/200 [16:23<00:00,  4.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0075, precision = 0.0065, recall = 0.0089\n",
      "F1 boe = 0.4499, precision = 0.4063, recall = 0.5038\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/fa.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [00:00, 10144.39it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0491, precision = 0.0370, recall = 0.0729\n",
      "F1 boe = 0.4552, precision = 0.3529, recall = 0.6408\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/sr.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15011it [00:01, 9743.64it/s] \n",
      "100%|██████████| 169/169 [13:48<00:00,  4.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0946, precision = 0.0667, recall = 0.1627\n",
      "F1 boe = 0.3871, precision = 0.2780, recall = 0.6372\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/tr.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "997it [00:00, 21305.42it/s]\n",
      "100%|██████████| 11/11 [00:53<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.1327, precision = 0.1362, recall = 0.1294\n",
      "F1 boe = 0.5149, precision = 0.5488, recall = 0.4849\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/de.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13703it [00:02, 6112.08it/s]\n",
      "100%|██████████| 189/189 [15:14<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.2122, precision = 0.1566, recall = 0.3290\n",
      "F1 boe = 0.4329, precision = 0.3339, recall = 0.6154\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/es.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10284it [00:01, 7382.07it/s]\n",
      "100%|██████████| 144/144 [11:51<00:00,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0806, precision = 0.0651, recall = 0.1059\n",
      "F1 boe = 0.4563, precision = 0.3852, recall = 0.5596\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/ja.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3410it [00:00, 6122.71it/s]\n",
      "100%|██████████| 47/47 [03:48<00:00,  4.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0002, precision = 0.0003, recall = 0.0001\n",
      "F1 boe = 0.2689, precision = 0.4229, recall = 0.1971\n"
     ]
    }
   ],
   "source": [
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    test_data = load_file(test_data_path)\n",
    "    # test_data = test_data[0:300]\n",
    "    \n",
    "    predictions = get_predictions_using_windows(test_data)\n",
    "    (f1, precision, recall), (f1_boe, precision_boe, recall_boe) = compute_scores(test_data, predictions)\n",
    "    \n",
    "    print(f\"F1 = {f1:.4f}, precision = {precision:.4f}, recall = {recall:.4f}\")\n",
    "    print(f\"F1 boe = {f1_boe:.4f}, precision = {precision_boe:.4f}, recall = {recall_boe:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c8bdc31-fdfe-4193-a7a8-e23ee43b9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_shift(text):\n",
    "    for idx,ch in enumerate(text):\n",
    "        if not ch.isalpha():\n",
    "            return idx\n",
    "\n",
    "def convert_data_for_disambiguation(data, lang):\n",
    "    # convert examples to 1 entity per example and shift if needed\n",
    "    if lang=='ar':\n",
    "        MAX_LENGTH = 600\n",
    "        MAX_OFFSET = 400\n",
    "    elif lang == 'ja':\n",
    "        MAX_LENGTH = 350\n",
    "        MAX_OFFSET = 250\n",
    "    else:\n",
    "        MAX_LENGTH = 800\n",
    "        MAX_OFFSET = 600\n",
    "    new_examples = []\n",
    "    for example in tqdm(data):\n",
    "        original_text = example['original_text']\n",
    "        for _, _, ent, _, offset, length in example['gt_entities']:\n",
    "            shift = 0\n",
    "            if len(original_text) > MAX_LENGTH and offset > MAX_OFFSET:\n",
    "                shift = (offset - MAX_OFFSET)\n",
    "                shift += shift_shift(original_text[shift:])\n",
    "            new_example = {\n",
    "                'original_text': original_text[shift:],\n",
    "                'gt_entities': [[0,0,ent,_,offset-shift,length]],\n",
    "            }\n",
    "            new_examples.append(new_example)\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a77ac96a-b29e-45c3-9ab1-43ddcd387572",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ta.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ar.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/fa.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/sr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/tr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/de.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/es.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ja.jsonl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d8d8cbe-523c-4e9e-8212-06e5f56e7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_disambiguation(test_data, predictions):\n",
    "    support = 0\n",
    "    support_only_predicted = 0\n",
    "    correct = 0\n",
    "    incorrect_pos = 0\n",
    "\n",
    "    for example_idx, (example, prediction) in tqdm(enumerate(zip(test_data, predictions))):\n",
    "#         targets = {\n",
    "#             (offset,length):ent_id\n",
    "#             for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "#         }\n",
    "#         prediction = {\n",
    "#             (offset,length):ent_id\n",
    "#             for offset,length,ent_id in zip(prediction['offsets'], prediction['lengths'], prediction['entities'])\n",
    "#         }\n",
    "\n",
    "#         support += len(targets)\n",
    "#         support_only_predicted += len(prediction)\n",
    "        \n",
    "#         correct += sum(1 for pos,ent_id in prediction.items() if (pos in targets and targets[pos] == ent_id))\n",
    "#         incorrect_pos += sum(1 for pos,_ in prediction.items() if pos not in targets)\n",
    "        if len(prediction['entities']) == 0:\n",
    "            continue\n",
    "        target = example['gt_entities'][0][2]\n",
    "        prediction = prediction['entities'][0]\n",
    "        correct += (target == prediction)\n",
    "        support += 1\n",
    "\n",
    "    accuracy = correct/support\n",
    "    # accuracy_only_predicted = correct/support_only_predicted\n",
    "\n",
    "    return accuracy, support #, accuracy_only_predicted, support_only_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0352a459-b764-439b-873a-481061055348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /fsx/movb/data/matcha/mewsli-9/ta.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 11419.94it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3834.62it/s]\n",
      "100%|██████████| 11/11 [00:47<00:00,  4.29s/it]\n",
      "2692it [00:00, 1057513.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracty 0.8959881129271917, support 2692\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/ar.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1468it [00:00, 9301.85it/s] \n",
      "100%|██████████| 1468/1468 [00:00<00:00, 79520.06it/s]\n",
      "100%|██████████| 29/29 [02:08<00:00,  4.42s/it]\n",
      "7367it [00:00, 1161109.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracty 0.8839419030813085, support 7367\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/en.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12679it [00:00, 67349.94it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 12563.32it/s]\n",
      "100%|██████████| 247/247 [17:07<00:00,  4.16s/it]\n",
      "63164it [00:00, 915818.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracty 0.8362519644632606, support 62358\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/fa.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [00:00, 9809.22it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 253873.87it/s]\n",
      "100%|██████████| 3/3 [00:07<00:00,  2.62s/it]\n",
      "535it [00:00, 785422.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracty 0.8953271028037383, support 535\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/sr.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15011it [00:00, 16498.93it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 342448.07it/s]\n",
      "100%|██████████| 102/102 [06:37<00:00,  3.89s/it]\n",
      "25957it [00:00, 1128015.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracty 0.9280292814486611, support 25955\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/tr.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "997it [00:00, 21980.48it/s]\n",
      "100%|██████████| 997/997 [00:00<00:00, 120129.88it/s]\n",
      "100%|██████████| 23/23 [01:28<00:00,  3.83s/it]\n",
      "5811it [00:00, 949848.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracty 0.8652555498193082, support 5811\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/de.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13703it [00:02, 6202.94it/s] \n",
      "100%|██████████| 10000/10000 [00:00<00:00, 12988.24it/s]\n",
      "100%|██████████| 186/186 [12:39<00:00,  4.08s/it]\n",
      "47603it [00:00, 968088.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracty 0.896246034913766, support 47603\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/es.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10284it [00:01, 7281.39it/s] \n",
      "100%|██████████| 10000/10000 [00:00<00:00, 12100.67it/s]\n",
      "100%|██████████| 214/214 [14:12<00:00,  3.98s/it]\n",
      "54713it [00:00, 916439.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracty 0.8769031126057792, support 54713\n",
      "Processing /fsx/movb/data/matcha/mewsli-9/ja.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3410it [00:00, 6116.93it/s]\n",
      "100%|██████████| 3410/3410 [00:00<00:00, 71673.07it/s]\n",
      "100%|██████████| 135/135 [08:33<00:00,  3.80s/it]\n",
      "34463it [00:00, 850289.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracty 0.841477569496837, support 34462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    lang = test_data_path[-8:-6]\n",
    "    test_data = load_file(test_data_path)\n",
    "    test_data = convert_data_for_disambiguation(test_data[:10000], lang)\n",
    "    predictions = model_eval.get_disambiguation_predictions(test_data)\n",
    "    accuracy, support = metrics_disambiguation(test_data, predictions)\n",
    "    print(f\"Accuracty {accuracy}, support {support}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ed791-41fa-4f19-8170-5b1857059681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bela3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "235bcda9aabf5f8363267684c8e0d7a57e9fb12569fe58ae215aef35e2f0a58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
