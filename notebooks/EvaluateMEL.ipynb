{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff95f192-5b1e-4caf-a5cc-3290b0aebf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yaml\n",
    "from hydra.experimental import compose, initialize_config_module\n",
    "import hydra\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import faiss\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "from bela.transforms.spm_transform import SPMTransform\n",
    "from bela.evaluation.model_eval import ModelEval, load_file\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db8ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_transform = SPMTransform(max_seq_len=100000)\n",
    "\n",
    "\n",
    "def get_windows(text):\n",
    "    tokens = sp_transform([text])[0]\n",
    "    tokens = tokens[1:-1]\n",
    "    window_length = 254\n",
    "    windows = []\n",
    "    for window_start in range(0,len(tokens),window_length//2):\n",
    "        start_pos = tokens[window_start][1]\n",
    "        if window_start + window_length >= len(tokens):\n",
    "            end_pos = tokens[-1][2]\n",
    "        else:\n",
    "            end_pos = tokens[window_start + window_length][2]\n",
    "        windows.append((start_pos, end_pos))\n",
    "    return windows\n",
    "\n",
    "    \n",
    "def convert_predictions_to_dict(example_predictions):\n",
    "    predictions = []\n",
    "    if len(example_predictions) > 0:\n",
    "        offsets, lengths, entities, md_scores, el_scores = zip(*example_predictions) \n",
    "    else:\n",
    "        offsets, lengths, entities, md_scores, el_scores = [], [], [], [], []\n",
    "    return {\n",
    "        'offsets': offsets,\n",
    "        'lengths': lengths,\n",
    "        'entities': entities,\n",
    "        'md_scores': md_scores,\n",
    "        'el_scores': el_scores,\n",
    "    }\n",
    "\n",
    "\n",
    "def group_predictions_by_example(all_predictions, extended_examples):\n",
    "    grouped_predictions = defaultdict(list)\n",
    "    for prediction, extended_example in zip(all_predictions, extended_examples):\n",
    "        window_start = extended_example['window_start']\n",
    "        prediction = dict(prediction)\n",
    "        prediction['offsets'] = [offset + window_start for offset in prediction['offsets']]\n",
    "        grouped_predictions[extended_example['document_id']].append((\n",
    "            prediction\n",
    "        ))\n",
    "    \n",
    "    predictions = {}\n",
    "    for document_id, example_prediction_list in grouped_predictions.items():\n",
    "        example_predictions = []\n",
    "        for prediction in example_prediction_list:\n",
    "            for offset,length,ent,md_score,el_score in zip(\n",
    "                prediction['offsets'],\n",
    "                prediction['lengths'],\n",
    "                prediction['entities'],\n",
    "                prediction['md_scores'],\n",
    "                prediction['el_scores'],\n",
    "            ):\n",
    "                example_predictions.append((offset,length,ent,md_score,el_score))\n",
    "                example_predictions = sorted(example_predictions)\n",
    "        predictions[document_id] = example_predictions\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "    \n",
    "def group_predictions_by_example(all_predictions, extended_examples):\n",
    "    grouped_predictions = defaultdict(list)\n",
    "    for prediction, extended_example in zip(all_predictions, extended_examples):\n",
    "        window_start = extended_example['window_start']\n",
    "        prediction = dict(prediction)\n",
    "        prediction['offsets'] = [offset + window_start for offset in prediction['offsets']]\n",
    "        grouped_predictions[extended_example['document_id']].append((\n",
    "            prediction\n",
    "        ))\n",
    "    \n",
    "    predictions = {}\n",
    "    for document_id, example_prediction_list in grouped_predictions.items():\n",
    "        example_predictions = []\n",
    "        for prediction in example_prediction_list:\n",
    "            for offset,length,ent,md_score,el_score in zip(\n",
    "                prediction['offsets'],\n",
    "                prediction['lengths'],\n",
    "                prediction['entities'],\n",
    "                prediction['md_scores'],\n",
    "                prediction['el_scores'],\n",
    "            ):\n",
    "                example_predictions.append((offset,length,ent,md_score,el_score))\n",
    "                example_predictions = sorted(example_predictions)\n",
    "        predictions[document_id] = example_predictions\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def merge_predictions(example_predictions):\n",
    "    filtered_example_predictions = []\n",
    "\n",
    "    current_end = None\n",
    "    current_offset = None\n",
    "    current_length = None\n",
    "    current_ent_id = None\n",
    "    current_md_score = None\n",
    "    current_el_score = None\n",
    "\n",
    "    for offset,length,ent_id,md_score,el_score in example_predictions:\n",
    "        if current_end is None:\n",
    "            current_end = offset + length\n",
    "            current_offset = offset\n",
    "            current_length = length\n",
    "            current_ent_id = ent_id\n",
    "            current_md_score = md_score\n",
    "            current_el_score = el_score\n",
    "            continue\n",
    "\n",
    "        if offset < current_end:\n",
    "            # intersection of two predictions\n",
    "            if md_score > current_md_score:\n",
    "                current_ent_id = ent_id\n",
    "                current_offset = offset\n",
    "                current_length = length\n",
    "                current_md_score = md_score\n",
    "                current_el_score = el_score\n",
    "        else:\n",
    "            filtered_example_predictions.append((\n",
    "                current_offset,\n",
    "                current_length,\n",
    "                current_ent_id,\n",
    "                current_md_score,\n",
    "                current_el_score\n",
    "            ))\n",
    "            current_ent_id = ent_id\n",
    "            current_offset = offset\n",
    "            current_length = length\n",
    "            current_md_score = md_score\n",
    "            current_el_score = el_score\n",
    "\n",
    "        current_end = offset+length\n",
    "\n",
    "    if current_offset is not None:\n",
    "        filtered_example_predictions.append((\n",
    "            current_offset,\n",
    "            current_length,\n",
    "            current_ent_id,\n",
    "            current_md_score,\n",
    "            current_el_score\n",
    "        ))\n",
    "    \n",
    "    return filtered_example_predictions\n",
    "\n",
    "def compute_scores(data, predictions, md_threshold=0.2, el_threshold=0.05):\n",
    "    tp, fp, support = 0, 0, 0\n",
    "    tp_boe, fp_boe, support_boe = 0, 0, 0\n",
    "\n",
    "    predictions_per_example = []\n",
    "    for example, example_predictions in zip(data, predictions):\n",
    "\n",
    "        example_targets = {\n",
    "            (offset,length):ent_id\n",
    "            for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "        }\n",
    "\n",
    "        example_predictions = {\n",
    "            (offset, length):ent_id\n",
    "            for offset, length, ent_id, md_score, el_score in zip(\n",
    "                example_predictions['offsets'],\n",
    "                example_predictions['lengths'],\n",
    "                example_predictions['entities'],\n",
    "                example_predictions['md_scores'],\n",
    "                example_predictions['el_scores'],\n",
    "            )\n",
    "            if (el_score > el_threshold and md_score > md_threshold) \n",
    "        }\n",
    "\n",
    "        predictions_per_example.append((len(example_targets), len(example_predictions)))\n",
    "\n",
    "        for pos, ent in example_targets.items():\n",
    "            support += 1\n",
    "            if pos in example_predictions and example_predictions[pos] == ent:\n",
    "                tp += 1\n",
    "        for pos, ent in example_predictions.items():\n",
    "            if pos not in example_targets or example_targets[pos] != ent:\n",
    "                fp += 1\n",
    "\n",
    "        example_targets_set = set(example_targets.values())\n",
    "        example_predictions_set = set(example_predictions.values())\n",
    "\n",
    "        for ent in example_targets_set:\n",
    "            support_boe += 1\n",
    "            if ent in example_predictions_set:\n",
    "                tp_boe += 1\n",
    "        for ent in example_predictions_set:\n",
    "            if ent not in example_targets_set:\n",
    "                fp_boe += 1\n",
    "\n",
    "    def safe_division(a, b):\n",
    "        if b == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return a / b\n",
    "\n",
    "\n",
    "    def compute_f1_p_r(tp, fp, fn):\n",
    "        precision = safe_division(tp, (tp + fp))\n",
    "        recall = safe_division(tp, (tp + fn))\n",
    "        f1 = safe_division(2 * tp, (2 * tp + fp + fn))\n",
    "        return f1, precision, recall\n",
    "\n",
    "    fn = support - tp\n",
    "    fn_boe = support_boe - tp_boe\n",
    "    return compute_f1_p_r(tp, fp, fn), compute_f1_p_r(tp_boe, fp_boe, fn_boe)\n",
    "\n",
    "def correct_mention_offsets(example_predictions, text):\n",
    "    TEXT_SEPARATORS = [' ','.',',','!','?','-','\\n']\n",
    "    corrected_example_predictions = []\n",
    "    for offset,length,ent_id,md_score,el_score in example_predictions:\n",
    "        while offset !=0 and offset<len(text) and (text[offset-1] not in TEXT_SEPARATORS or text[offset] in TEXT_SEPARATORS):\n",
    "            offset += 1\n",
    "            length -= 1\n",
    "        while offset+length < len(text) and text[offset+length] not in TEXT_SEPARATORS:\n",
    "            length += 1\n",
    "        corrected_example_predictions.append((\n",
    "            offset,length,ent_id,md_score,el_score\n",
    "        ))\n",
    "    return corrected_example_predictions\n",
    "\n",
    "\n",
    "def get_predictions_using_windows(test_data):\n",
    "    extended_examples = []\n",
    "\n",
    "    for example in test_data:\n",
    "        text = example['original_text']\n",
    "        windows = get_windows(text)\n",
    "        for idx, (start_pos, end_pos) in enumerate(windows):\n",
    "            new_text = text[start_pos:end_pos]\n",
    "            extended_examples.append({\n",
    "                'document_id': example['document_id'],\n",
    "                'original_text': new_text,\n",
    "                'gt_entities': example['gt_entities'],\n",
    "                'window_idx': idx,\n",
    "                'window_start': start_pos,\n",
    "                'window_end': end_pos,\n",
    "            })\n",
    "\n",
    "    all_predictions = model_eval.get_predictions(extended_examples)\n",
    "    predictions_dict = group_predictions_by_example(all_predictions, extended_examples)\n",
    "\n",
    "    predictions = []\n",
    "    for example in test_data:\n",
    "        document_id = example['document_id']\n",
    "        text = example['original_text']\n",
    "        example_predictions = predictions_dict[document_id]\n",
    "        example_predictions = merge_predictions(example_predictions)\n",
    "        example_predictions = correct_mention_offsets(example_predictions, text)\n",
    "        example_predictions = convert_predictions_to_dict(example_predictions)\n",
    "        predictions.append(example_predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def shift_shift(text):\n",
    "    for idx,ch in enumerate(text):\n",
    "        if not ch.isalpha():\n",
    "            return idx\n",
    "\n",
    "def convert_data_for_disambiguation(data, lang):\n",
    "    # convert examples to 1 entity per example and shift if needed\n",
    "    if lang=='ar':\n",
    "        MAX_LENGTH = 600\n",
    "        MAX_OFFSET = 400\n",
    "    elif lang == 'ja':\n",
    "        MAX_LENGTH = 350\n",
    "        MAX_OFFSET = 250\n",
    "    else:\n",
    "        MAX_LENGTH = 800\n",
    "        MAX_OFFSET = 600\n",
    "    new_examples = []\n",
    "    for example in tqdm(data):\n",
    "        original_text = example['original_text']\n",
    "        for _, _, ent, _, offset, length in example['gt_entities']:\n",
    "            shift = 0\n",
    "            if len(original_text) > MAX_LENGTH and offset > MAX_OFFSET:\n",
    "                shift = (offset - MAX_OFFSET)\n",
    "                shift += shift_shift(original_text[shift:])\n",
    "            new_example = {\n",
    "                'original_text': original_text[shift:],\n",
    "                'gt_entities': [[0,0,ent,_,offset-shift,length]],\n",
    "            }\n",
    "            new_examples.append(new_example)\n",
    "    return new_examples\n",
    "\n",
    "\n",
    "def metrics_disambiguation(test_data, predictions):\n",
    "    support = 0\n",
    "    support_only_predicted = 0\n",
    "    correct = 0\n",
    "    incorrect_pos = 0\n",
    "\n",
    "    for example_idx, (example, prediction) in tqdm(enumerate(zip(test_data, predictions))):\n",
    "#         targets = {\n",
    "#             (offset,length):ent_id\n",
    "#             for _,_,ent_id,_,offset,length in example['gt_entities']\n",
    "#         }\n",
    "#         prediction = {\n",
    "#             (offset,length):ent_id\n",
    "#             for offset,length,ent_id in zip(prediction['offsets'], prediction['lengths'], prediction['entities'])\n",
    "#         }\n",
    "\n",
    "#         support += len(targets)\n",
    "#         support_only_predicted += len(prediction)\n",
    "        \n",
    "#         correct += sum(1 for pos,ent_id in prediction.items() if (pos in targets and targets[pos] == ent_id))\n",
    "#         incorrect_pos += sum(1 for pos,_ in prediction.items() if pos not in targets)\n",
    "        if len(prediction['entities']) == 0:\n",
    "            continue\n",
    "        target = example['gt_entities'][0][2]\n",
    "        prediction = prediction['entities'][0]\n",
    "        correct += (target == prediction)\n",
    "        support += 1\n",
    "\n",
    "    accuracy = correct/support\n",
    "    # accuracy_only_predicted = correct/support_only_predicted\n",
    "\n",
    "    return accuracy, support #, accuracy_only_predicted, support_only_predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f8f2dc7",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf93bf95-295e-4efd-8133-bd9aedf156d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun -s cumulative -q -l 20 -T prun_2.txt\n",
    "\n",
    "# e2e model with isotropic embeddings\n",
    "checkpoint_path = '/checkpoints/movb/bela/2023-01-18-220105/0/lightning_logs/version_4820/checkpoints/last.ckpt'\n",
    "checkpoint_path = '/checkpoints/movb/bela/2023-01-13-023711/0/lightning_logs/version_4144/checkpoints/last.ckpt'\n",
    "checkpoint_path = '/checkpoints/movb/bela/2022-11-27-225013/0/lightning_logs/version_286287/checkpoints/last_15000.ckpt'  # Works\n",
    "#model_eval = ModelEval(checkpoint_path, config_name=\"joint_el_mel_new\")\n",
    "model_eval = ModelEval(checkpoint_path, config_name=\"joint_el_mel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0516d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set thresholds\n",
    "model_eval.task.md_threshold = 0.05\n",
    "model_eval.task.el_threshold = 0.05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9df37725",
   "metadata": {},
   "source": [
    "# End-to-end Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "000fd83c-fd03-4e8a-a3da-d8df70477d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "269it [00:00, 4578.08it/s]\n",
      "100%|█████████████████████████████████████████████████████| 15/15 [01:27<00:00,  5.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0100, precision = 0.0330, recall = 0.0059\n",
      "F1 boe = 0.2576, precision = 0.3394, recall = 0.2076\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ta.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ar.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/fa.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/sr.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/tr.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/de.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/es.jsonl',\n",
    "    #'/fsx/movb/data/matcha/mewsli-9/ja.jsonl',\n",
    "    \"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl\"\n",
    "]\n",
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    test_data = load_file(test_data_path)\n",
    "    #test_data = [sample for sample in test_data if \"ENG_\" in sample[\"document_id\"]]\n",
    "    #test_data = test_data[200:201]\n",
    "    #test_data = [sample for sample in test_data if \"document_id\" in sample]\n",
    "    # test_data = test_data[0:300]\n",
    "    \n",
    "    predictions = get_predictions_using_windows(test_data)\n",
    "    (f1, precision, recall), (f1_boe, precision_boe, recall_boe) = compute_scores(test_data, predictions)\n",
    "    \n",
    "    print(f\"F1 = {f1:.4f}, precision = {precision:.4f}, recall = {recall:.4f}\")\n",
    "    print(f\"F1 boe = {f1_boe:.4f}, precision = {precision_boe:.4f}, recall = {recall_boe:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6da72624",
   "metadata": {},
   "source": [
    "# Disambiguation Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ta.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ar.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/en.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/fa.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/sr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/tr.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/de.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/es.jsonl',\n",
    "    '/fsx/movb/data/matcha/mewsli-9/ja.jsonl',\n",
    "]\n",
    "for test_data_path in datasets:\n",
    "    print(f\"Processing {test_data_path}\")\n",
    "    lang = test_data_path[-8:-6]\n",
    "    test_data = load_file(test_data_path)\n",
    "    test_data = convert_data_for_disambiguation(test_data[:10000], lang)\n",
    "    predictions = model_eval.get_disambiguation_predictions(test_data)\n",
    "    accuracy, support = metrics_disambiguation(test_data, predictions)\n",
    "    print(f\"Accuracty {accuracy}, support {support}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2930a292",
   "metadata": {},
   "source": [
    "# Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_samples(samples, batch_size):\n",
    "    # Yield batches of samples\n",
    "    for i in range(0, len(samples), batch_size):\n",
    "        yield samples[i : i + batch_size]\n",
    "\n",
    "\n",
    "texts = [sample[\"original_text\"] for sample in load_file(\"/fsx/movb/data/matcha/mewsli-9/en.jsonl\")]\n",
    "batch_size = 2048\n",
    "print(f\"Processing {len(texts)} texts, {batch_size=}\")\n",
    "%time _ = [model_eval.process_batch(batch_texts) for batch_texts in tqdm(batch_samples(texts, batch_size))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e9e7307",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a77ac96a-b29e-45c3-9ab1-43ddcd387572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = test_data[200]\n",
    "text = sample[\"original_text\"]\n",
    "ground_truth_entities = [entity[2] for entity in sample[\"gt_entities\"]]  # gt_entities = [[0, 0, 873, 14, 'Q7747', 'wiki'], ...]\n",
    "prediction = predictions[200]\n",
    "print(f\"{ground_truth_entities=}\")\n",
    "#print(f\"{text=} -> {prediction=}\")\n",
    "for offset, length, entity, md_score, el_score in zip(prediction[\"offsets\"], prediction[\"lengths\"], prediction[\"entities\"], prediction[\"md_scores\"], prediction[\"el_scores\"]):\n",
    "    print(f\"{text[offset:offset+length]} -> {entity=}, {md_score=}, {el_score=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "235bcda9aabf5f8363267684c8e0d7a57e9fb12569fe58ae215aef35e2f0a58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
