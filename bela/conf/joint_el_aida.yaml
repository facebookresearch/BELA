defaults:
  - config
  - datamodule: joint_el_datamodule
  - task/transform: joint_el_xlmr_transform
  - task/model: xlmr
  - task/optim: adamw
  - trainer: gpu_1_host

task:
  _target_: bela.task.joint_el_task.JointELTask
  embeddings_path: /fsx/movb/data/matcha/en/embeddings_500K.pt
  optim:
    lr: 5e-05
  # pretraining
  # load_from_checkpoint: "/checkpoints/movb/hydra_outputs/main/2021-09-13-001149/0/lightning_logs/version_89311/checkpoints/checkpoint_best.ckpt"
  # trained on 1st par of wikipedia
  # load_from_checkpoint: "/data/home/movb/work/BELA/outputs/2022-03-01/10-48-13/lightning_logs/version_0/checkpoints/checkpoint_best.ckpt"
  # fine-tuned on AIDA
  load_from_checkpoint: "/data/home/movb/work/BELA/outputs/2022-04-12/02-07-45/lightning_logs/version_0/checkpoints/checkpoint_best.ckpt"
  train_saliency: False
  use_gpu_index: True
  transform:
    max_seq_len: 256

datamodule:
  # train_path: /fsx/movb/data/matcha/en/wiki_train_1st_par.txt
  # val_path: /fsx/movb/data/matcha/en/wiki_eval_1st_par.txt
  # test_path: /fsx/movb/data/matcha/en/wiki_test_1st_par.txt
  # AIDA -en
  train_path: /fsx/movb/data/matcha/aida/aida_train.jsonl_split
  val_path: /fsx/movb/data/matcha/aida/aida_testb.jsonl_split
  test_path: /fsx/movb/data/matcha/aida/aida_testa.jsonl_split
  ent_catalogue_idx_path: /fsx/movb/data/matcha/en/embeddings_500K_entity_ids.txt
  batch_size: 12
  use_raw_text: false

trainer:
  max_epochs: 30

checkpoint_callback:
    monitor: valid_f1
