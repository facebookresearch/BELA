defaults:
  - config
  - datamodule: joint_el_datamodule
  - task/transform: joint_el_xlmr_raw_transform_large
  - task/model: xlmr_large
  - task/optim: adamw
  - trainer: gpu_1_host

task:
  _target_: bela.task.joint_el_task.JointELTask
  only_train_disambiguation: false
  train_saliency: false
  embeddings_path: /fsx/movb/data/matcha/mel/embeddings_new.pt
  optim:
    lr: 1e-05
  use_gpu_index: true
  # disambiguation checkpoint
  load_from_checkpoint: "/checkpoints/movb/bela/2023-02-15-200343/0/lightning_logs/version_127953/checkpoints/checkpoint_8.ckpt"
datamodule:
  train_path: /fsx/movb/data/matcha/mewsli-9-splitted/train_split.jsonl
  val_path: /fsx/movb/data/matcha/mewsli-9-splitted/val_split.jsonl
  test_path: /fsx/movb/data/matcha/mewsli-9-splitted/test_split.jsonl
  ent_catalogue_idx_path: /fsx/movb/data/matcha/mel/index_new.txt
  batch_size: 24

trainer:
  max_epochs: 30

checkpoint_callback:
    monitor: valid_f1