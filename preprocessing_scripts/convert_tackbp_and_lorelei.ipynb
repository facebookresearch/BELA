{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "from itertools import islice, product\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "#import itables.interactive\n",
    "import pandas as pd\n",
    "# Set interactive pandas\n",
    "#itables.options.mode = \"notebook\"\n",
    "\n",
    "from bela.utils.analysis_utils import Sample, Entity\n",
    "\n",
    "\n",
    "def yield_jsonl_lines(path):\n",
    "    with open(path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "def inspect_json(json_dict, indent=0, max_elements=3):\n",
    "    \"\"\"Only print the first 3 keys of a dict, recursively\"\"\"\n",
    "    for i, (key, value) in enumerate(json_dict.items()):\n",
    "        if isinstance(value, dict):\n",
    "            print(\"  \" * indent + f\"{key}:\")\n",
    "            inspect_json(value, indent=indent + 1)\n",
    "        else:\n",
    "            print(\"  \" * indent + f\"{key}: {value}\")\n",
    "        if i + 1 == max_elements:\n",
    "            print(\"  \" * indent + \"...\")\n",
    "            break\n",
    "\n",
    "\n",
    "def read_jsonl_file(path, n_lines=None):\n",
    "    return list(tqdm(islice(yield_jsonl_lines(path), n_lines), total=n_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DEPRECATED_infer_original_text(texts):\n",
    "    \"\"\"Given a list of texts, corresponding to the same document but with different [START] mention [END] tokens, try to infer the original text by removing the [START] and [END] tokens and the extra spaces that were added.\"\"\"\n",
    "    text_counter = Counter()\n",
    "    for text in texts:\n",
    "        left_context, mention, right_context = re.match(r\"(.*) \\[START\\] (.*) \\[END\\] (.*)\", text, re.DOTALL).groups()  # re.DOTALL to match newlines with .\n",
    "        # We don't know if there is a space between the context and the mention. Sometimes there was even 2 in the original. Try all combinations.\n",
    "        # There are multiple possible characters that were stripped\n",
    "        #possible_characters = [\"\", \" \", \"\\n\", \"\\u3000\", \"\\u3000\\u3000\"]  # \\u3000 happens a lot in chinese texts\n",
    "        # Sometimes there can be a sequence of 2 of them\n",
    "        #possible_separators = set([\"\".join(sep) for sep in product(possible_characters, repeat=2)])\n",
    "        possible_separators = [\"\", \" \", \"\\n\", \"\\u3000\", \" \\n\", \"\\n\\n\", \"\\u3000\\u3000\", \" \\u3000\"]\n",
    "        # Then we can have either of those before and after the mention\n",
    "        for sep_1, sep_2 in product(possible_separators, repeat=2):\n",
    "            text = f\"{left_context}{sep_1}{mention}{sep_2}{right_context}\"\n",
    "            text_counter[text] += 1\n",
    "    #assert text_counter.most_common()[0][1] != text_counter.most_common()[1][1], f\"Could not infer original text, the most common text is not unique: {text_counter.most_common()[:2]}\"\n",
    "    if len(texts) > 1 and text_counter.most_common()[0][1] == text_counter.most_common()[1][1]:\n",
    "        print(f\"Could not infer original text, the most common text is not unique: {text_counter.most_common()[:2]}\")\n",
    "    # Assume that the text with the more occurences is the correct one\n",
    "    #assert text_counter.most_common()[0][1] > 1, f\"Could not infer original text, the most common text occurs only once: {text_counter.most_common()[0][0]}\"\n",
    "    if len(texts) > 1 and text_counter.most_common()[0][1] <= 1:\n",
    "        print(f\"Could not infer original text, the most common text occurs only once: {text_counter.most_common()[0][0]}\")\n",
    "    return text_counter.most_common()[0][0]\n",
    "\n",
    "def merge_contexts(longuest_left_context, longuest_right_context):\n",
    "    # Concatenate the two contexts after removing the overlapping part\n",
    "    # First find the length of the overlapping part, e.g. if longuest_left_context = \"abcdef\" and longuest_right_context = \"cdefgh\", then the overlapping part is \"cdef\"\n",
    "    overlapping_length = 0\n",
    "    for i in range(1, len(longuest_right_context) + 1):\n",
    "        if longuest_left_context[-i:] == longuest_right_context[:i]:\n",
    "            overlapping_length = i\n",
    "    assert overlapping_length > 3\n",
    "    #if overlapping_length == 0:\n",
    "    #    print(f\"Could not merge contexts: {longuest_left_context} and {longuest_right_context}\")\n",
    "    #    # HACK: This fixes the following edge case\n",
    "    #    # texts = [\n",
    "    #    #     '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en [START] Charleston [END] SC </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>',\n",
    "    #    #     '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en Charleston [START] SC [END] </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>'\n",
    "    #    # ]\n",
    "    #    # original_text = '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en Charleston SC </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>',\n",
    "    #    # assert infer_original_text(texts) == original_text\n",
    "    #    longuest_left_context += \" \"\n",
    "    return longuest_left_context + longuest_right_context[overlapping_length:]\n",
    "\n",
    "\n",
    "def test_merge_contexts():\n",
    "    assert merge_contexts(\"abcdef\", \"cdefgh\") == \"abcdefgh\"\n",
    "    assert merge_contexts(\"abcdef\", \"cdefghijkl\") == \"abcdefghijkl\"\n",
    "    assert merge_contexts(\"abcdef\", \"cdef\") == \"abcdef\"\n",
    "    assert merge_contexts(\"zabcabc\", \"abcabcde\") == \"zabcabcde\"\n",
    "    assert merge_contexts(\"abcdef\", \"abcdef\") == \"abcdef\"\n",
    "test_merge_contexts()\n",
    "\n",
    "\n",
    "def infer_original_text(texts):\n",
    "    \"\"\"Given a list of texts, corresponding to the same document but with different [START] mention [END] tokens, try to infer the original text by removing the [START] and [END] tokens and the extra spaces that were added.\n",
    "    The big issue is that when adding [START] and [END] some characters were removed (whitespace, newlines, \\u3000 etc.). E.g. \"abc \\n mention    def\" -> \"abc [START] mention [END] def\", so we try to recover the original text\n",
    "    \"\"\"\n",
    "    def infer_original_text_from_single_text(text):\n",
    "        left_context, mention, right_context = re.match(r\"(.*) \\[START\\] (.*) \\[END\\] (.*)\", text, re.DOTALL).groups()  # re.DOTALL to match newlines with .\n",
    "        return f\"{left_context} {mention} {right_context}\"\n",
    "    \n",
    "    if len(texts) == 1:\n",
    "        return infer_original_text_from_single_text(texts[0])\n",
    "\n",
    "    longuest_left_context = max(texts, key=lambda text: len(text.split(\" [START] \")[0])).split(\" [START] \")[0]\n",
    "    longuest_right_context = max(texts, key=lambda text: len(text.split(\" [END] \")[-1])).split(\" [END] \")[-1]\n",
    "    try:\n",
    "        return merge_contexts(longuest_left_context, longuest_right_context)\n",
    "    except AssertionError:\n",
    "        # Fallback\n",
    "        return infer_original_text_from_single_text(texts[0])\n",
    "\n",
    "\n",
    "def test_infer_original_text():\n",
    "    texts = [\n",
    "        '<DOC id=\"CMN_NW_001149_20150314_F0000005H\"> <SOURCE>http://sports.hangzhou.com.cn/zxbd/content/2015-03/14/content_5689329.htm</SOURCE> <DATE_TIME>2015-03-14T00:00:00</DATE_TIME> <HEADLINE> “ [START] 刀锋战士 [END] ”阻止枪杀案件重审 申诉被驳回 </HEADLINE> <TEXT> <P>    在13日的法院审理中，残奥会明星皮斯托瑞斯的法律团队企图阻止其枪杀女友案件重新在最高法院审理的申诉被驳回。 </P> <P> \\u3000\\u30002014年11月4日，政府检察官向法庭提交了上诉文件，文件对皮斯托瑞斯杀死女友案的定罪和判刑均提出了上诉。之后，法庭批准了此项上诉。 </P> <P> \\u3000\\u3000“刀锋战士”的辩护律师企图阻止法院重新开庭审理此案的申诉被驳回后，律师表示控辩双方都有权利陈述自己的观点。 </P> <P> \\u3000\\u3000现年27岁的皮斯托瑞斯在2013年2月14日“情人节”当天，透过浴室的门射杀了女友斯滕坎普。南非北豪登省最高法院2014年9月12日做出决定，判定皮斯托瑞斯谋杀罪名不成立，并裁定他犯有过失杀人罪。 </P> <P> \\u3000\\u3000北豪登省最高法院庭审法官玛希帕2014年10月21日宣布，皮斯托瑞斯因过失导致女友斯滕坎普死亡，将在监狱服刑5年。皮斯托瑞斯表示将不会对此判决提出上诉，他将有资格在服刑10个月后，获得在家禁闭服刑的权利。 </P> <P> \\u3000\\u3000现在法官玛希帕批准了检察官的上诉申请，而驳回了辩护律师的申诉，这意味着这场旷日持久的官司仍将继续。 </P> </TEXT> </DOC>',\n",
    "        '<DOC id=\"CMN_NW_001149_20150314_F0000005H\"> <SOURCE>http://sports.hangzhou.com.cn/zxbd/content/2015-03/14/content_5689329.htm</SOURCE> <DATE_TIME>2015-03-14T00:00:00</DATE_TIME> <HEADLINE> “刀锋战士”阻止枪杀案件重审 申诉被驳回 </HEADLINE> <TEXT> <P>    在13日的法院审理中，残奥会明星 [START] 皮斯托瑞斯 [END] 的法律团队企图阻止其枪杀女友案件重新在最高法院审理的申诉被驳回。 </P> <P> \\u3000\\u30002014年11月4日，政府检察官向法庭提交了上诉文件，文件对皮斯托瑞斯杀死女友案的定罪和判刑均提出了上诉。之后，法庭批准了此项上诉。 </P> <P> \\u3000\\u3000“刀锋战士”的辩护律师企图阻止法院重新开庭审理此案的申诉被驳回后，律师表示控辩双方都有权利陈述自己的观点。 </P> <P> \\u3000\\u3000现年27岁的皮斯托瑞斯在2013年2月14日“情人节”当天，透过浴室的门射杀了女友斯滕坎普。南非北豪登省最高法院2014年9月12日做出决定，判定皮斯托瑞斯谋杀罪名不成立，并裁定他犯有过失杀人罪。 </P> <P> \\u3000\\u3000北豪登省最高法院庭审法官玛希帕2014年10月21日宣布，皮斯托瑞斯因过失导致女友斯滕坎普死亡，将在监狱服刑5年。皮斯托瑞斯表示将不会对此判决提出上诉，他将有资格在服刑10个月后，获得在家禁闭服刑的权利。 </P> <P> \\u3000\\u3000现在法官玛希帕批准了检察官的上诉申请，而驳回了辩护律师的申诉，这意味着这场旷日持久的官司仍将继续。 </P> </TEXT> </DOC>',\n",
    "        '<DOC id=\"CMN_NW_001149_20150314_F0000005H\"> <SOURCE>http://sports.hangzhou.com.cn/zxbd/content/2015-03/14/content_5689329.htm</SOURCE> <DATE_TIME>2015-03-14T00:00:00</DATE_TIME> <HEADLINE> “刀锋战士”阻止枪杀案件重审 申诉被驳回 </HEADLINE> <TEXT> <P>    在13日的法院审理中，残奥会明星皮斯托瑞斯的法律团队企图阻止其枪杀女友案件重新在最高法院审理的申诉被驳回。 </P> <P> \\u3000\\u30002014年11月4日，政府检察官向法庭提交了上诉文件，文件对 [START] 皮斯托瑞斯 [END] 杀死女友案的定罪和判刑均提出了上诉。之后，法庭批准了此项上诉。 </P> <P> \\u3000\\u3000“刀锋战士”的辩护律师企图阻止法院重新开庭审理此案的申诉被驳回后，律师表示控辩双方都有权利陈述自己的观点。 </P> <P> \\u3000\\u3000现年27岁的皮斯托瑞斯在2013年2月14日“情人节”当天，透过浴室的门射杀了女友斯滕坎普。南非北豪登省最高法院2014年9月12日做出决定，判定皮斯托瑞斯谋杀罪名不成立，并裁定他犯有过失杀人罪。 </P> <P> \\u3000\\u3000北豪登省最高法院庭审法官玛希帕2014年10月21日宣布，皮斯托瑞斯因过失导致女友斯滕坎普死亡，将在监狱服刑5年。皮斯托瑞斯表示将不会对此判决提出上诉，他将有资格在服刑10个月后，获得在家禁闭服刑的权利。 </P> <P> \\u3000\\u3000现在法官玛希帕批准了检察官的上诉申请，而驳回了辩护律师的申诉，这意味着这场旷日持久的官司仍将继续。 </P> </TEXT> </DOC>',\n",
    "    ]\n",
    "    original_text = '<DOC id=\"CMN_NW_001149_20150314_F0000005H\"> <SOURCE>http://sports.hangzhou.com.cn/zxbd/content/2015-03/14/content_5689329.htm</SOURCE> <DATE_TIME>2015-03-14T00:00:00</DATE_TIME> <HEADLINE> “刀锋战士”阻止枪杀案件重审 申诉被驳回 </HEADLINE> <TEXT> <P>    在13日的法院审理中，残奥会明星皮斯托瑞斯的法律团队企图阻止其枪杀女友案件重新在最高法院审理的申诉被驳回。 </P> <P> \\u3000\\u30002014年11月4日，政府检察官向法庭提交了上诉文件，文件对皮斯托瑞斯杀死女友案的定罪和判刑均提出了上诉。之后，法庭批准了此项上诉。 </P> <P> \\u3000\\u3000“刀锋战士”的辩护律师企图阻止法院重新开庭审理此案的申诉被驳回后，律师表示控辩双方都有权利陈述自己的观点。 </P> <P> \\u3000\\u3000现年27岁的皮斯托瑞斯在2013年2月14日“情人节”当天，透过浴室的门射杀了女友斯滕坎普。南非北豪登省最高法院2014年9月12日做出决定，判定皮斯托瑞斯谋杀罪名不成立，并裁定他犯有过失杀人罪。 </P> <P> \\u3000\\u3000北豪登省最高法院庭审法官玛希帕2014年10月21日宣布，皮斯托瑞斯因过失导致女友斯滕坎普死亡，将在监狱服刑5年。皮斯托瑞斯表示将不会对此判决提出上诉，他将有资格在服刑10个月后，获得在家禁闭服刑的权利。 </P> <P> \\u3000\\u3000现在法官玛希帕批准了检察官的上诉申请，而驳回了辩护律师的申诉，这意味着这场旷日持久的官司仍将继续。 </P> </TEXT> </DOC>'\n",
    "    infered_text = infer_original_text(texts)\n",
    "    assert original_text == infered_text, f\"original_text != infered_text:\\n{original_text}\\n{infered_text}\"\n",
    "    texts = [\n",
    "        'Công an Hà Nội khủng bố tinh thần anh Trịnh Bá Phương: [START] CÔNG [END] AN HÀ NỘI KHỦNG BỐ TINH THẦN ANH TRỊNH B... https://t.co/bEocspCx5k',\n",
    "        'Công an [START] Hà Nội [END] khủng bố tinh thần anh Trịnh Bá Phương: \\n\\n        CÔNG AN HÀ NỘI KHỦNG BỐ TINH THẦN ANH TRỊNH B... https://t.co/bEocspCx5k',\n",
    "    ]\n",
    "    original_text = \"Công an Hà Nội khủng bố tinh thần anh Trịnh Bá Phương: \\n\\n        CÔNG AN HÀ NỘI KHỦNG BỐ TINH THẦN ANH TRỊNH B... https://t.co/bEocspCx5k\"\n",
    "    infered_text = infer_original_text(texts)\n",
    "    assert original_text == infered_text, f\"original_text != infered_text:\\n{original_text}\\n{infered_text}\"\n",
    "    texts = [\n",
    "        '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en [START] Charleston [END] SC </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>',\n",
    "        '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en Charleston [START] SC [END] </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>',\n",
    "    ]\n",
    "    original_text = '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en Charleston SC </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>'\n",
    "    infered_text = infer_original_text(texts)\n",
    "    assert original_text == infered_text, f\"original_text != infered_text:\\n{original_text}\\n{infered_text}\"\n",
    "    # TODO: This doesn't pass yet because [END] of first mention is [START] of second mention and no overlap\n",
    "    #texts = [\n",
    "    #    '#OromoRevolution \" [START] Godina Arsii Lixaa [END] Aanaa shaallaatti waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU',\n",
    "    #    '#OromoRevolution \"Godina Arsii Lixaa [START] Aanaa shaallaatti [END] waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU',\n",
    "    #]\n",
    "    #original_text = '#OromoRevolution \"Godina Arsii Lixaa Aanaa shaallaatti waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU'\n",
    "    #infered_text = infer_original_text(texts)\n",
    "    #assert original_text == infered_text, f\"original_text != infered_text:\\n{original_text}\\n{infered_text}\"\n",
    "test_infer_original_text()\n",
    "\n",
    "\n",
    "def fix_offset(entity, mention):\n",
    "    assert entity.mention != mention\n",
    "    # Search for the mention close to the start of the entity\n",
    "    fixed_offset = entity.text[entity.offset-5:].index(mention) + entity.offset - 5\n",
    "    entity.offset = fixed_offset\n",
    "    assert entity.mention == mention\n",
    "    return entity\n",
    "\n",
    "\n",
    "def sample_to_json(sample: Sample) -> str:\n",
    "    \"\"\"Convert a sample to a jsonl string.\"\"\"\n",
    "    return {\"data_example_id\": sample.sample_id, \"original_text\": sample.text, \"gt_entities\": [[0, 0, entity.entity_id, \"wiki\", entity.offset, entity.length] for entity in sample.ground_truth_entities]}\n",
    "\n",
    "    \n",
    "def convert_tackbp_json_mentions_to_sample(json_mentions: List[Dict]) -> Sample:\n",
    "    \"\"\"Given mentions from a single document, convert them to a Sample object.\"\"\"\n",
    "    # Check they all come from the same document\n",
    "    assert len(set(json_mention[\"meta\"][\"document_id\"] for json_mention in json_mentions)) == 1\n",
    "    # Convert json_mentions to sample\n",
    "    json_mentions.sort(key=lambda json_mention: json_mention[\"meta\"][\"start_offset\"])\n",
    "    ground_truth_entities = []\n",
    "    text = infer_original_text([json_mention[\"input\"] for json_mention in json_mentions])\n",
    "    for json_mention in json_mentions:\n",
    "        offset = len(json_mention[\"meta\"][\"left_context\"])\n",
    "        length = len(json_mention[\"meta\"][\"mention\"])\n",
    "        # Leading whitespaces\n",
    "        additional_spaces = len(text[offset:]) - len(text[offset:].lstrip(\" \\n\\u3000\"))\n",
    "        offset += additional_spaces\n",
    "        entity = Entity(text=text, offset=offset, length=length, entity_id=json_mention[\"entity_id\"])\n",
    "        # Sometimes there are extra spaces / newlines in the original text mention that were removed in the metadata\n",
    "        # json_mention[\"meta\"][\"mention\"] = 'Must u h a m m a d u Bow u h a r i'\n",
    "        # entity.mention = 'Must u h a m m a d u \\nBow u h a r '\n",
    "        assert entity.entity_id is not None\n",
    "        #assert entity.mention == json_mention[\"meta\"][\"mention\"], f\"{entity.mention=} != {json_mention['meta']['mention']=}\"\n",
    "        if entity.mention != json_mention[\"meta\"][\"mention\"]:\n",
    "            fix_offset(entity, json_mention[\"meta\"][\"mention\"])\n",
    "            #print(f\"{entity.mention=} != {json_mention['meta']['mention']=}\")\n",
    "        ground_truth_entities.append(entity)\n",
    "    return Sample(text=text, sample_id=json_mentions[0][\"meta\"][\"document_id\"], ground_truth_entities=ground_truth_entities)\n",
    "\n",
    "\n",
    "def convert_tackbp_json_mentions_to_samples(json_mentions: List[Dict]) -> List[Sample]:\n",
    "    \"\"\"Given a list of json mentions, group them by document and convert them to Samples objects.\"\"\"\n",
    "    # Group mentions together\n",
    "    aggregated_mentions = defaultdict(list)\n",
    "    for json_mention in json_mentions:\n",
    "        aggregated_mentions[json_mention[\"meta\"][\"document_id\"]].append(json_mention)\n",
    "    samples = [convert_tackbp_json_mentions_to_sample(json_mentions) for json_mentions in tqdm(aggregated_mentions.values())]\n",
    "    return samples\n",
    "\n",
    "\n",
    "def convert_lorelei_json_mentions_to_sample(json_mentions: List[Dict]) -> Sample:\n",
    "    \"\"\"Given a list of json mentions, convert them to a Sample object.\"\"\"\n",
    "    # Check that all mentions are from the same document\n",
    "    assert len(set(json_mention[\"meta\"][\"doc_id\"] for json_mention in json_mentions)) == 1\n",
    "    # Order mentions by start offset\n",
    "    json_mentions.sort(key=lambda json_mention: json_mention[\"meta\"][\"start_char\"])\n",
    "    ground_truth_entities = []\n",
    "    text = infer_original_text([json_mention[\"input\"] for json_mention in json_mentions])\n",
    "    for json_mention in json_mentions:\n",
    "        offset = len(json_mention[\"meta\"][\"left_context\"])\n",
    "        length = len(json_mention[\"meta\"][\"mention\"])\n",
    "        # Leading whitespaces\n",
    "        additional_spaces = len(text[offset:]) - len(text[offset:].lstrip(\" \\n\\u3000\"))\n",
    "        offset += additional_spaces\n",
    "        entity = Entity(text=text, offset=offset, length=length, entity_id=json_mention[\"entity_id\"])\n",
    "        # Sometimes there are extra spaces / newlines in the original text mention that were removed in the metadata\n",
    "        assert entity.entity_id is not None\n",
    "        #assert entity.mention == json_mention[\"meta\"][\"mention\"], f\"{entity.mention=} != {json_mention['meta']['mention']=}\"\n",
    "\n",
    "        if entity.mention != json_mention[\"meta\"][\"mention\"]:\n",
    "            fix_offset(entity, json_mention[\"meta\"][\"mention\"])  # HACK: Sometimes the offset is still not good\n",
    "            #print(f\"{entity.mention=} != {json_mention['meta']['mention']=}\")\n",
    "        ground_truth_entities.append(entity)\n",
    "    return Sample(text=text, sample_id=json_mentions[0][\"meta\"][\"doc_id\"], ground_truth_entities=ground_truth_entities)\n",
    "\n",
    "\n",
    "def convert_lorelei_json_mentions_to_samples(json_mentions: List[Dict]) -> List[Sample]:\n",
    "    \"\"\"Given a list of json mentions, group them by document and convert them to Samples objects.\"\"\"\n",
    "    # Group mentions together\n",
    "    aggregated_mentions = defaultdict(list)\n",
    "    for json_mention in json_mentions:\n",
    "        aggregated_mentions[json_mention[\"meta\"][\"doc_id\"]].append(json_mention)\n",
    "    samples = [convert_lorelei_json_mentions_to_sample(json_mentions) for json_mentions in tqdm(aggregated_mentions.values())]\n",
    "    return samples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the TACKBP jsonl from Nicola De Cao's backup to bela format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8bd0c21b92450a9e5c45f61292ba15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one entity for ENG_NW_001004_20141029_F00000003: ['Q612', 'Q613']. Taking only the first one.\n",
      "More than one entity for SPA_NW_001078_20150105_F0000002W: ['Q693039', 'Q813']. Taking only the first one.\n",
      "More than one entity for SPA_NW_001078_20150105_F0000002W: ['Q693039', 'Q813']. Taking only the first one.\n",
      "More than one entity for SPA_NW_001078_20150105_F0000002W: ['Q693039', 'Q813']. Taking only the first one.\n",
      "More than one entity for SPA_NW_001066_20150105_F0000002H: ['Q693039', 'Q813']. Taking only the first one.\n",
      "More than one entity for ENG_DF_001220_20150404_F0000007K: ['Q612', 'Q613']. Taking only the first one.\n",
      "More than one entity for CMN_DF_000181_20140830_F000000DG: ['Q693039', 'Q813']. Taking only the first one.\n",
      "More than one entity for CMN_DF_000191_20150401_F000000DX: ['Q693039', 'Q813']. Taking only the first one.\n",
      "ENG: 9027 mentions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ed93e337234e579d693fc78642d6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m language_mentions \u001b[39m=\u001b[39m [json_mention \u001b[39mfor\u001b[39;00m json_mention \u001b[39min\u001b[39;00m json_mentions \u001b[39mif\u001b[39;00m json_mention[\u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdocument_id\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mstartswith(language)]\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(language_mentions)\u001b[39m}\u001b[39;00m\u001b[39m mentions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m samples \u001b[39m=\u001b[39m convert_tackbp_json_mentions_to_samples(language_mentions)\n\u001b[1;32m     20\u001b[0m all_samples\u001b[39m.\u001b[39mextend(samples)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(samples)\u001b[39m}\u001b[39;00m\u001b[39m samples\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 156\u001b[0m, in \u001b[0;36mconvert_tackbp_json_mentions_to_samples\u001b[0;34m(json_mentions)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mfor\u001b[39;00m json_mention \u001b[39min\u001b[39;00m json_mentions:\n\u001b[1;32m    155\u001b[0m     aggregated_mentions[json_mention[\u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdocument_id\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mappend(json_mention)\n\u001b[0;32m--> 156\u001b[0m samples \u001b[39m=\u001b[39m [convert_tackbp_json_mentions_to_sample(json_mentions) \u001b[39mfor\u001b[39;00m json_mentions \u001b[39min\u001b[39;00m tqdm(aggregated_mentions\u001b[39m.\u001b[39mvalues())]\n\u001b[1;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m samples\n",
      "Cell \u001b[0;32mIn[58], line 156\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mfor\u001b[39;00m json_mention \u001b[39min\u001b[39;00m json_mentions:\n\u001b[1;32m    155\u001b[0m     aggregated_mentions[json_mention[\u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdocument_id\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mappend(json_mention)\n\u001b[0;32m--> 156\u001b[0m samples \u001b[39m=\u001b[39m [convert_tackbp_json_mentions_to_sample(json_mentions) \u001b[39mfor\u001b[39;00m json_mentions \u001b[39min\u001b[39;00m tqdm(aggregated_mentions\u001b[39m.\u001b[39mvalues())]\n\u001b[1;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m samples\n",
      "Cell \u001b[0;32mIn[58], line 144\u001b[0m, in \u001b[0;36mconvert_tackbp_json_mentions_to_sample\u001b[0;34m(json_mentions)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39m#assert entity.mention == json_mention[\"meta\"][\"mention\"], f\"{entity.mention=} != {json_mention['meta']['mention']=}\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m entity\u001b[39m.\u001b[39mmention \u001b[39m!=\u001b[39m json_mention[\u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmention\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 144\u001b[0m     fix_offset(entity, json_mention[\u001b[39m\"\u001b[39;49m\u001b[39mmeta\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mmention\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    145\u001b[0m     \u001b[39m#print(f\"{entity.mention=} != {json_mention['meta']['mention']=}\")\u001b[39;00m\n\u001b[1;32m    146\u001b[0m ground_truth_entities\u001b[39m.\u001b[39mappend(entity)\n",
      "Cell \u001b[0;32mIn[58], line 112\u001b[0m, in \u001b[0;36mfix_offset\u001b[0;34m(entity, mention)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39massert\u001b[39;00m entity\u001b[39m.\u001b[39mmention \u001b[39m!=\u001b[39m mention\n\u001b[1;32m    111\u001b[0m \u001b[39m# Search for the mention close to the start of the entity\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m fixed_offset \u001b[39m=\u001b[39m entity\u001b[39m.\u001b[39;49mtext[entity\u001b[39m.\u001b[39;49moffset\u001b[39m-\u001b[39;49m\u001b[39m5\u001b[39;49m:]\u001b[39m.\u001b[39;49mindex(mention) \u001b[39m+\u001b[39m entity\u001b[39m.\u001b[39moffset \u001b[39m-\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m    113\u001b[0m entity\u001b[39m.\u001b[39moffset \u001b[39m=\u001b[39m fixed_offset\n\u001b[1;32m    114\u001b[0m \u001b[39massert\u001b[39;00m entity\u001b[39m.\u001b[39mmention \u001b[39m==\u001b[39m mention\n",
      "\u001b[0;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "\n",
    "retrieved_data_folder = Path(\"/fsx/louismartin/bela/retrieved_from_aws_backup/\")\n",
    "for phase in [\"train\", \"dev\"]:\n",
    "    kilt_format_path = retrieved_data_folder / f\"ndecao/TACKBP2015/{phase}.jsonl\"\n",
    "    json_mentions = read_jsonl_file(kilt_format_path)\n",
    "    for json_mention in json_mentions:\n",
    "        # json_sample[\"output\"] looks something like [{'KB_ID': 'm.0d06m5', 'answer': ['Q6294']}] \n",
    "        # Convert it to -> ['Q6294']\n",
    "        entities = [entity_id for entity in json_mention[\"output\"] for entity_id in entity[\"answer\"]]\n",
    "        if len(entities) > 1:\n",
    "            print(f\"More than one entity for {json_mention['meta']['document_id']}: {entities}. Taking only the first one.\")\n",
    "        json_mention[\"entity_id\"] = entities[0]\n",
    "    json_mentions = [json_mention for json_mention in json_mentions if json_mention[\"entity_id\"] is not None]\n",
    "\n",
    "    languages = [\"ENG\", \"SPA\", \"CMN\"]\n",
    "    all_samples = []\n",
    "    for language in languages:\n",
    "        language_mentions = [json_mention for json_mention in json_mentions if json_mention[\"meta\"][\"document_id\"].startswith(language)]\n",
    "        print(f\"{language}: {len(language_mentions)} mentions\")\n",
    "        samples = convert_tackbp_json_mentions_to_samples(language_mentions)\n",
    "        all_samples.extend(samples)\n",
    "        print(f\"{language}: {len(samples)} samples\")\n",
    "        output_path = retrieved_data_folder / f\"ndecao/TACKBP2015/{phase}_bela_format_{language.lower()}.jsonl\"\n",
    "        with open(output_path, \"w\") as f:\n",
    "            for sample in samples:\n",
    "                f.write(json.dumps(sample_to_json(sample)) + \"\\n\")\n",
    "    output_path = retrieved_data_folder / f\"ndecao/TACKBP2015/{phase}_bela_format_all_languages.jsonl\"\n",
    "    random.shuffle(all_samples)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for sample in all_samples:\n",
    "            f.write(json.dumps(sample_to_json(sample)) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lorelei\n",
    "Lorelei samples look like this:\n",
    "```\n",
    "{'id': 'Men-VIE_SN_000370_20160324_G0T100LHC-7', 'input': ' [START] Bỉ [END] truy tìm nghi phạm mới trong vụ đánh bom ga tàu điện ngầm Brussels. - Mua bán thiết bị tàu biển https://t.co/Rf0J4cQKgu qua @sharethis', 'output': [{'answer': ['Q31']}], 'meta': {'left_context': '', 'mention': 'Bỉ', 'right_context': 'truy tìm nghi phạm mới trong vụ đánh bom ga tàu điện ngầm Brussels. - Mua bán thiết bị tàu biển https://t.co/Rf0J4cQKgu qua @sharethis', 'doc_id': 'VIE_SN_000370_20160324_G0T100LHC', 'entity_id': 'Ent-VIE_SN_000370_20160324_G0T100LHC-7', 'entity_type': 'GPE', 'mention_status': 'representative', 'start_char': '0', 'end_char': '1', 'original_mention_text': '__\\n', 'system_run_id': 'LDC', 'mention_text_2': '__', 'extents': 'VIE_SN_000370_20160324_G0T100LHC:0-1', 'kb_id': '2802361', 'mention_type': 'NAM', 'confidence': '1.0\\n'}, 'candidates': ['Q31', 'Q166776', 'Q697625', 'Q17435', 'Q815524', 'Q216022', 'Q20997', 'Q205317', 'Q381124', 'Q21000', 'Q1095', 'Q234', 'Q658870', 'Q15873988', 'Q792312', 'Q3992', 'Q792419', 'Q134121', 'Q240']}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/lorelei/lorelei_vietnamese.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9919d9dd244f7da7dac226a0f12985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a1eb35a9da4420b83a781e26d113f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/lorelei/lorelei_oromo.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8648ee74ef54544bfa50a90e8d8e08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedd6c3565c9481aa43bd2a5016a8e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/lorelei/lorelei_tigrinya.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd4505a816b4ae886825d65eb5842ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c7158ad96841d2b7a9ef4428cb8e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/lorelei/lorelei_ukrainian.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845dd08bf711489d97ba73a89ebc4120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c24d237389b41118d5478861169052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lorelei_dir = Path(\"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/lorelei/\")\n",
    "for path in lorelei_dir.glob(\"*.jsonl\"): \n",
    "    if \"bela_format\" in path.name:  # Already processed\n",
    "        continue\n",
    "    print(path)\n",
    "    json_mentions = read_jsonl_file(path)\n",
    "    for json_mention in json_mentions:\n",
    "        # json_sample[\"output\"] looks something like [{'KB_ID': 'm.0d06m5', 'answer': ['Q6294']}] \n",
    "        # Convert it to -> ['Q6294']\n",
    "        entities = [entity_id for entity in json_mention.get(\"output\", []) for entity_id in entity[\"answer\"]]\n",
    "        #if len(entities) > 1:\n",
    "        #    print(f\"More than one entity for {json_mention['meta']['doc_id']}: {entities}. Taking only the first one.\")\n",
    "        if len(entities) == 0:\n",
    "            json_mention[\"entity_id\"] = None\n",
    "            continue\n",
    "        json_mention[\"entity_id\"] = entities[0]\n",
    "    json_mentions = [json_mention for json_mention in json_mentions if json_mention[\"entity_id\"] is not None]\n",
    "    samples = convert_lorelei_json_mentions_to_samples(json_mentions)\n",
    "    output_path = lorelei_dir / f\"{path.stem}_bela_format.jsonl\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for sample in samples:\n",
    "            f.write(json.dumps(sample_to_json(sample)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#OromoRevolution \" __Godina Arsii Lixaa__ Aanaa shaallaatti waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU\n",
      "#OromoRevolution \"Godina Arsii Lixaa __Aanaa shaallaatti__ waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU\n",
      "['#OromoRevolution \"Godina Arsii Lixaa', 'Aanaa shaallaatti', 'waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU']\n",
      "#OromoRevolution \"Godina Arsii Lixaa Aanaa shaallaatti waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU\n",
      "#OromoRevolution \"Godina Arsii Lixaa Aanaa shaallaatti waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern to match [START] and [END] tags with optional spaces\n",
    "pattern = re.compile(r'\\s*\\[START\\]\\s*(.*?)\\s*\\[END\\]\\s*')\n",
    "\n",
    "# Define the annotated texts\n",
    "annotated_texts = [\n",
    "    '#OromoRevolution \" [START] Godina Arsii Lixaa [END] Aanaa shaallaatti waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU',\n",
    "    '#OromoRevolution \"Godina Arsii Lixaa [START] Aanaa shaallaatti [END] waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU'\n",
    "]\n",
    "\n",
    "# Initialize a set to keep track of unique entities\n",
    "entities = set()\n",
    "\n",
    "# Replace the [START] and [END] tags with placeholders\n",
    "for annotated_text in annotated_texts:\n",
    "    text = pattern.sub(r' __\\1__ ', annotated_text)\n",
    "    entities.update(re.findall(pattern, annotated_text))\n",
    "    print(text)\n",
    "\n",
    "# Join the texts together and replace the placeholders with entities\n",
    "print([text.strip() for text in text.split('__') if text.strip()])\n",
    "original_text = ' '.join(text.strip() for text in text.split('__') if text.strip())\n",
    "print(original_text)\n",
    "for entity in entities:\n",
    "    original_text = original_text.replace('__{}__'.format(entity), entity)\n",
    "\n",
    "print(original_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of files in BELA/Matcha format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aida_format_path = \"/fsx/kassner/data_BELA/wikipedia/aida_pretrain.jsonl\"\n",
    "# gt_entities: list of entities in the format [offset (words), length (words), wiki_data_id, kb_type?]\n",
    "# entities_raw: list of entities in the format [offset (chars), length (chars), wiki_data_id, kb_type?]\n",
    "# Seems that kb_type is always \"wiki\": set([entity[-1] for sample in df_aida[\"entities_raw\"].tolist() for entity in sample])\n",
    "df_aida = pd.DataFrame(read_jsonl_file(aida_format_path, n_lines=100000))\n",
    "df_aida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d76b99c7f04805835343263f93a0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_example_id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>gt_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14932393_1</td>\n",
       "      <td>Adobe Creek rises on the west flank of Sonoma...</td>\n",
       "      <td>[[0, 0, Q7562183, wiki, 40, 15], [0, 0, Q71716...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1010039_0</td>\n",
       "      <td>Esther Gorostiza Garai (Atxondo, Bizkaia, 19...</td>\n",
       "      <td>[[0, 0, Q1242393, wiki, 26, 7], [0, 0, Q93366,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>458956_3</td>\n",
       "      <td>&lt;section  Articles connexes &gt; industrie automo...</td>\n",
       "      <td>[[0, 0, Q65445, wiki, 52, 14], [0, 0, Q184937,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10739205_2</td>\n",
       "      <td>From 1956 to 1963, Habib worked as an assista...</td>\n",
       "      <td>[[0, 0, Q1044, wiki, 237, 12], [0, 0, Q502276,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116287_1</td>\n",
       "      <td>South Sanford is located at lat:43.4019444444...</td>\n",
       "      <td>[[0, 0, Q637413, wiki, 119, 27]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>11782414_1</td>\n",
       "      <td>Bei den Olympischen Jugendspielen 2010 in Sin...</td>\n",
       "      <td>[[0, 0, Q613716, wiki, 9, 30], [0, 0, Q815514,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>16744_0</td>\n",
       "      <td>minidesno200pxKraljevska palata Aranjuez Conc...</td>\n",
       "      <td>[[0, 0, Q29, wiki, 96, 8], [0, 0, Q151084, wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>7646568_0</td>\n",
       "      <td>Werner Protzel (* 5. Oktober 1973 in Rosenhei...</td>\n",
       "      <td>[[0, 0, Q2930, wiki, 19, 10], [0, 0, Q2477, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>38438_0</td>\n",
       "      <td>Марек Михал Грехута ( 10 Арванхоёрдугаар сар...</td>\n",
       "      <td>[[0, 0, Q36, wiki, 78, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>667438_1</td>\n",
       "      <td>فهرست آثار ملی ایران, سازمان میراث فرهنگی، صن...</td>\n",
       "      <td>[[0, 0, Q5958900, wiki, 1, 20], [0, 0, Q335599...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      data_example_id                                      original_text  \\\n",
       "0          14932393_1   Adobe Creek rises on the west flank of Sonoma...   \n",
       "1           1010039_0    Esther Gorostiza Garai (Atxondo, Bizkaia, 19...   \n",
       "2            458956_3  <section  Articles connexes > industrie automo...   \n",
       "3          10739205_2   From 1956 to 1963, Habib worked as an assista...   \n",
       "4            116287_1   South Sanford is located at lat:43.4019444444...   \n",
       "...               ...                                                ...   \n",
       "99995      11782414_1   Bei den Olympischen Jugendspielen 2010 in Sin...   \n",
       "99996         16744_0   minidesno200pxKraljevska palata Aranjuez Conc...   \n",
       "99997       7646568_0   Werner Protzel (* 5. Oktober 1973 in Rosenhei...   \n",
       "99998         38438_0    Марек Михал Грехута ( 10 Арванхоёрдугаар сар...   \n",
       "99999        667438_1   فهرست آثار ملی ایران, سازمان میراث فرهنگی، صن...   \n",
       "\n",
       "                                             gt_entities  \n",
       "0      [[0, 0, Q7562183, wiki, 40, 15], [0, 0, Q71716...  \n",
       "1      [[0, 0, Q1242393, wiki, 26, 7], [0, 0, Q93366,...  \n",
       "2      [[0, 0, Q65445, wiki, 52, 14], [0, 0, Q184937,...  \n",
       "3      [[0, 0, Q1044, wiki, 237, 12], [0, 0, Q502276,...  \n",
       "4                       [[0, 0, Q637413, wiki, 119, 27]]  \n",
       "...                                                  ...  \n",
       "99995  [[0, 0, Q613716, wiki, 9, 30], [0, 0, Q815514,...  \n",
       "99996  [[0, 0, Q29, wiki, 96, 8], [0, 0, Q151084, wik...  \n",
       "99997  [[0, 0, Q2930, wiki, 19, 10], [0, 0, Q2477, wi...  \n",
       "99998                         [[0, 0, Q36, wiki, 78, 5]]  \n",
       "99999  [[0, 0, Q5958900, wiki, 1, 20], [0, 0, Q335599...  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only care about raw text and chars offsets\n",
    "path = \"/fsx/movb/data/matcha/mel/test.txt\"\n",
    "df = pd.DataFrame(read_jsonl_file(path))\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old method\n",
    "\n",
    "The old method used dataframes and was broken / not robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnest_dict_column(df, column_name):\n",
    "    unnested_df = df[column_name].apply(pd.Series)\n",
    "    # Add prefix to avoid name collisions\n",
    "    unnested_df = unnested_df.add_prefix(f\"{column_name}.\")\n",
    "    return pd.concat([df.drop([column_name], axis=1), unnested_df], axis=1)\n",
    "\n",
    "\n",
    "def extract_entity_in_aida_format(row):\n",
    "    # Format: [offset (chars), length (chars), wiki_data_id, kb_type?]\n",
    "    return [len(row[\"meta.left_context_original\"]), len(row[\"meta.mention_original\"]), row[\"output.answer\"], \"wiki\"]  # TODO: Infered that the kb type is wiki but not sure\n",
    "\n",
    "\n",
    "def concatenate_contexts_and_mention(left_context, mention, right_context):\n",
    "    left_separator = \" \"\n",
    "    right_separator = \" \"\n",
    "    # If the right/left context starts/ends with a punctuation, then an extra space would be added, so we have to remove it   \n",
    "    left_punctuation_chars = tuple(\" ,!?;:()[]{}'’‘“\\\"\")\n",
    "    right_punctuation_chars = tuple(\" ,.!?;:()[]{}'’‘“\\\"\")\n",
    "    if right_context.startswith(right_punctuation_chars):\n",
    "        right_separator = \"\"\n",
    "    if left_context.endswith(left_punctuation_chars):\n",
    "        left_separator = \"\"\n",
    "    return \"\".join([left_context, left_separator, mention, right_separator, right_context])\n",
    "\n",
    "\n",
    "def fix_offsets(text, entity, mention):\n",
    "    \"\"\"Fix offsets in the entity list that might have been corrupted by the concatenation\n",
    "    - entity format: [offset (chars), length (chars), wiki_data_id, kb_type]\n",
    "    \"\"\"\n",
    "    offset, length, _, _ = entity\n",
    "    recovered_mention = text[offset:offset + length]\n",
    "    if mention != recovered_mention:\n",
    "        # Find the mention in the text\n",
    "        lookup_offset = offset - (len(mention) - 1)  # So that we can't match another mention that would be right before the mention\n",
    "        infered_offset = text[lookup_offset:].find(mention) + (lookup_offset)\n",
    "        if infered_offset == -1:\n",
    "            raise ValueError(f\"Could not find mention {mention} in text {text}\")\n",
    "        # Fix the offset\n",
    "        assert abs(infered_offset - offset) < 10, f\"Offset is too far from the infered offset: {offset=}, {infered_offset=}, {mention=}, {text[offset:offset + 50]=}\"\n",
    "        entity[0] = infered_offset\n",
    "        entity[1] = len(mention)\n",
    "    return entity\n",
    "\n",
    "\n",
    "# Quick unit tests\n",
    "assert fix_offsets(\"hello world\", [7, 5, \"Q123\", \"wiki\"], \"world\") == [6, 5, \"Q123\", \"wiki\"]\n",
    "assert fix_offsets(\"hello world world\", [11, 5, \"Q123\", \"wiki\"], \"world\") == [12, 5, \"Q123\", \"wiki\"]\n",
    "\n",
    "\n",
    "def is_correct_entity_offset(text, entity, mention):\n",
    "    offset, length, _, _ = entity\n",
    "    return text[offset:offset + length] == mention\n",
    "\n",
    "\n",
    "def infer_original_text(texts_with_annotated_mentions):\n",
    "    # NOT USED YET\n",
    "    \"\"\"Infer the original text from the texts with annotated mentions.\n",
    "    Each annotation might have introduced additional spaces but we don't know when, so we combine the longest left and right context to infer the original text.\n",
    "    Use with: `df_kilt.groupby(\"meta.document_id\").agg({\"original_text\": infer_original_text}).reset_index()`\n",
    "    \"\"\"\n",
    "    if len(texts_with_annotated_mentions) == 1:\n",
    "        [text] = texts_with_annotated_mentions\n",
    "        left_context = text.split(\" [START]\")[0]\n",
    "        right_context = text.split(\"[END] \")[1]\n",
    "        mention = text.split(\"[START] \")[1].split(\" [END]\")[0]\n",
    "        return concatenate_contexts_and_mention(left_context, mention, right_context)\n",
    "    left_contexts = [text.split(\" [START]\")[0] for text in texts_with_annotated_mentions]\n",
    "    right_contexts = [text.split(\"[END] \")[1] for text in texts_with_annotated_mentions]\n",
    "    longest_left_context = max(left_contexts, key=len)\n",
    "    longest_right_context = max(right_contexts, key=len)\n",
    "    # Some of the longest left context's end is often some of the start of the right context\n",
    "    # Find the substring in common\n",
    "    intersection = \"\"\n",
    "    for i in range(len(longest_right_context)):\n",
    "        if longest_left_context.endswith(longest_right_context[:i]):\n",
    "            intersection = longest_right_context[:i]\n",
    "    assert intersection in longest_right_context\n",
    "    # Merge the two contexts, removing the intersection\n",
    "    merged = \"\".join([longest_left_context, longest_right_context.replace(intersection, \"\")])\n",
    "    return merged\n",
    "\n",
    "\n",
    "\n",
    "def convert_kilt_to_bela_format(kilt_format_path):\n",
    "    \"\"\"Example usage: `convert_kilt_to_bela_format(Path(\"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train.jsonl\"))`\n",
    "    \"\"\"\n",
    "    df_kilt = pd.DataFrame(read_jsonl_file(kilt_format_path, n_lines=10000))\n",
    "    df_kilt = unnest_dict_column(df_kilt, \"meta\")\n",
    "    assert df_kilt[\"output\"].apply(len).unique().tolist() == [1]\n",
    "    df_kilt[\"output\"] = df_kilt[\"output\"].apply(lambda x: x[0])  # Only one output per sample\n",
    "    df_kilt = unnest_dict_column(df_kilt, \"output\")\n",
    "    # assert df_kilt[\"output.answer\"].apply(len).unique().tolist() == [1]\n",
    "    df_kilt[\"output.answer\"] = df_kilt[\"output.answer\"].apply(lambda x: x[0])  # Take only the first option, e.g. sample TEDL15_TRAINING_06967 has [Q612, Q613] for mention Dubai\n",
    "\n",
    "    df_kilt[\"entities_raw\"] = df_kilt.apply(extract_entity_in_aida_format, axis=1).tolist()\n",
    "    # Aggregate \"entities_raw\" as list and \"meta.input_original\" as a single string (should be unique)\n",
    "    df_kilt[\"text_raw\"] = df_kilt.apply(lambda row: concatenate_contexts_and_mention(row[\"meta.left_context_original\"], row[\"meta.mention_original\"], row[\"meta.right_context_original\"]), axis=1)\n",
    "    df_bela_format = df_kilt.groupby(\"meta.document_id\").agg({\n",
    "        \"text_raw\": \"first\",  # TODO: The concatenation sometimes produces some different texts (adding extra spaces between mention and context or removing them).\n",
    "        \"entities_raw\": list,\n",
    "        \"meta.mention_original\": list,\n",
    "    }).reset_index()\n",
    "    # Fix offsets\n",
    "    for _, row in df_bela_format.iterrows():\n",
    "        fixed_entities = []\n",
    "        for i, (entity, mention) in enumerate(zip(row[\"entities_raw\"], row[\"meta.mention_original\"])):\n",
    "            if not is_correct_entity_offset(row[\"text_raw\"], entity, mention):\n",
    "                try:\n",
    "                    entity = fix_offsets(row[\"text_raw\"], entity, mention)\n",
    "                except AssertionError as e:\n",
    "                    print(f\"Skipping mention in {row['meta.document_id']=}: {e}\")\n",
    "                    continue\n",
    "                assert is_correct_entity_offset(row[\"text_raw\"], entity, mention), f\"{mention=}, {entity=}\"\n",
    "            fixed_entities.append(entity)\n",
    "        row[\"entities_raw\"] = fixed_entities\n",
    "    df_bela_format[\"entities_raw\"] = df_bela_format[\"entities_raw\"].apply(lambda entities: [[0, 0] + entity for entity in entities])  # Add dummy 0, 0 for backward compatibility\n",
    "    # HACK: reorder entities in the format that is used in EvaluateMEL.ipynb notebook. Format in EvaluateMEL.ipynb: _,_,ent_id,_,offset,length.\n",
    "    # TODO: We should uniformize. \n",
    "    df_bela_format[\"entities_raw\"] = df_bela_format[\"entities_raw\"].apply(lambda entities: [(dummy_1, dummy_2, entity, source, offset, length) for dummy_1, dummy_2, offset, length, entity, source in entities])\n",
    "    # document_id\toriginal_text\tgt_entities\n",
    "    df_bela_format = df_bela_format.rename(columns={\"meta.document_id\": \"document_id\", \"text_raw\": \"original_text\", \"entities_raw\": \"gt_entities\"})\n",
    "    df_bela_format = df_bela_format[[\"document_id\", \"original_text\", \"gt_entities\"]]\n",
    "    new_path = kilt_format_path.parent / f\"{kilt_format_path.stem}_bela_format.jsonl\"\n",
    "    # Write to jsonl \n",
    "    with open(new_path, \"w\", encoding=\"utf8\") as f:\n",
    "        for _, row in df_bela_format.iterrows():\n",
    "            f.write(row.to_json() + \"\\n\")\n",
    "    print(f\"Saved {new_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieved_data_folder = Path(\"/fsx/louismartin/bela/retrieved_from_aws_backup/\")\n",
    "kilt_format_path = retrieved_data_folder / \"ndecao/TACKBP2015/train.jsonl\"\n",
    "#convert_kilt_to_bela_format(kilt_format_path)\n",
    "df_kilt = pd.DataFrame(read_jsonl_file(kilt_format_path, n_lines=10000))\n",
    "df_kilt = unnest_dict_column(df_kilt, \"meta\")\n",
    "assert df_kilt[\"output\"].apply(len).unique().tolist() == [1]\n",
    "df_kilt[\"output\"] = df_kilt[\"output\"].apply(lambda x: x[0])  # Only one output per sample\n",
    "df_kilt = unnest_dict_column(df_kilt, \"output\")\n",
    "# assert df_kilt[\"output.answer\"].apply(len).unique().tolist() == [1]\n",
    "df_kilt[\"output.answer\"] = df_kilt[\"output.answer\"].apply(lambda x: x[0])  # Take only the first option, e.g. sample TEDL15_TRAINING_06967 has [Q612, Q613] for mention Dubai\n",
    "\n",
    "df_kilt[\"entities_raw\"] = df_kilt.apply(extract_entity_in_aida_format, axis=1).tolist()\n",
    "# Aggregate \"entities_raw\" as list and \"meta.input_original\" as a single string (should be unique)\n",
    "df_kilt[\"text_raw\"] = df_kilt.apply(lambda row: concatenate_contexts_and_mention(row[\"meta.left_context_original\"], row[\"meta.mention_original\"], row[\"meta.right_context_original\"]), axis=1)\n",
    "df_bela_format = df_kilt.groupby(\"meta.document_id\").agg({\n",
    "    \"text_raw\": \"first\",  # TODO: The concatenation sometimes produces some different texts (adding extra spaces between mention and context or removing them).\n",
    "    \"entities_raw\": list,\n",
    "    \"meta.mention_original\": list,\n",
    "}).reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bela.utils.analysis_utils import Entity, Sample\n",
    "\n",
    "\n",
    "def coerce_to_sample(jsonl_sample):\n",
    "    \"\"\"Coerce a jsonl sample to a Sample object.\n",
    "    \"\"\"\n",
    "    if isinstance(jsonl_sample, Sample):\n",
    "        return jsonl_sample\n",
    "    # BELA format\n",
    "    if all(key in jsonl_sample for key in [\"document_id\", \"original_text\", \"gt_entities\"]):\n",
    "        ground_truth_entities = [\n",
    "            Entity(entity_id=entity_id, offset=offset, length=length, text=jsonl_sample['original_text'])\n",
    "            for _, _, entity_id, _, offset, length in jsonl_sample['gt_entities']\n",
    "        ]\n",
    "        return Sample(text=jsonl_sample['original_text'], ground_truth_entities=ground_truth_entities)\n",
    "    raise ValueError(f\"Unknown format for {jsonl_sample=}\")\n",
    "\n",
    "\n",
    "def coerce_to_entity(jsonl_entity):\n",
    "    \"\"\"Coerce a jsonl entity to an Entity object.\n",
    "    \"\"\"\n",
    "    if isinstance(jsonl_entity, Entity):\n",
    "        return jsonl_entity\n",
    "    # KILT format\n",
    "    if all(key in jsonl_entity for key in [\"id\", \"input\", \"output\", \"meta\"]):\n",
    "        return Entity(entity_id=jsonl_entity[\"output\"][\"answer\"], offset=jsonl_entity[\"meta\"][\"offset\"], length=jsonl_entity[\"meta\"][\"length\"], text=jsonl_entity[\"input\"])\n",
    "    raise ValueError(f\"Unknown format for {jsonl_entity=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "#kilt_format_path = Path.home() / \"dev/BELA/data/KILT_format/TACKBP2015_training.jsonl\"\n",
    "retrieved_data_folder = Path(\"/fsx/louismartin/bela/retrieved_from_aws_backup/\")\n",
    "kilt_format_path = retrieved_data_folder / \"ndecao/TACKBP2015/train.jsonl\"\n",
    "for line in islice(yield_jsonl_lines(kilt_format_path), 10, 100):\n",
    "    if not line[\"meta\"][\"document_id\"].startswith(\"EN\"):\n",
    "        continue\n",
    "    print(\"*\" * 100)\n",
    "    pprint(line, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df[\"meta.document_id\"].str.startswith(\"ENG\")]  # Take only english\n",
    "#df = df[~df[\"meta.document_id\"].str.startswith(\"CMN\")]  # Remove Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"hello world \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_kilt[\"meta.document_id\"] == \"CMN_NW_001145_20150413_F0000005B\"\n",
    "df_kilt[mask].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Documents with texts not joined correctly\")\n",
    "mask = (df[\"n_text_raw\"] > 1)\n",
    "print(df[mask][\"meta.document_id\"].tolist()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id = \"ENG_NW_001006_20150301_F00000005\"  # Bosco café\n",
    "document_id = \"CMN_DF_000178_20150318_F000000CO\"\n",
    "document_id = \"CMN_DF_000178_20150318_F000000CO\"\n",
    "mask = df[\"meta.document_id\"] == document_id\n",
    "print(list(df[mask].head(1).to_dict(orient=\"records\")[0][\"text_raw\"])[0])\n",
    "print(\"------------\")\n",
    "print(list(df[mask].head(1).to_dict(orient=\"records\")[0][\"text_raw\"])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_kilt[\"output\"].apply(lambda x: x[0][\"answer\"] != [None])\n",
    "df_kilt[mask]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "235bcda9aabf5f8363267684c8e0d7a57e9fb12569fe58ae215aef35e2f0a58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
