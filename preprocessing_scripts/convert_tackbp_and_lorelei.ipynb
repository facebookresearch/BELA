{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "from itertools import islice, product\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "#import itables.interactive\n",
    "import pandas as pd\n",
    "# Set interactive pandas\n",
    "#itables.options.mode = \"notebook\"\n",
    "\n",
    "from bela.utils.analysis_utils import Sample, Entity\n",
    "\n",
    "def yield_jsonl_lines(path):\n",
    "    with open(path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "def inspect_json(json_dict, indent=0, max_elements=3):\n",
    "    \"\"\"Only print the first 3 keys of a dict, recursively\"\"\"\n",
    "    for i, (key, value) in enumerate(json_dict.items()):\n",
    "        if isinstance(value, dict):\n",
    "            print(\"  \" * indent + f\"{key}:\")\n",
    "            inspect_json(value, indent=indent + 1)\n",
    "        else:\n",
    "            print(\"  \" * indent + f\"{key}: {value}\")\n",
    "        if i + 1 == max_elements:\n",
    "            print(\"  \" * indent + \"...\")\n",
    "            break\n",
    "\n",
    "\n",
    "def read_jsonl_file(path, n_lines=None):\n",
    "    return list(tqdm(islice(yield_jsonl_lines(path), n_lines), total=n_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DEPRECATED_infer_original_text(texts):\n",
    "    \"\"\"Given a list of texts, corresponding to the same document but with different [START] mention [END] tokens, try to infer the original text by removing the [START] and [END] tokens and the extra spaces that were added.\"\"\"\n",
    "    text_counter = Counter()\n",
    "    for text in texts:\n",
    "        left_context, mention, right_context = re.match(r\"(.*) \\[START\\] (.*) \\[END\\] (.*)\", text, re.DOTALL).groups()  # re.DOTALL to match newlines with .\n",
    "        # We don't know if there is a space between the context and the mention. Sometimes there was even 2 in the original. Try all combinations.\n",
    "        # There are multiple possible characters that were stripped\n",
    "        #possible_characters = [\"\", \" \", \"\\n\", \"\\u3000\", \"\\u3000\\u3000\"]  # \\u3000 happens a lot in chinese texts\n",
    "        # Sometimes there can be a sequence of 2 of them\n",
    "        #possible_separators = set([\"\".join(sep) for sep in product(possible_characters, repeat=2)])\n",
    "        possible_separators = [\"\", \" \", \"\\n\", \"\\u3000\", \" \\n\", \"\\n\\n\", \"\\u3000\\u3000\", \" \\u3000\"]\n",
    "        # Then we can have either of those before and after the mention\n",
    "        for sep_1, sep_2 in product(possible_separators, repeat=2):\n",
    "            text = f\"{left_context}{sep_1}{mention}{sep_2}{right_context}\"\n",
    "            text_counter[text] += 1\n",
    "    #assert text_counter.most_common()[0][1] != text_counter.most_common()[1][1], f\"Could not infer original text, the most common text is not unique: {text_counter.most_common()[:2]}\"\n",
    "    if len(texts) > 1 and text_counter.most_common()[0][1] == text_counter.most_common()[1][1]:\n",
    "        print(f\"Could not infer original text, the most common text is not unique: {text_counter.most_common()[:2]}\")\n",
    "    # Assume that the text with the more occurences is the correct one\n",
    "    #assert text_counter.most_common()[0][1] > 1, f\"Could not infer original text, the most common text occurs only once: {text_counter.most_common()[0][0]}\"\n",
    "    if len(texts) > 1 and text_counter.most_common()[0][1] <= 1:\n",
    "        print(f\"Could not infer original text, the most common text occurs only once: {text_counter.most_common()[0][0]}\")\n",
    "    return text_counter.most_common()[0][0]\n",
    "\n",
    "\n",
    "def merge_contexts(longuest_left_context, longuest_right_context):\n",
    "    # Concatenate the two contexts after removing the overlapping part\n",
    "    # First find the length of the overlapping part, e.g. if longuest_left_context = \"abcdef\" and longuest_right_context = \"cdefgh\", then the overlapping part is \"cdef\"\n",
    "    overlapping_length = 0\n",
    "    for i in range(1, len(longuest_right_context) + 1):\n",
    "        if longuest_left_context[-i:] == longuest_right_context[:i]:\n",
    "            overlapping_length = i\n",
    "    assert overlapping_length > 0\n",
    "    #if overlapping_length == 0:\n",
    "    #    print(f\"Could not merge contexts: {longuest_left_context} and {longuest_right_context}\")\n",
    "    #    # HACK: This fixes the following edge case\n",
    "    #    # texts = [\n",
    "    #    #     '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en [START] Charleston [END] SC </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>',\n",
    "    #    #     '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en Charleston [START] SC [END] </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>'\n",
    "    #    # ]\n",
    "    #    # original_text = '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en Charleston SC </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>',\n",
    "    #    # assert infer_original_text(texts) == original_text\n",
    "    #    longuest_left_context += \" \"\n",
    "    return longuest_left_context + longuest_right_context[overlapping_length:]\n",
    "\n",
    "\n",
    "def test_merge_contexts():\n",
    "    assert merge_contexts(\"abcdef\", \"cdefgh\") == \"abcdefgh\"\n",
    "    assert merge_contexts(\"abcdef\", \"cdefghijkl\") == \"abcdefghijkl\"\n",
    "    assert merge_contexts(\"abcdef\", \"cdef\") == \"abcdef\"\n",
    "    assert merge_contexts(\"zabcabc\", \"abcabcde\") == \"zabcabcde\"\n",
    "    assert merge_contexts(\"abcdef\", \"abcdef\") == \"abcdef\"\n",
    "test_merge_contexts()\n",
    "\n",
    "\n",
    "def infer_original_text(texts):\n",
    "    \"\"\"Given a list of texts, corresponding to the same document but with different [START] mention [END] tokens, try to infer the original text by removing the [START] and [END] tokens and the extra spaces that were added.\n",
    "    The big issue is that when adding [START] and [END] some characters were removed (whitespace, newlines, \\u3000 etc.). E.g. \"abc \\n mention    def\" -> \"abc [START] mention [END] def\", so we try to recover the original text\n",
    "    \"\"\"\n",
    "    longuest_left_context = max(texts, key=lambda text: len(text.split(\" [START] \")[0])).split(\" [START] \")[0]\n",
    "    longuest_right_context = max(texts, key=lambda text: len(text.split(\" [END] \")[-1])).split(\" [END] \")[-1]\n",
    "    try:\n",
    "        return merge_contexts(longuest_left_context, longuest_right_context)\n",
    "    except AssertionError:\n",
    "        # Fallback\n",
    "        text = texts[0]\n",
    "        left_context, mention, right_context = re.match(r\"(.*) \\[START\\] (.*) \\[END\\] (.*)\", text, re.DOTALL).groups()  # re.DOTALL to match newlines with .\n",
    "        return f\"{left_context} {mention} {right_context}\"\n",
    "\n",
    "\n",
    "def test_infer_original_text():\n",
    "    texts = [\n",
    "        '<DOC id=\"CMN_NW_001149_20150314_F0000005H\"> <SOURCE>http://sports.hangzhou.com.cn/zxbd/content/2015-03/14/content_5689329.htm</SOURCE> <DATE_TIME>2015-03-14T00:00:00</DATE_TIME> <HEADLINE> “ [START] 刀锋战士 [END] ”阻止枪杀案件重审 申诉被驳回 </HEADLINE> <TEXT> <P>    在13日的法院审理中，残奥会明星皮斯托瑞斯的法律团队企图阻止其枪杀女友案件重新在最高法院审理的申诉被驳回。 </P> <P> \\u3000\\u30002014年11月4日，政府检察官向法庭提交了上诉文件，文件对皮斯托瑞斯杀死女友案的定罪和判刑均提出了上诉。之后，法庭批准了此项上诉。 </P> <P> \\u3000\\u3000“刀锋战士”的辩护律师企图阻止法院重新开庭审理此案的申诉被驳回后，律师表示控辩双方都有权利陈述自己的观点。 </P> <P> \\u3000\\u3000现年27岁的皮斯托瑞斯在2013年2月14日“情人节”当天，透过浴室的门射杀了女友斯滕坎普。南非北豪登省最高法院2014年9月12日做出决定，判定皮斯托瑞斯谋杀罪名不成立，并裁定他犯有过失杀人罪。 </P> <P> \\u3000\\u3000北豪登省最高法院庭审法官玛希帕2014年10月21日宣布，皮斯托瑞斯因过失导致女友斯滕坎普死亡，将在监狱服刑5年。皮斯托瑞斯表示将不会对此判决提出上诉，他将有资格在服刑10个月后，获得在家禁闭服刑的权利。 </P> <P> \\u3000\\u3000现在法官玛希帕批准了检察官的上诉申请，而驳回了辩护律师的申诉，这意味着这场旷日持久的官司仍将继续。 </P> </TEXT> </DOC>',\n",
    "        '<DOC id=\"CMN_NW_001149_20150314_F0000005H\"> <SOURCE>http://sports.hangzhou.com.cn/zxbd/content/2015-03/14/content_5689329.htm</SOURCE> <DATE_TIME>2015-03-14T00:00:00</DATE_TIME> <HEADLINE> “刀锋战士”阻止枪杀案件重审 申诉被驳回 </HEADLINE> <TEXT> <P>    在13日的法院审理中，残奥会明星 [START] 皮斯托瑞斯 [END] 的法律团队企图阻止其枪杀女友案件重新在最高法院审理的申诉被驳回。 </P> <P> \\u3000\\u30002014年11月4日，政府检察官向法庭提交了上诉文件，文件对皮斯托瑞斯杀死女友案的定罪和判刑均提出了上诉。之后，法庭批准了此项上诉。 </P> <P> \\u3000\\u3000“刀锋战士”的辩护律师企图阻止法院重新开庭审理此案的申诉被驳回后，律师表示控辩双方都有权利陈述自己的观点。 </P> <P> \\u3000\\u3000现年27岁的皮斯托瑞斯在2013年2月14日“情人节”当天，透过浴室的门射杀了女友斯滕坎普。南非北豪登省最高法院2014年9月12日做出决定，判定皮斯托瑞斯谋杀罪名不成立，并裁定他犯有过失杀人罪。 </P> <P> \\u3000\\u3000北豪登省最高法院庭审法官玛希帕2014年10月21日宣布，皮斯托瑞斯因过失导致女友斯滕坎普死亡，将在监狱服刑5年。皮斯托瑞斯表示将不会对此判决提出上诉，他将有资格在服刑10个月后，获得在家禁闭服刑的权利。 </P> <P> \\u3000\\u3000现在法官玛希帕批准了检察官的上诉申请，而驳回了辩护律师的申诉，这意味着这场旷日持久的官司仍将继续。 </P> </TEXT> </DOC>',\n",
    "        '<DOC id=\"CMN_NW_001149_20150314_F0000005H\"> <SOURCE>http://sports.hangzhou.com.cn/zxbd/content/2015-03/14/content_5689329.htm</SOURCE> <DATE_TIME>2015-03-14T00:00:00</DATE_TIME> <HEADLINE> “刀锋战士”阻止枪杀案件重审 申诉被驳回 </HEADLINE> <TEXT> <P>    在13日的法院审理中，残奥会明星皮斯托瑞斯的法律团队企图阻止其枪杀女友案件重新在最高法院审理的申诉被驳回。 </P> <P> \\u3000\\u30002014年11月4日，政府检察官向法庭提交了上诉文件，文件对 [START] 皮斯托瑞斯 [END] 杀死女友案的定罪和判刑均提出了上诉。之后，法庭批准了此项上诉。 </P> <P> \\u3000\\u3000“刀锋战士”的辩护律师企图阻止法院重新开庭审理此案的申诉被驳回后，律师表示控辩双方都有权利陈述自己的观点。 </P> <P> \\u3000\\u3000现年27岁的皮斯托瑞斯在2013年2月14日“情人节”当天，透过浴室的门射杀了女友斯滕坎普。南非北豪登省最高法院2014年9月12日做出决定，判定皮斯托瑞斯谋杀罪名不成立，并裁定他犯有过失杀人罪。 </P> <P> \\u3000\\u3000北豪登省最高法院庭审法官玛希帕2014年10月21日宣布，皮斯托瑞斯因过失导致女友斯滕坎普死亡，将在监狱服刑5年。皮斯托瑞斯表示将不会对此判决提出上诉，他将有资格在服刑10个月后，获得在家禁闭服刑的权利。 </P> <P> \\u3000\\u3000现在法官玛希帕批准了检察官的上诉申请，而驳回了辩护律师的申诉，这意味着这场旷日持久的官司仍将继续。 </P> </TEXT> </DOC>',\n",
    "    ]\n",
    "    original_text = '<DOC id=\"CMN_NW_001149_20150314_F0000005H\"> <SOURCE>http://sports.hangzhou.com.cn/zxbd/content/2015-03/14/content_5689329.htm</SOURCE> <DATE_TIME>2015-03-14T00:00:00</DATE_TIME> <HEADLINE> “刀锋战士”阻止枪杀案件重审 申诉被驳回 </HEADLINE> <TEXT> <P>    在13日的法院审理中，残奥会明星皮斯托瑞斯的法律团队企图阻止其枪杀女友案件重新在最高法院审理的申诉被驳回。 </P> <P> \\u3000\\u30002014年11月4日，政府检察官向法庭提交了上诉文件，文件对皮斯托瑞斯杀死女友案的定罪和判刑均提出了上诉。之后，法庭批准了此项上诉。 </P> <P> \\u3000\\u3000“刀锋战士”的辩护律师企图阻止法院重新开庭审理此案的申诉被驳回后，律师表示控辩双方都有权利陈述自己的观点。 </P> <P> \\u3000\\u3000现年27岁的皮斯托瑞斯在2013年2月14日“情人节”当天，透过浴室的门射杀了女友斯滕坎普。南非北豪登省最高法院2014年9月12日做出决定，判定皮斯托瑞斯谋杀罪名不成立，并裁定他犯有过失杀人罪。 </P> <P> \\u3000\\u3000北豪登省最高法院庭审法官玛希帕2014年10月21日宣布，皮斯托瑞斯因过失导致女友斯滕坎普死亡，将在监狱服刑5年。皮斯托瑞斯表示将不会对此判决提出上诉，他将有资格在服刑10个月后，获得在家禁闭服刑的权利。 </P> <P> \\u3000\\u3000现在法官玛希帕批准了检察官的上诉申请，而驳回了辩护律师的申诉，这意味着这场旷日持久的官司仍将继续。 </P> </TEXT> </DOC>'\n",
    "    infered_text = infer_original_text(texts)\n",
    "    assert original_text == infered_text, f\"original_text != infered_text:\\n{original_text}\\n{infered_text}\"\n",
    "    texts = [\n",
    "        'Công an Hà Nội khủng bố tinh thần anh Trịnh Bá Phương: [START] CÔNG [END] AN HÀ NỘI KHỦNG BỐ TINH THẦN ANH TRỊNH B... https://t.co/bEocspCx5k',\n",
    "        'Công an [START] Hà Nội [END] khủng bố tinh thần anh Trịnh Bá Phương: \\n\\n        CÔNG AN HÀ NỘI KHỦNG BỐ TINH THẦN ANH TRỊNH B... https://t.co/bEocspCx5k',\n",
    "    ]\n",
    "    original_text = \"Công an Hà Nội khủng bố tinh thần anh Trịnh Bá Phương: \\n\\n        CÔNG AN HÀ NỘI KHỦNG BỐ TINH THẦN ANH TRỊNH B... https://t.co/bEocspCx5k\"\n",
    "    infered_text = infer_original_text(texts)\n",
    "    assert original_text == infered_text, f\"original_text != infered_text:\\n{original_text}\\n{infered_text}\"\n",
    "    texts = [\n",
    "        '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en [START] Charleston [END] SC </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>',\n",
    "        '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en Charleston [START] SC [END] </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>',\n",
    "    ]\n",
    "    original_text = '<doc id=\"SPA_DF_000404_20150618_F001000AU\"> <headline> Un loco blanquito tirotea una iglesia en Charleston SC </headline> <post id=\"p1\" author=\"chikilinda52\" datetime=\"2015-06-18T21:25:00\"> http://noticias.univision.com/article/2372330/2015-06-17/estados-unidos/noticias/tiroteo-multiple-en... </post> <post id=\"p2\" author=\"carmensantos\" datetime=\"2015-06-28T00:18:00\"> la pena de muerte para este desgraci*ado </post> <post id=\"p3\" author=\"arcoiris02\" datetime=\"2015-06-29T01:09:00\"> horrible </post> <post id=\"p4\" author=\"andreitu\" datetime=\"2015-07-07T23:39:00\"> pobre gentes </post> <post id=\"p5\" author=\"amazona71\" datetime=\"2015-07-11T01:28:00\"> debio morir antes </post> </doc>'\n",
    "    infered_text = infer_original_text(texts)\n",
    "    assert original_text == infered_text, f\"original_text != infered_text:\\n{original_text}\\n{infered_text}\"\n",
    "    # TODO: This doesn't pass yet\n",
    "    # texts = [\n",
    "    #     '#OromoRevolution \" [START] Godina Arsii Lixaa [END] Aanaa shaallaatti waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU',\n",
    "    #     '#OromoRevolution \"Godina Arsii Lixaa [START] Aanaa shaallaatti [END] waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU',\n",
    "    # ]\n",
    "    # original_text = '#OromoRevolution \"Godina Arsii Lixaa Aanaa shaallaatti waraana ummata Oromoofi waraana Agaazii gidduutti... https://t.co/LAxEOiaiEU'\n",
    "    # infered_text = infer_original_text(texts)\n",
    "    # assert original_text == infered_text, f\"original_text != infered_text:\\n{original_text}\\n{infered_text}\"\n",
    "test_infer_original_text()\n",
    "\n",
    "\n",
    "def convert_tackbp_json_mentions_to_sample(json_mentions: List[Dict]) -> Sample:\n",
    "    \"\"\"Given mentions from a single document, convert them to a Sample object.\"\"\"\n",
    "    # Check they all come from the same document\n",
    "    assert len(set(json_mention[\"meta\"][\"document_id\"] for json_mention in json_mentions)) == 1\n",
    "    # Convert json_mentions to sample\n",
    "    json_mentions.sort(key=lambda json_mention: json_mention[\"meta\"][\"start_offset\"])\n",
    "    ground_truth_entities = []\n",
    "    text = infer_original_text([json_mention[\"input\"] for json_mention in json_mentions])\n",
    "    for json_mention in json_mentions:\n",
    "        offset = len(json_mention[\"meta\"][\"left_context\"])\n",
    "        length = len(json_mention[\"meta\"][\"mention\"])\n",
    "        # Leading whitespaces\n",
    "        additional_spaces = len(text[offset:]) - len(text[offset:].lstrip(\" \\n\\u3000\"))\n",
    "        offset += additional_spaces\n",
    "        entity = Entity(text=text, offset=offset, length=length, entity_id=json_mention[\"entity_id\"])\n",
    "        # Sometimes there are extra spaces / newlines in the original text mention that were removed in the metadata\n",
    "        # json_mention[\"meta\"][\"mention\"] = 'Must u h a m m a d u Bow u h a r i'\n",
    "        # entity.mention = 'Must u h a m m a d u \\nBow u h a r '\n",
    "        assert entity.entity_id is not None\n",
    "        #assert entity.mention == json_mention[\"meta\"][\"mention\"], f\"{entity.mention=} != {json_mention['meta']['mention']=}\"\n",
    "        if entity.mention != json_mention[\"meta\"][\"mention\"]:\n",
    "            print(f\"{entity.mention=} != {json_mention['meta']['mention']=}\")\n",
    "        ground_truth_entities.append(entity)\n",
    "    return Sample(text=text, sample_id=json_mentions[0][\"meta\"][\"document_id\"], ground_truth_entities=ground_truth_entities)\n",
    "\n",
    "\n",
    "def convert_tackbp_json_mentions_to_samples(json_mentions: List[Dict]) -> List[Sample]:\n",
    "    \"\"\"Given a list of json mentions, group them by document and convert them to Samples objects.\"\"\"\n",
    "    # Group mentions together\n",
    "    aggregated_mentions = defaultdict(list)\n",
    "    for json_mention in json_mentions:\n",
    "        aggregated_mentions[json_mention[\"meta\"][\"document_id\"]].append(json_mention)\n",
    "    samples = [convert_tackbp_json_mentions_to_sample(json_mentions) for json_mentions in tqdm(aggregated_mentions.values())]\n",
    "    return samples\n",
    "\n",
    "\n",
    "def sample_to_json(sample: Sample) -> str:\n",
    "    \"\"\"Convert a sample to a jsonl string.\"\"\"\n",
    "    return {\"data_example_id\": sample.sample_id, \"original_text\": sample.text, \"gt_entities\": [[0, 0, entity.entity_id, \"wiki\", entity.offset, entity.length] for entity in sample.ground_truth_entities]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the TACKBP jsonl from Nicola De Cao's backup to bela format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9334def0239498183b51e2e02b670a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one entity for ENG_NW_001004_20141029_F00000003: ['Q612', 'Q613']. Taking only the first one.\n",
      "More than one entity for SPA_NW_001078_20150105_F0000002W: ['Q693039', 'Q813']. Taking only the first one.\n",
      "More than one entity for SPA_NW_001078_20150105_F0000002W: ['Q693039', 'Q813']. Taking only the first one.\n",
      "More than one entity for SPA_NW_001078_20150105_F0000002W: ['Q693039', 'Q813']. Taking only the first one.\n",
      "More than one entity for SPA_NW_001066_20150105_F0000002H: ['Q693039', 'Q813']. Taking only the first one.\n",
      "More than one entity for ENG_DF_001220_20150404_F0000007K: ['Q612', 'Q613']. Taking only the first one.\n",
      "More than one entity for CMN_DF_000181_20140830_F000000DG: ['Q693039', 'Q813']. Taking only the first one.\n",
      "More than one entity for CMN_DF_000191_20150401_F000000DX: ['Q693039', 'Q813']. Taking only the first one.\n",
      "ENG: 9027 mentions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cef498954fe46f08d86a0ff400cc634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity.mention='Must u h a m m a d u \\nBow u h a r ' != json_mention['meta']['mention']='Must u h a m m a d u Bow u h a r i'\n",
      "ENG: 168 samples\n",
      "SPA: 3151 mentions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9738fdf4708d4723bbe03995dfdba690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity.mention='BORIS  Nemtso' != json_mention['meta']['mention']='BORIS Nemtsov'\n",
      "entity.mention='Vladimir  Puti' != json_mention['meta']['mention']='Vladimir Putin'\n",
      "entity.mention='AMERICA  LATIN' != json_mention['meta']['mention']='AMERICA LATINA'\n",
      "entity.mention='UNION  SOVIETIC' != json_mention['meta']['mention']='UNION SOVIETICA'\n",
      "SPA: 128 samples\n",
      "CMN: 10520 mentions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dde4ba4c06487184a63e179f647b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMN: 147 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d704693e6fcf4590ae30b4a2763f5e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one entity for CMN_DF_000020_20150627_F00100079: ['Q313327', 'Q51358']. Taking only the first one.\n",
      "More than one entity for ENG_DF_001206_20150624_F001000CN: ['Q10846439', 'Q3884']. Taking only the first one.\n",
      "More than one entity for ENG_DF_001471_20150528_F001000D2: ['Q7341', 'Q663764']. Taking only the first one.\n",
      "More than one entity for ENG_NW_001436_20140731_F0010006P: ['Q4951156', 'Q1460']. Taking only the first one.\n",
      "More than one entity for SPA_DF_001255_20150626_F00100085: ['Q10846439', 'Q3884']. Taking only the first one.\n",
      "More than one entity for SPA_DF_001255_20150626_F00100085: ['Q10846439', 'Q3884']. Taking only the first one.\n",
      "More than one entity for SPA_DF_001258_20150623_F001000B7: ['Q10846439', 'Q3884']. Taking only the first one.\n",
      "ENG: 10234 mentions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4761f6e89a5548bab680c3015f5e4e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENG: 167 samples\n",
      "SPA: 4046 mentions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6cbd4dae134a649359efbd5a82f5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity.mention='Federación Interanacional  Fumones Asociado' != json_mention['meta']['mention']='Federación Interanacional Fumones Asociados'\n",
      "SPA: 167 samples\n",
      "CMN: 8476 mentions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73afb3c07ad41c48939e72a3c1136f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMN: 166 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "retrieved_data_folder = Path(\"/fsx/louismartin/bela/retrieved_from_aws_backup/\")\n",
    "for phase in [\"train\", \"dev\"]:\n",
    "    kilt_format_path = retrieved_data_folder / f\"ndecao/TACKBP2015/{phase}.jsonl\"\n",
    "    json_mentions = read_jsonl_file(kilt_format_path)\n",
    "    for json_mention in json_mentions:\n",
    "        # json_sample[\"output\"] looks something like [{'KB_ID': 'm.0d06m5', 'answer': ['Q6294']}] \n",
    "        # Convert it to -> ['Q6294']\n",
    "        entities = [entity_id for entity in json_mention[\"output\"] for entity_id in entity[\"answer\"]]\n",
    "        if len(entities) > 1:\n",
    "            print(f\"More than one entity for {json_mention['meta']['document_id']}: {entities}. Taking only the first one.\")\n",
    "        json_mention[\"entity_id\"] = entities[0]\n",
    "    json_mentions = [json_mention for json_mention in json_mentions if json_mention[\"entity_id\"] is not None]\n",
    "\n",
    "    languages = [\"ENG\", \"SPA\", \"CMN\"]\n",
    "    all_samples = []\n",
    "    for language in languages:\n",
    "        language_mentions = [json_mention for json_mention in json_mentions if json_mention[\"meta\"][\"document_id\"].startswith(language)]\n",
    "        print(f\"{language}: {len(language_mentions)} mentions\")\n",
    "        samples = convert_tackbp_json_mentions_to_samples(language_mentions)\n",
    "        all_samples.extend(samples)\n",
    "        print(f\"{language}: {len(samples)} samples\")\n",
    "        output_path = retrieved_data_folder / f\"ndecao/TACKBP2015/{phase}_bela_format_{language.lower()}.jsonl\"\n",
    "        with open(output_path, \"w\") as f:\n",
    "            for sample in samples:\n",
    "                f.write(json.dumps(sample_to_json(sample)) + \"\\n\")\n",
    "    output_path = retrieved_data_folder / f\"ndecao/TACKBP2015/{phase}_bela_format_all_languages.jsonl\"\n",
    "    random.shuffle(all_samples)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for sample in all_samples:\n",
    "            f.write(json.dumps(sample_to_json(sample)) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lorelei\n",
    "Lorelei samples look like this:\n",
    "```\n",
    "{'id': 'Men-VIE_SN_000370_20160324_G0T100LHC-7', 'input': ' [START] Bỉ [END] truy tìm nghi phạm mới trong vụ đánh bom ga tàu điện ngầm Brussels. - Mua bán thiết bị tàu biển https://t.co/Rf0J4cQKgu qua @sharethis', 'output': [{'answer': ['Q31']}], 'meta': {'left_context': '', 'mention': 'Bỉ', 'right_context': 'truy tìm nghi phạm mới trong vụ đánh bom ga tàu điện ngầm Brussels. - Mua bán thiết bị tàu biển https://t.co/Rf0J4cQKgu qua @sharethis', 'doc_id': 'VIE_SN_000370_20160324_G0T100LHC', 'entity_id': 'Ent-VIE_SN_000370_20160324_G0T100LHC-7', 'entity_type': 'GPE', 'mention_status': 'representative', 'start_char': '0', 'end_char': '1', 'original_mention_text': '__\\n', 'system_run_id': 'LDC', 'mention_text_2': '__', 'extents': 'VIE_SN_000370_20160324_G0T100LHC:0-1', 'kb_id': '2802361', 'mention_type': 'NAM', 'confidence': '1.0\\n'}, 'candidates': ['Q31', 'Q166776', 'Q697625', 'Q17435', 'Q815524', 'Q216022', 'Q20997', 'Q205317', 'Q381124', 'Q21000', 'Q1095', 'Q234', 'Q658870', 'Q15873988', 'Q792312', 'Q3992', 'Q792419', 'Q134121', 'Q240']}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/lorelei/lorelei_vietnamese.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80339e23dcb41a1a4fa4bb2e10e8e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one entity for VIE_SN_000370_20160324_G0T100LHC: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for VIE_SN_000370_20160326_G0T100LPM: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for VIE_SN_000370_20160318_G0T100L5Z: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_SN_000370_20160324_G0T100LFV: ['Q1054184', 'Q424']. Taking only the first one.\n",
      "More than one entity for VIE_SN_000370_20160324_G0T100LG2: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for VIE_SN_000370_20160322_G0T100KKL: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for VIE_SN_000370_20160318_G0T100L6Q: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_SN_000370_20160327_G0T100LQ6: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for VIE_SN_000370_20160324_G0T100LHY: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for VIE_SN_000370_20160325_G0T100LJC: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for VIE_SN_000370_20160311_G0T100KV4: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000751_20150511_G0022DDGS: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_000794_20160510_G0022DDH0: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001093_20151008_G0022DDJX: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001093_20151008_G0022DDJX: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001093_20151008_G0022DDJX: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001093_20151008_G0022DDJX: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001093_20151008_G0022DDJX: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001093_20151008_G0022DDJX: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001093_20151008_G0022DDJX: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001093_20151008_G0022DDJX: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001585_20160510_G0022DDLR: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001585_20160510_G0022DDLR: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_WL_001585_20160510_G0022DDLR: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001776_20151108_G0020SAZR: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001776_20151108_G0020SAZR: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001776_20151108_G0020SAZR: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001776_20151108_G0020SAZR: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001776_20151108_G0020SAZR: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001776_20151108_G0020SAZR: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001776_20151108_G0020SAZR: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000741_20141003_G0022DD6R: ['Q219060', 'Q1176995']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000747_20150928_G0022DCVW: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000747_20150928_G0022DCVW: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000757_20110210_G0022DCW3: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000757_20110210_G0022DCW3: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000757_20110210_G0022DCW3: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000757_20110210_G0022DCW3: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000757_20110210_G0022DCW3: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000757_20110210_G0022DCW3: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000757_20110210_G0022DCW3: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000757_20110210_G0022DCW3: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_000974_20130731_G0022DD7Y: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001223_20151230_G0022DD6H: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001223_20151230_G0022DD6H: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001223_20151230_G0022DD6H: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001224_20150811_G0022DD6J: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001224_20150811_G0022DD6J: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001224_20150811_G0022DD6J: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001348_20140804_G0022DD85: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001348_20140804_G0022DD85: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001348_20140804_G0022DD85: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001348_20140804_G0022DD85: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001348_20140804_G0022DD85: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001402_20080805_G0022DDAT: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001402_20080805_G0022DDAT: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_NW_001402_20080805_G0022DDAT: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_DF_000731_20130910_G0022DCIH: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_DF_000787_20160510_G0022DCII: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_DF_000787_20160510_G0022DCII: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_DF_000787_20160510_G0022DCII: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_DF_000787_20160510_G0022DCII: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_DF_000787_20160510_G0022DCII: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_DF_000787_20160510_G0022DCII: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_DF_000787_20160510_G0022DCII: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for VIE_DF_000787_20160510_G0022DCII: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for VIE_DF_001348_20160509_G0022DCKL: ['Q29520', 'Q148']. Taking only the first one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b2b97ce3b0434eab5c7a03c6b9cff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity.mention='Thanh Hóa: ' != json_mention['meta']['mention']='Thạch Thành'\n",
      "entity.mention='hành “thảo' != json_mention['meta']['mention']='Trung Quốc'\n",
      "/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/lorelei/lorelei_oromo.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102321538c734ee1a17c004a30b2b1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one entity for IL6_SN_000370_20161006_H0T00611X: ['Q3398478', 'Q7231129', 'Q3604', 'Q1118459']. Taking only the first one.\n",
      "More than one entity for IL6_SN_000370_20160426_H0T005ZK2: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for IL6_SN_000370_20160601_H0T005ZT7: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL6_SN_000370_20160614_H0T005ZVT: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for IL6_NW_020412_20150827_H0040MR7W: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL6_WL_020405_20170307_H0040LOHJ: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for IL6_WL_020405_20170307_H0040LOHJ: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for IL6_WL_020405_20170307_H0040LOHJ: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for IL6_WL_020405_20170307_H0040LOHJ: ['Q3960', 'Q408']. Taking only the first one.\n",
      "More than one entity for IL6_WL_020535_20151124_H0040MR6K: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for IL6_WL_020535_20151124_H0040MR6K: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for IL6_WL_020535_20151124_H0040MR6K: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for IL6_WL_020535_20151124_H0040MR6K: ['Q29520', 'Q148']. Taking only the first one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed12df17a4e4d5984fbfac04f513f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity.mention='FXG itti fufee -' != json_mention['meta']['mention']='Magaala Ambootti'\n",
      "entity.mention='rraa... ' != json_mention['meta']['mention']='Djibouti'\n",
      "entity.mention='a Aanaa shaallaat' != json_mention['meta']['mention']='Aanaa shaallaatti'\n",
      "entity.mention='amichi Bulli ' != json_mention['meta']['mention']='magala dodola'\n",
      "entity.mention='Protest' != json_mention['meta']['mention']='#Oromia'\n",
      "entity.mention='r Magaalaa Amb' != json_mention['meta']['mention']='Magaalaa Amboo'\n",
      "entity.mention='i aanaa Muneess' != json_mention['meta']['mention']='aanaa Muneessaa'\n",
      "entity.mention='r Aanaa Dand' != json_mention['meta']['mention']='Aanaa Dandii'\n",
      "entity.mention='anaa noolee kabbatti ' != json_mention['meta']['mention']='aanaa noolee kabbatti'\n",
      "entity.mention='anaa baddannoo ' != json_mention['meta']['mention']='aanaa baddannoo'\n",
      "entity.mention='anaa Laaloo Ass' != json_mention['meta']['mention']='Wallaggaa Lixaa'\n",
      "entity.mention='a Magaalaan Naqamt' != json_mention['meta']['mention']='Magaalaan Naqamtee'\n",
      "entity.mention='naa Gola Odaa ' != json_mention['meta']['mention']='Harargee Bahaa'\n",
      "entity.mention='a Magaalaan Naqamt' != json_mention['meta']['mention']='Magaalaan Naqamtee'\n",
      "entity.mention='ana Adabba ' != json_mention['meta']['mention']='aana Adabba'\n",
      "entity.mention='odina Salaale' != json_mention['meta']['mention']='Kaaba shaggar'\n",
      "entity.mention='a Aanaa Gullis' != json_mention['meta']['mention']='Aanaa Gullisoo'\n",
      "entity.mention='a Naannoo Harar' != json_mention['meta']['mention']='Naannoo Hararii'\n",
      "entity.mention='a Magaalaa Najjoot' != json_mention['meta']['mention']='Magaalaa Najjootti'\n",
      "entity.mention='a Magaalaa Naqam' != json_mention['meta']['mention']='Magaalaa Naqamte'\n",
      "entity.mention='a analee addadda k' != json_mention['meta']['mention']='n godina arsii lix'\n",
      "entity.mention='r Sulult' != json_mention['meta']['mention']='Sulultaa'\n",
      "entity.mention='a Magaalaa Najj' != json_mention['meta']['mention']='Magaalaa Najjoo'\n",
      "entity.mention='i Ganda qote bulaa Aabbuu seer' != json_mention['meta']['mention']='Ganda qote bulaa Aabbuu seeraa'\n",
      "entity.mention='e Agarf' != json_mention['meta']['mention']='Agarfaa'\n",
      "entity.mention='r Magaalaa Amb' != json_mention['meta']['mention']='Magaalaa Amboo'\n",
      "entity.mention='naa qoree ' != json_mention['meta']['mention']='anaa qoree'\n",
      "entity.mention='aanno mana hidh' != json_mention['meta']['mention']='Magaalaa Jimmaa'\n",
      "entity.mention='haa Aanaa... https:' != json_mention['meta']['mention']='a   godina shawaa b'\n",
      "entity.mention='r Amb' != json_mention['meta']['mention']='Amboo'\n",
      "entity.mention='Aanaa Xiyyo' != json_mention['meta']['mention']='Godina Arsi'\n",
      "entity.mention='i negele borenaa' != json_mention['meta']['mention']='negele borenaati'\n",
      "entity.mention='a Shawaa Lixaat' != json_mention['meta']['mention']='Shawaa Lixaatti'\n",
      "entity.mention='a Magaalaa Gidaam' != json_mention['meta']['mention']='Magaalaa Gidaamii'\n",
      "entity.mention='kkana ture. \"Mucaa wag' != json_mention['meta']['mention']='Magaalaa Dirree Dhawaa'\n",
      "entity.mention='naa adaabbaattii g' != json_mention['meta']['mention']='aanaa adaabbaattii'\n",
      "entity.mention='cihuun ' != json_mention['meta']['mention']='Oromiya'\n",
      "entity.mention='anaa kombolcha ' != json_mention['meta']['mention']='aanaa kombolcha'\n",
      "entity.mention='a Aana Dodol' != json_mention['meta']['mention']='Aana Dodolaa'\n",
      "entity.mention='e Kuttaayee,Lixa Shagg' != json_mention['meta']['mention']='Kuttaayee,Lixa Shaggar'\n",
      "entity.mention='anaa iluu galaan ma' != json_mention['meta']['mention']='godina shawaa lixaa'\n",
      "entity.mention='a Aanaa Meettaa Roobiit' != json_mention['meta']['mention']='Aanaa Meettaa Roobiitti'\n",
      "/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/lorelei/lorelei_tigrinya.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b91c3adc8c744e388f6731d60097421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one entity for IL5_NW_020508_20170425_H0040MI12: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYU: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYU: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYU: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYU: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYU: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYU: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYU: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYU: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYU: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160324_G0T0009JP: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160324_G0T0009JP: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020508_20170425_H0040MHM9: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020508_20170425_H0040MHM9: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020508_20170425_H0040MHM9: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020508_20170425_H0040MHM9: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020508_20170425_H0040MHM9: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020508_20170425_H0040MHM9: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020508_20170425_H0040MHM9: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160112_G0T000A2Q: ['Q406', 'Q16869']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160112_G0T000A2Q: ['Q406', 'Q16869']. Taking only the first one.\n",
      "More than one entity for IL5_WL_020506_20160425_H0040MDZH: ['Q3398478', 'Q7231129', 'Q3604', 'Q1118459']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160407_G0T0009GD: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160908_G0040KJ4B: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20151123_G0T000AI4: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20151123_G0T000AI4: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20151123_G0T000AI5: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for IL5_DF_020521_20170505_H0040MWIB: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160608_H0040MC9U: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160608_H0040MC9U: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160608_H0040MC9U: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160608_H0040MC9U: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160608_H0040MC9U: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160608_H0040MC9U: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160608_H0040MC9U: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160608_H0040MC9U: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160608_H0040MC9U: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20161001_G0040KSYS: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160106_G0T000A4D: ['Q28179', 'Q423']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160517_G0T00096L: ['Q1054184', 'Q424']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160517_G0T00096L: ['Q1054184', 'Q424']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160119_G0T000A0V: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160523_H0040MC9X: ['Q11767', 'Q796']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160523_H0040MC9X: ['Q11767', 'Q796']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20151125_G0T000AH3: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20151125_G0T000AH3: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20151125_G0T000AH3: ['Q29520', 'Q148']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160629_G0T0008UF: ['Q406', 'Q16869']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160330_G0T0009I4: ['Q229', 'Q395903']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020060_20161227_H0040MWIA: ['Q1384', 'Q22654']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020060_20160825_G0040KICM: ['Q1490', 'Q7473516', 'Q396867']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20151228_G0T000A6S: ['Q11767', 'Q796']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20151228_G0T000A6S: ['Q11767', 'Q796']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160520_G0T000952: ['Q229', 'Q395903']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160520_G0T000952: ['Q229', 'Q395903']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020060_20160329_G0040KQUF: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020060_20160329_G0040KQUF: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020060_20160329_G0040KQUF: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020060_20160329_G0040KQUF: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020060_20160329_G0040KQUF: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020079_20160610_G0040IC0W: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020079_20160610_G0040IC0W: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020079_20160610_G0040IC0W: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160716_H0040MC9N: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_WL_020083_20160928_G0040KNS6: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_WL_020083_20160928_G0040KNS6: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_WL_020083_20160928_G0040KNS6: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_WL_020083_20160928_G0040KNS6: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160114_G0T000A23: ['Q188161', 'Q252']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160329_G0T0009IH: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160329_G0T0009IH: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20150605_G0040EZAQ: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20150605_G0040EZAQ: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160318_G0T0009L3: ['Q11767', 'Q796']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYV: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYV: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYV: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYV: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYV: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYV: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYV: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYV: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYV: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020062_20160929_G0040KSYV: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160614_H0040MC9S: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160614_H0040MC9S: ['Q613', 'Q612']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160614_H0040MC9S: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020490_20160723_H0040MC9M: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_WL_020506_20160402_H0040ME11: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_WL_020506_20160402_H0040ME11: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_WL_020506_20160402_H0040ME11: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160322_G0T0009KB: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160322_G0T0009KC: ['Q239', 'Q240']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020079_20150903_G0040IC2W: ['Q3067114', 'Q36378']. Taking only the first one.\n",
      "More than one entity for IL5_NW_020508_20170425_H0040MHM5: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160705_G0T0008T7: ['Q11767', 'Q796']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160726_G0T0008MU: ['Q15878', 'Q18677875']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20160928_H0T00362B: ['Q212', 'Q133356']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20161014_H0T003669: ['Q227511', 'Q1049']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20170124_H0T0036KE: ['Q11767', 'Q796']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20170124_H0T0036KE: ['Q11767', 'Q796']. Taking only the first one.\n",
      "More than one entity for IL5_SN_000370_20170330_H0T0036OH: ['Q227511', 'Q1049']. Taking only the first one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bb4d583a474644af07f0f433cc5a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity.mention='ልፍል ጥ' != json_mention['meta']['mention']='`ትግራይ'\n",
      "entity.mention='ጃምላ ዝተቐዝፉ' != json_mention['meta']['mention']='ማእከላይ ባሕሪ'\n",
      "entity.mention='\\xa0ኢትዮጵ' != json_mention['meta']['mention']='ኢትዮጵያ'\n",
      "entity.mention='a ኤርትራው' != json_mention['meta']['mention']='ኤርትራውያን'\n",
      "entity.mention='\\xa0ኣመሪ' != json_mention['meta']['mention']='ኣመሪካ'\n",
      "entity.mention='ዝበፅሕ ሓደ' != json_mention['meta']['mention']='ዓዲ ጣልያን'\n",
      "entity.mention='ዑቕባ ' != json_mention['meta']['mention']='ኣውሮፖ'\n",
      "entity.mention='ን ፊንላን' != json_mention['meta']['mention']='ፊንላንድን'\n",
      "entity.mention='a #ارتر' != json_mention['meta']['mention']='#ارتريا'\n",
      "/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/lorelei/lorelei_oromo_bela_format.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940601650074458da982fe2b41d7a0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m json_mentions \u001b[39m=\u001b[39m read_jsonl_file(path)\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m json_mention \u001b[39min\u001b[39;00m json_mentions:\n\u001b[1;32m     40\u001b[0m     \u001b[39m# json_sample[\"output\"] looks something like [{'KB_ID': 'm.0d06m5', 'answer': ['Q6294']}] \u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Convert it to -> ['Q6294']\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     entities \u001b[39m=\u001b[39m [entity_id \u001b[39mfor\u001b[39;00m entity \u001b[39min\u001b[39;00m json_mention[\u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mfor\u001b[39;00m entity_id \u001b[39min\u001b[39;00m entity[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(entities) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     44\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMore than one entity for \u001b[39m\u001b[39m{\u001b[39;00mjson_mention[\u001b[39m'\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdoc_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mentities\u001b[39m}\u001b[39;00m\u001b[39m. Taking only the first one.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'output'"
     ]
    }
   ],
   "source": [
    "def convert_lorelei_json_mentions_to_sample(json_mentions: List[Dict]) -> Sample:\n",
    "    \"\"\"Given a list of json mentions, convert them to a Sample object.\"\"\"\n",
    "    # Check that all mentions are from the same document\n",
    "    assert len(set(json_mention[\"meta\"][\"doc_id\"] for json_mention in json_mentions)) == 1\n",
    "    # Order mentions by start offset\n",
    "    json_mentions.sort(key=lambda json_mention: json_mention[\"meta\"][\"start_char\"])\n",
    "    ground_truth_entities = []\n",
    "    text = infer_original_text([json_mention[\"input\"] for json_mention in json_mentions])\n",
    "    for json_mention in json_mentions:\n",
    "        offset = len(json_mention[\"meta\"][\"left_context\"])\n",
    "        length = len(json_mention[\"meta\"][\"mention\"])\n",
    "        # Leading whitespaces\n",
    "        additional_spaces = len(text[offset:]) - len(text[offset:].lstrip(\" \\n\\u3000\"))\n",
    "        offset += additional_spaces\n",
    "        entity = Entity(text=text, offset=offset, length=length, entity_id=json_mention[\"entity_id\"])\n",
    "        # Sometimes there are extra spaces / newlines in the original text mention that were removed in the metadata\n",
    "        assert entity.entity_id is not None\n",
    "        #assert entity.mention == json_mention[\"meta\"][\"mention\"], f\"{entity.mention=} != {json_mention['meta']['mention']=}\"\n",
    "        if entity.mention != json_mention[\"meta\"][\"mention\"]:\n",
    "            print(f\"{entity.mention=} != {json_mention['meta']['mention']=}\")\n",
    "        ground_truth_entities.append(entity)\n",
    "    return Sample(text=text, sample_id=json_mentions[0][\"meta\"][\"doc_id\"], ground_truth_entities=ground_truth_entities)\n",
    "\n",
    "\n",
    "def convert_lorelei_json_mentions_to_samples(json_mentions: List[Dict]) -> List[Sample]:\n",
    "    \"\"\"Given a list of json mentions, group them by document and convert them to Samples objects.\"\"\"\n",
    "    # Group mentions together\n",
    "    aggregated_mentions = defaultdict(list)\n",
    "    for json_mention in json_mentions:\n",
    "        aggregated_mentions[json_mention[\"meta\"][\"doc_id\"]].append(json_mention)\n",
    "    samples = [convert_lorelei_json_mentions_to_sample(json_mentions) for json_mentions in tqdm(aggregated_mentions.values())]\n",
    "    return samples\n",
    "\n",
    "\n",
    "lorelei_dir = Path(\"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/lorelei/\")\n",
    "for path in lorelei_dir.glob(\"*.jsonl\"):\n",
    "    print(path)\n",
    "    json_mentions = read_jsonl_file(path)\n",
    "    for json_mention in json_mentions:\n",
    "        # json_sample[\"output\"] looks something like [{'KB_ID': 'm.0d06m5', 'answer': ['Q6294']}] \n",
    "        # Convert it to -> ['Q6294']\n",
    "        entities = [entity_id for entity in json_mention[\"output\"] for entity_id in entity[\"answer\"]]\n",
    "        if len(entities) > 1:\n",
    "            print(f\"More than one entity for {json_mention['meta']['doc_id']}: {entities}. Taking only the first one.\")\n",
    "        if len(entities) == 0:\n",
    "            json_mention[\"entity_id\"] = None\n",
    "            continue\n",
    "        json_mention[\"entity_id\"] = entities[0]\n",
    "    json_mentions = [json_mention for json_mention in json_mentions if json_mention[\"entity_id\"] is not None]\n",
    "    samples = convert_lorelei_json_mentions_to_samples(json_mentions)\n",
    "    output_path = lorelei_dir / f\"{path.stem}_bela_format.jsonl\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for sample in samples:\n",
    "            f.write(json.dumps(sample_to_json(sample)) + \"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of files in BELA/Matcha format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aida_format_path = \"/fsx/kassner/data_BELA/wikipedia/aida_pretrain.jsonl\"\n",
    "# gt_entities: list of entities in the format [offset (words), length (words), wiki_data_id, kb_type?]\n",
    "# entities_raw: list of entities in the format [offset (chars), length (chars), wiki_data_id, kb_type?]\n",
    "# Seems that kb_type is always \"wiki\": set([entity[-1] for sample in df_aida[\"entities_raw\"].tolist() for entity in sample])\n",
    "df_aida = pd.DataFrame(read_jsonl_file(aida_format_path, n_lines=100000))\n",
    "df_aida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d76b99c7f04805835343263f93a0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_example_id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>gt_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14932393_1</td>\n",
       "      <td>Adobe Creek rises on the west flank of Sonoma...</td>\n",
       "      <td>[[0, 0, Q7562183, wiki, 40, 15], [0, 0, Q71716...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1010039_0</td>\n",
       "      <td>Esther Gorostiza Garai (Atxondo, Bizkaia, 19...</td>\n",
       "      <td>[[0, 0, Q1242393, wiki, 26, 7], [0, 0, Q93366,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>458956_3</td>\n",
       "      <td>&lt;section  Articles connexes &gt; industrie automo...</td>\n",
       "      <td>[[0, 0, Q65445, wiki, 52, 14], [0, 0, Q184937,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10739205_2</td>\n",
       "      <td>From 1956 to 1963, Habib worked as an assista...</td>\n",
       "      <td>[[0, 0, Q1044, wiki, 237, 12], [0, 0, Q502276,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116287_1</td>\n",
       "      <td>South Sanford is located at lat:43.4019444444...</td>\n",
       "      <td>[[0, 0, Q637413, wiki, 119, 27]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>11782414_1</td>\n",
       "      <td>Bei den Olympischen Jugendspielen 2010 in Sin...</td>\n",
       "      <td>[[0, 0, Q613716, wiki, 9, 30], [0, 0, Q815514,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>16744_0</td>\n",
       "      <td>minidesno200pxKraljevska palata Aranjuez Conc...</td>\n",
       "      <td>[[0, 0, Q29, wiki, 96, 8], [0, 0, Q151084, wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>7646568_0</td>\n",
       "      <td>Werner Protzel (* 5. Oktober 1973 in Rosenhei...</td>\n",
       "      <td>[[0, 0, Q2930, wiki, 19, 10], [0, 0, Q2477, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>38438_0</td>\n",
       "      <td>Марек Михал Грехута ( 10 Арванхоёрдугаар сар...</td>\n",
       "      <td>[[0, 0, Q36, wiki, 78, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>667438_1</td>\n",
       "      <td>فهرست آثار ملی ایران, سازمان میراث فرهنگی، صن...</td>\n",
       "      <td>[[0, 0, Q5958900, wiki, 1, 20], [0, 0, Q335599...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      data_example_id                                      original_text  \\\n",
       "0          14932393_1   Adobe Creek rises on the west flank of Sonoma...   \n",
       "1           1010039_0    Esther Gorostiza Garai (Atxondo, Bizkaia, 19...   \n",
       "2            458956_3  <section  Articles connexes > industrie automo...   \n",
       "3          10739205_2   From 1956 to 1963, Habib worked as an assista...   \n",
       "4            116287_1   South Sanford is located at lat:43.4019444444...   \n",
       "...               ...                                                ...   \n",
       "99995      11782414_1   Bei den Olympischen Jugendspielen 2010 in Sin...   \n",
       "99996         16744_0   minidesno200pxKraljevska palata Aranjuez Conc...   \n",
       "99997       7646568_0   Werner Protzel (* 5. Oktober 1973 in Rosenhei...   \n",
       "99998         38438_0    Марек Михал Грехута ( 10 Арванхоёрдугаар сар...   \n",
       "99999        667438_1   فهرست آثار ملی ایران, سازمان میراث فرهنگی، صن...   \n",
       "\n",
       "                                             gt_entities  \n",
       "0      [[0, 0, Q7562183, wiki, 40, 15], [0, 0, Q71716...  \n",
       "1      [[0, 0, Q1242393, wiki, 26, 7], [0, 0, Q93366,...  \n",
       "2      [[0, 0, Q65445, wiki, 52, 14], [0, 0, Q184937,...  \n",
       "3      [[0, 0, Q1044, wiki, 237, 12], [0, 0, Q502276,...  \n",
       "4                       [[0, 0, Q637413, wiki, 119, 27]]  \n",
       "...                                                  ...  \n",
       "99995  [[0, 0, Q613716, wiki, 9, 30], [0, 0, Q815514,...  \n",
       "99996  [[0, 0, Q29, wiki, 96, 8], [0, 0, Q151084, wik...  \n",
       "99997  [[0, 0, Q2930, wiki, 19, 10], [0, 0, Q2477, wi...  \n",
       "99998                         [[0, 0, Q36, wiki, 78, 5]]  \n",
       "99999  [[0, 0, Q5958900, wiki, 1, 20], [0, 0, Q335599...  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only care about raw text and chars offsets\n",
    "path = \"/fsx/movb/data/matcha/mel/test.txt\"\n",
    "df = pd.DataFrame(read_jsonl_file(path))\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old method\n",
    "\n",
    "The old method used dataframes and was broken / not robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnest_dict_column(df, column_name):\n",
    "    unnested_df = df[column_name].apply(pd.Series)\n",
    "    # Add prefix to avoid name collisions\n",
    "    unnested_df = unnested_df.add_prefix(f\"{column_name}.\")\n",
    "    return pd.concat([df.drop([column_name], axis=1), unnested_df], axis=1)\n",
    "\n",
    "\n",
    "def extract_entity_in_aida_format(row):\n",
    "    # Format: [offset (chars), length (chars), wiki_data_id, kb_type?]\n",
    "    return [len(row[\"meta.left_context_original\"]), len(row[\"meta.mention_original\"]), row[\"output.answer\"], \"wiki\"]  # TODO: Infered that the kb type is wiki but not sure\n",
    "\n",
    "\n",
    "def concatenate_contexts_and_mention(left_context, mention, right_context):\n",
    "    left_separator = \" \"\n",
    "    right_separator = \" \"\n",
    "    # If the right/left context starts/ends with a punctuation, then an extra space would be added, so we have to remove it   \n",
    "    left_punctuation_chars = tuple(\" ,!?;:()[]{}'’‘“\\\"\")\n",
    "    right_punctuation_chars = tuple(\" ,.!?;:()[]{}'’‘“\\\"\")\n",
    "    if right_context.startswith(right_punctuation_chars):\n",
    "        right_separator = \"\"\n",
    "    if left_context.endswith(left_punctuation_chars):\n",
    "        left_separator = \"\"\n",
    "    return \"\".join([left_context, left_separator, mention, right_separator, right_context])\n",
    "\n",
    "\n",
    "def fix_offsets(text, entity, mention):\n",
    "    \"\"\"Fix offsets in the entity list that might have been corrupted by the concatenation\n",
    "    - entity format: [offset (chars), length (chars), wiki_data_id, kb_type]\n",
    "    \"\"\"\n",
    "    offset, length, _, _ = entity\n",
    "    recovered_mention = text[offset:offset + length]\n",
    "    if mention != recovered_mention:\n",
    "        # Find the mention in the text\n",
    "        lookup_offset = offset - (len(mention) - 1)  # So that we can't match another mention that would be right before the mention\n",
    "        infered_offset = text[lookup_offset:].find(mention) + (lookup_offset)\n",
    "        if infered_offset == -1:\n",
    "            raise ValueError(f\"Could not find mention {mention} in text {text}\")\n",
    "        # Fix the offset\n",
    "        assert abs(infered_offset - offset) < 10, f\"Offset is too far from the infered offset: {offset=}, {infered_offset=}, {mention=}, {text[offset:offset + 50]=}\"\n",
    "        entity[0] = infered_offset\n",
    "        entity[1] = len(mention)\n",
    "    return entity\n",
    "\n",
    "\n",
    "# Quick unit tests\n",
    "assert fix_offsets(\"hello world\", [7, 5, \"Q123\", \"wiki\"], \"world\") == [6, 5, \"Q123\", \"wiki\"]\n",
    "assert fix_offsets(\"hello world world\", [11, 5, \"Q123\", \"wiki\"], \"world\") == [12, 5, \"Q123\", \"wiki\"]\n",
    "\n",
    "\n",
    "def is_correct_entity_offset(text, entity, mention):\n",
    "    offset, length, _, _ = entity\n",
    "    return text[offset:offset + length] == mention\n",
    "\n",
    "\n",
    "def infer_original_text(texts_with_annotated_mentions):\n",
    "    # NOT USED YET\n",
    "    \"\"\"Infer the original text from the texts with annotated mentions.\n",
    "    Each annotation might have introduced additional spaces but we don't know when, so we combine the longest left and right context to infer the original text.\n",
    "    Use with: `df_kilt.groupby(\"meta.document_id\").agg({\"original_text\": infer_original_text}).reset_index()`\n",
    "    \"\"\"\n",
    "    if len(texts_with_annotated_mentions) == 1:\n",
    "        [text] = texts_with_annotated_mentions\n",
    "        left_context = text.split(\" [START]\")[0]\n",
    "        right_context = text.split(\"[END] \")[1]\n",
    "        mention = text.split(\"[START] \")[1].split(\" [END]\")[0]\n",
    "        return concatenate_contexts_and_mention(left_context, mention, right_context)\n",
    "    left_contexts = [text.split(\" [START]\")[0] for text in texts_with_annotated_mentions]\n",
    "    right_contexts = [text.split(\"[END] \")[1] for text in texts_with_annotated_mentions]\n",
    "    longest_left_context = max(left_contexts, key=len)\n",
    "    longest_right_context = max(right_contexts, key=len)\n",
    "    # Some of the longest left context's end is often some of the start of the right context\n",
    "    # Find the substring in common\n",
    "    intersection = \"\"\n",
    "    for i in range(len(longest_right_context)):\n",
    "        if longest_left_context.endswith(longest_right_context[:i]):\n",
    "            intersection = longest_right_context[:i]\n",
    "    assert intersection in longest_right_context\n",
    "    # Merge the two contexts, removing the intersection\n",
    "    merged = \"\".join([longest_left_context, longest_right_context.replace(intersection, \"\")])\n",
    "    return merged\n",
    "\n",
    "\n",
    "\n",
    "def convert_kilt_to_bela_format(kilt_format_path):\n",
    "    \"\"\"Example usage: `convert_kilt_to_bela_format(Path(\"/fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train.jsonl\"))`\n",
    "    \"\"\"\n",
    "    df_kilt = pd.DataFrame(read_jsonl_file(kilt_format_path, n_lines=10000))\n",
    "    df_kilt = unnest_dict_column(df_kilt, \"meta\")\n",
    "    assert df_kilt[\"output\"].apply(len).unique().tolist() == [1]\n",
    "    df_kilt[\"output\"] = df_kilt[\"output\"].apply(lambda x: x[0])  # Only one output per sample\n",
    "    df_kilt = unnest_dict_column(df_kilt, \"output\")\n",
    "    # assert df_kilt[\"output.answer\"].apply(len).unique().tolist() == [1]\n",
    "    df_kilt[\"output.answer\"] = df_kilt[\"output.answer\"].apply(lambda x: x[0])  # Take only the first option, e.g. sample TEDL15_TRAINING_06967 has [Q612, Q613] for mention Dubai\n",
    "\n",
    "    df_kilt[\"entities_raw\"] = df_kilt.apply(extract_entity_in_aida_format, axis=1).tolist()\n",
    "    # Aggregate \"entities_raw\" as list and \"meta.input_original\" as a single string (should be unique)\n",
    "    df_kilt[\"text_raw\"] = df_kilt.apply(lambda row: concatenate_contexts_and_mention(row[\"meta.left_context_original\"], row[\"meta.mention_original\"], row[\"meta.right_context_original\"]), axis=1)\n",
    "    df_bela_format = df_kilt.groupby(\"meta.document_id\").agg({\n",
    "        \"text_raw\": \"first\",  # TODO: The concatenation sometimes produces some different texts (adding extra spaces between mention and context or removing them).\n",
    "        \"entities_raw\": list,\n",
    "        \"meta.mention_original\": list,\n",
    "    }).reset_index()\n",
    "    # Fix offsets\n",
    "    for _, row in df_bela_format.iterrows():\n",
    "        fixed_entities = []\n",
    "        for i, (entity, mention) in enumerate(zip(row[\"entities_raw\"], row[\"meta.mention_original\"])):\n",
    "            if not is_correct_entity_offset(row[\"text_raw\"], entity, mention):\n",
    "                try:\n",
    "                    entity = fix_offsets(row[\"text_raw\"], entity, mention)\n",
    "                except AssertionError as e:\n",
    "                    print(f\"Skipping mention in {row['meta.document_id']=}: {e}\")\n",
    "                    continue\n",
    "                assert is_correct_entity_offset(row[\"text_raw\"], entity, mention), f\"{mention=}, {entity=}\"\n",
    "            fixed_entities.append(entity)\n",
    "        row[\"entities_raw\"] = fixed_entities\n",
    "    df_bela_format[\"entities_raw\"] = df_bela_format[\"entities_raw\"].apply(lambda entities: [[0, 0] + entity for entity in entities])  # Add dummy 0, 0 for backward compatibility\n",
    "    # HACK: reorder entities in the format that is used in EvaluateMEL.ipynb notebook. Format in EvaluateMEL.ipynb: _,_,ent_id,_,offset,length.\n",
    "    # TODO: We should uniformize. \n",
    "    df_bela_format[\"entities_raw\"] = df_bela_format[\"entities_raw\"].apply(lambda entities: [(dummy_1, dummy_2, entity, source, offset, length) for dummy_1, dummy_2, offset, length, entity, source in entities])\n",
    "    # document_id\toriginal_text\tgt_entities\n",
    "    df_bela_format = df_bela_format.rename(columns={\"meta.document_id\": \"document_id\", \"text_raw\": \"original_text\", \"entities_raw\": \"gt_entities\"})\n",
    "    df_bela_format = df_bela_format[[\"document_id\", \"original_text\", \"gt_entities\"]]\n",
    "    new_path = kilt_format_path.parent / f\"{kilt_format_path.stem}_bela_format.jsonl\"\n",
    "    # Write to jsonl \n",
    "    with open(new_path, \"w\", encoding=\"utf8\") as f:\n",
    "        for _, row in df_bela_format.iterrows():\n",
    "            f.write(row.to_json() + \"\\n\")\n",
    "    print(f\"Saved {new_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieved_data_folder = Path(\"/fsx/louismartin/bela/retrieved_from_aws_backup/\")\n",
    "kilt_format_path = retrieved_data_folder / \"ndecao/TACKBP2015/train.jsonl\"\n",
    "#convert_kilt_to_bela_format(kilt_format_path)\n",
    "df_kilt = pd.DataFrame(read_jsonl_file(kilt_format_path, n_lines=10000))\n",
    "df_kilt = unnest_dict_column(df_kilt, \"meta\")\n",
    "assert df_kilt[\"output\"].apply(len).unique().tolist() == [1]\n",
    "df_kilt[\"output\"] = df_kilt[\"output\"].apply(lambda x: x[0])  # Only one output per sample\n",
    "df_kilt = unnest_dict_column(df_kilt, \"output\")\n",
    "# assert df_kilt[\"output.answer\"].apply(len).unique().tolist() == [1]\n",
    "df_kilt[\"output.answer\"] = df_kilt[\"output.answer\"].apply(lambda x: x[0])  # Take only the first option, e.g. sample TEDL15_TRAINING_06967 has [Q612, Q613] for mention Dubai\n",
    "\n",
    "df_kilt[\"entities_raw\"] = df_kilt.apply(extract_entity_in_aida_format, axis=1).tolist()\n",
    "# Aggregate \"entities_raw\" as list and \"meta.input_original\" as a single string (should be unique)\n",
    "df_kilt[\"text_raw\"] = df_kilt.apply(lambda row: concatenate_contexts_and_mention(row[\"meta.left_context_original\"], row[\"meta.mention_original\"], row[\"meta.right_context_original\"]), axis=1)\n",
    "df_bela_format = df_kilt.groupby(\"meta.document_id\").agg({\n",
    "    \"text_raw\": \"first\",  # TODO: The concatenation sometimes produces some different texts (adding extra spaces between mention and context or removing them).\n",
    "    \"entities_raw\": list,\n",
    "    \"meta.mention_original\": list,\n",
    "}).reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bela.utils.analysis_utils import Entity, Sample\n",
    "\n",
    "\n",
    "def coerce_to_sample(jsonl_sample):\n",
    "    \"\"\"Coerce a jsonl sample to a Sample object.\n",
    "    \"\"\"\n",
    "    if isinstance(jsonl_sample, Sample):\n",
    "        return jsonl_sample\n",
    "    # BELA format\n",
    "    if all(key in jsonl_sample for key in [\"document_id\", \"original_text\", \"gt_entities\"]):\n",
    "        ground_truth_entities = [\n",
    "            Entity(entity_id=entity_id, offset=offset, length=length, text=jsonl_sample['original_text'])\n",
    "            for _, _, entity_id, _, offset, length in jsonl_sample['gt_entities']\n",
    "        ]\n",
    "        return Sample(text=jsonl_sample['original_text'], ground_truth_entities=ground_truth_entities)\n",
    "    raise ValueError(f\"Unknown format for {jsonl_sample=}\")\n",
    "\n",
    "\n",
    "def coerce_to_entity(jsonl_entity):\n",
    "    \"\"\"Coerce a jsonl entity to an Entity object.\n",
    "    \"\"\"\n",
    "    if isinstance(jsonl_entity, Entity):\n",
    "        return jsonl_entity\n",
    "    # KILT format\n",
    "    if all(key in jsonl_entity for key in [\"id\", \"input\", \"output\", \"meta\"]):\n",
    "        return Entity(entity_id=jsonl_entity[\"output\"][\"answer\"], offset=jsonl_entity[\"meta\"][\"offset\"], length=jsonl_entity[\"meta\"][\"length\"], text=jsonl_entity[\"input\"])\n",
    "    raise ValueError(f\"Unknown format for {jsonl_entity=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "#kilt_format_path = Path.home() / \"dev/BELA/data/KILT_format/TACKBP2015_training.jsonl\"\n",
    "retrieved_data_folder = Path(\"/fsx/louismartin/bela/retrieved_from_aws_backup/\")\n",
    "kilt_format_path = retrieved_data_folder / \"ndecao/TACKBP2015/train.jsonl\"\n",
    "for line in islice(yield_jsonl_lines(kilt_format_path), 10, 100):\n",
    "    if not line[\"meta\"][\"document_id\"].startswith(\"EN\"):\n",
    "        continue\n",
    "    print(\"*\" * 100)\n",
    "    pprint(line, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /fsx/louismartin/bela/retrieved_from_aws_backup/ndecao/TACKBP2015/train_bela_format.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df[\"meta.document_id\"].str.startswith(\"ENG\")]  # Take only english\n",
    "#df = df[~df[\"meta.document_id\"].str.startswith(\"CMN\")]  # Remove Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"hello world \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_kilt[\"meta.document_id\"] == \"CMN_NW_001145_20150413_F0000005B\"\n",
    "df_kilt[mask].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Documents with texts not joined correctly\")\n",
    "mask = (df[\"n_text_raw\"] > 1)\n",
    "print(df[mask][\"meta.document_id\"].tolist()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id = \"ENG_NW_001006_20150301_F00000005\"  # Bosco café\n",
    "document_id = \"CMN_DF_000178_20150318_F000000CO\"\n",
    "document_id = \"CMN_DF_000178_20150318_F000000CO\"\n",
    "mask = df[\"meta.document_id\"] == document_id\n",
    "print(list(df[mask].head(1).to_dict(orient=\"records\")[0][\"text_raw\"])[0])\n",
    "print(\"------------\")\n",
    "print(list(df[mask].head(1).to_dict(orient=\"records\")[0][\"text_raw\"])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_kilt[\"output\"].apply(lambda x: x[0][\"answer\"] != [None])\n",
    "df_kilt[mask]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "235bcda9aabf5f8363267684c8e0d7a57e9fb12569fe58ae215aef35e2f0a58c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
